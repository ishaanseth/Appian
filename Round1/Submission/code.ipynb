{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96208800-22cf-4090-9174-9071d646af4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Train shape: (1567, 29)\n",
      "Test shape: (673, 28)\n",
      "Feature engineering complete.\n",
      "\n",
      "Train Data Info after processing:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1567 entries, 0 to 1566\n",
      "Data columns (total 31 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   ID                   1567 non-null   int64  \n",
      " 1   Education            1567 non-null   object \n",
      " 2   Marital_Status       1567 non-null   object \n",
      " 3   Income               1550 non-null   float64\n",
      " 4   Kidhome              1567 non-null   int64  \n",
      " 5   Teenhome             1567 non-null   int64  \n",
      " 6   Recency              1567 non-null   int64  \n",
      " 7   MntWines             1544 non-null   float64\n",
      " 8   MntFruits            1567 non-null   int64  \n",
      " 9   MntMeatProducts      1561 non-null   float64\n",
      " 10  MntFishProducts      1567 non-null   int64  \n",
      " 11  MntSweetProducts     1567 non-null   int64  \n",
      " 12  MntGoldProds         1555 non-null   float64\n",
      " 13  NumDealsPurchases    1567 non-null   int64  \n",
      " 14  NumWebPurchases      1567 non-null   int64  \n",
      " 15  NumCatalogPurchases  1567 non-null   int64  \n",
      " 16  NumStorePurchases    1567 non-null   int64  \n",
      " 17  NumWebVisitsMonth    1567 non-null   int64  \n",
      " 18  AcceptedCmp3         1567 non-null   int64  \n",
      " 19  AcceptedCmp4         1567 non-null   int64  \n",
      " 20  AcceptedCmp5         1567 non-null   int64  \n",
      " 21  AcceptedCmp1         1567 non-null   int64  \n",
      " 22  AcceptedCmp2         1567 non-null   int64  \n",
      " 23  Complain             1567 non-null   int64  \n",
      " 24  Target               1567 non-null   int64  \n",
      " 25  Age                  1564 non-null   float64\n",
      " 26  Customer_Lifetime    1567 non-null   float64\n",
      " 27  Children             1567 non-null   int64  \n",
      " 28  Total_Mnt            1567 non-null   float64\n",
      " 29  Total_Purchases      1567 non-null   int64  \n",
      " 30  Total_CmpAccepted    1567 non-null   int64  \n",
      "dtypes: float64(7), int64(22), object(2)\n",
      "memory usage: 379.6+ KB\n",
      "\n",
      "Test Data Info after processing:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 673 entries, 0 to 672\n",
      "Data columns (total 30 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   ID                   673 non-null    int64  \n",
      " 1   Education            673 non-null    object \n",
      " 2   Marital_Status       673 non-null    object \n",
      " 3   Income               666 non-null    float64\n",
      " 4   Kidhome              673 non-null    int64  \n",
      " 5   Teenhome             673 non-null    int64  \n",
      " 6   Recency              673 non-null    int64  \n",
      " 7   MntWines             661 non-null    float64\n",
      " 8   MntFruits            673 non-null    int64  \n",
      " 9   MntMeatProducts      664 non-null    float64\n",
      " 10  MntFishProducts      673 non-null    int64  \n",
      " 11  MntSweetProducts     673 non-null    int64  \n",
      " 12  MntGoldProds         666 non-null    float64\n",
      " 13  NumDealsPurchases    673 non-null    int64  \n",
      " 14  NumWebPurchases      673 non-null    int64  \n",
      " 15  NumCatalogPurchases  673 non-null    int64  \n",
      " 16  NumStorePurchases    673 non-null    int64  \n",
      " 17  NumWebVisitsMonth    673 non-null    int64  \n",
      " 18  AcceptedCmp3         673 non-null    int64  \n",
      " 19  AcceptedCmp4         673 non-null    int64  \n",
      " 20  AcceptedCmp5         673 non-null    int64  \n",
      " 21  AcceptedCmp1         673 non-null    int64  \n",
      " 22  AcceptedCmp2         673 non-null    int64  \n",
      " 23  Complain             673 non-null    int64  \n",
      " 24  Age                  673 non-null    float64\n",
      " 25  Customer_Lifetime    673 non-null    float64\n",
      " 26  Children             673 non-null    int64  \n",
      " 27  Total_Mnt            673 non-null    float64\n",
      " 28  Total_Purchases      673 non-null    int64  \n",
      " 29  Total_CmpAccepted    673 non-null    int64  \n",
      "dtypes: float64(7), int64(21), object(2)\n",
      "memory usage: 157.9+ KB\n",
      "\n",
      "Numerical features (27): ['Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Age', 'Customer_Lifetime', 'Children', 'Total_Mnt', 'Total_Purchases', 'Total_CmpAccepted']\n",
      "Categorical features (2): ['Education', 'Marital_Status']\n",
      "\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_6644\\1302510903.py:50: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_6644\\1302510903.py:69: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed['Customer_Lifetime'].fillna(df_processed['Customer_Lifetime'].median(), inplace=True) # Impute NaNs created by NaT\n",
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_6644\\1302510903.py:35: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_6644\\1302510903.py:50: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_6644\\1302510903.py:50: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_6644\\1302510903.py:69: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed['Customer_Lifetime'].fillna(df_processed['Customer_Lifetime'].median(), inplace=True) # Impute NaNs created by NaT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "Predicting on test data...\n",
      "Prediction complete.\n",
      "\n",
      "Submission file 'submission.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution:\n",
      "Target\n",
      "0    0.624071\n",
      "1    0.375929\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Training Set Evaluation (Sanity Check) ---\n",
      "Accuracy: 0.9713\n",
      "ROC AUC: 0.9970\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed (e.g., from one-hot encoder)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Often related to sparse output default\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# --- Feature Engineering & Preprocessing ---\n",
    "\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years\n",
    "    current_year = datetime.datetime.now().year\n",
    "    # Use a reasonable reference year based on Dt_Customer if available, otherwise current year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except: # Handle cases where Dt_Customer might not exist or be parseable easily\n",
    "        reference_year = current_year\n",
    "\n",
    "    # Replace very old birth years (e.g., < 1910) with NaN to be imputed later\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "\n",
    "    # Calculate Age (handle potential NaNs in Year_Birth temporarily)\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 2. Process Dt_Customer\n",
    "    # Convert 'Dt_Customer' to datetime, coercing errors\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "\n",
    "    # Find the latest date for calculating tenure if not provided (from training set)\n",
    "    if is_train:\n",
    "        global global_latest_date # Store latest date from training set globally\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1)\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        # Fallback if called on test set first or global_latest_date isn't set\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1) # Use start of next year as reference\n",
    "        print(f\"Warning: Using fallback latest date: {latest_date_to_use}\")\n",
    "\n",
    "\n",
    "    # Calculate Customer_Lifetime (Tenure in days)\n",
    "    # Handle potential NaT dates resulting from coerce errors\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    df_processed['Customer_Lifetime'].fillna(df_processed['Customer_Lifetime'].median(), inplace=True) # Impute NaNs created by NaT\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    # Consolidate categories\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner',\n",
    "        'Together': 'Partner',\n",
    "        'Absurd': 'Single',\n",
    "        'Alone': 'Single',\n",
    "        'YOLO': 'Single',\n",
    "        'Widow': 'Single',\n",
    "        'Divorced':'Single'\n",
    "         }) # Grouping Married/Together and others into Single for simplicity\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'}) # Group '2n Cycle' with 'Master'\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # Optionally drop original columns if 'Children' is deemed sufficient\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns (identified during EDA)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore') # errors='ignore' in case they were already dropped\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# Preprocess training data\n",
    "train_df_processed = preprocess_data(train_df, is_train=True)\n",
    "# Preprocess test data using the latest date from training data\n",
    "test_df_processed = preprocess_data(test_df, is_train=False, latest_date=global_latest_date)\n",
    "\n",
    "print(\"Feature engineering complete.\")\n",
    "print(\"\\nTrain Data Info after processing:\")\n",
    "train_df_processed.info()\n",
    "print(\"\\nTest Data Info after processing:\")\n",
    "test_df_processed.info()\n",
    "\n",
    "\n",
    "# --- Model Training ---\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns - crucial after feature engineering if columns were added/dropped differently (shouldn't happen here but good practice)\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0 # Add missing columns to test set with default value (0)\n",
    "\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0 # Add missing columns to train set with default value (0) - less likely\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types for preprocessing\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')), # Impute missing numericals (Age, Income, Customer_Lifetime)\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')), # Impute missing categoricals (if any)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Use sparse_output=False for easier debugging if needed\n",
    "])\n",
    "\n",
    "# Create a column transformer to apply different pipelines to different columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough') # Keep any columns not specified (though there shouldn't be any here)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "# GradientBoostingClassifier often works well. random_state for reproducibility.\n",
    "# Consider tuning hyperparameters later using GridSearchCV or RandomizedSearchCV\n",
    "model = GradientBoostingClassifier(n_estimators=150, # Increased slightly\n",
    "                                 learning_rate=0.08, # Slightly decreased\n",
    "                                 max_depth=4,       # Increased slightly\n",
    "                                 subsample=0.8,     # Added subsampling\n",
    "                                 random_state=42)\n",
    "\n",
    "# Create the full pipeline: preprocess + model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training dataset\n",
    "print(\"\\nTraining the model...\")\n",
    "pipeline.fit(X, y)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Prediction ---\n",
    "\n",
    "# Predict on the preprocessed test data\n",
    "print(\"Predicting on test data...\")\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Submission File Generation ---\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = 'submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename}' created successfully.\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nPredicted target distribution:\\n{submission_df['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate on the training set (for sanity check, not a true performance measure)\n",
    "train_preds = pipeline.predict(X)\n",
    "train_accuracy = accuracy_score(y, train_preds)\n",
    "train_roc_auc = roc_auc_score(y, pipeline.predict_proba(X)[:, 1]) # Use probabilities for AUC\n",
    "print(f\"\\n--- Training Set Evaluation (Sanity Check) ---\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc:.4f}\")\n",
    "# print(\"Classification Report:\\n\", classification_report(y, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "213b1ffc-1bb9-4f55-aa9b-586aa725681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded successfully.\n",
      "Preprocessing complete.\n",
      "Customer_Lifetime treated as numerical.\n",
      "\n",
      "Starting 5-Fold Cross-Validation...\n",
      "Cross-Validation finished.\n",
      "\n",
      "--- Cross-Validation Results ---\n",
      "Accuracy Scores per Fold: [0.82484076 0.82165605 0.84345048 0.83067093 0.82428115]\n",
      "Mean Accuracy: 0.8290\n",
      "Std Dev Accuracy: 0.0078\n",
      "------------------------------\n",
      "ROC AUC Scores per Fold: [0.90629269 0.90923839 0.91769972 0.92064824 0.90418388]\n",
      "Mean ROC AUC: 0.9116\n",
      "Std Dev ROC AUC: 0.0064\n",
      "------------------------------\n",
      "\n",
      "NOTE: The scores above are estimates of generalization performance.\n",
      "For the final submission, you should train the pipeline on the *entire* training set (X, y)\n",
      "and then predict on the preprocessed test set.\n",
      "Example final training step (run this *after* CV and hyperparameter tuning):\n",
      "# pipeline.fit(X, y)\n",
      "# test_predictions = pipeline.predict(X_test) # Assuming X_test is preprocessed test data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed (e.g., from one-hot encoder)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Often related to sparse output default\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    # test_df = pd.read_csv(\"test.csv\") # Not needed for CV evaluation\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Training data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (same as before) ---\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years & Calculate Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except:\n",
    "        reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Process Dt_Customer & Calculate Customer_Lifetime\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1)\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    # Impute median *before* dropping Dt_Customer to handle NaTs properly\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median() # Calculate median only once\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True) # Keep originals for now\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing to Training Data ---\n",
    "train_df_processed = preprocess_data(train_df.copy(), is_train=True) # Use copy to be safe\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- Prepare Data for CV ---\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "\n",
    "# Identify column types (ensure this happens *after* preprocessing)\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Check if Customer_Lifetime needs explicit imputation placeholder if it wasn't numeric initially\n",
    "if 'Customer_Lifetime' in numerical_features:\n",
    "     print(\"Customer_Lifetime treated as numerical.\")\n",
    "else:\n",
    "     print(\"Warning: Customer_Lifetime might not be numerical after preprocessing.\")\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Steps ---\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# --- Define Model (Using the same parameters as your previous run) ---\n",
    "# You can adjust these parameters later based on CV results\n",
    "model = GradientBoostingClassifier(n_estimators=150,\n",
    "                                 learning_rate=0.08,\n",
    "                                 max_depth=4,\n",
    "                                 subsample=0.8,\n",
    "                                 random_state=42)\n",
    "\n",
    "# --- Create Full Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# --- Set up K-Fold Cross-Validation ---\n",
    "N_SPLITS = 5 # Number of folds (5 or 10 are common)\n",
    "RANDOM_STATE_KFOLD = 42 # For reproducible splits\n",
    "\n",
    "# Use StratifiedKFold to maintain target class distribution in each fold\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "print(f\"\\nStarting {N_SPLITS}-Fold Cross-Validation...\")\n",
    "\n",
    "# --- Perform Cross-Validation and Calculate Scores ---\n",
    "\n",
    "# Accuracy Scores\n",
    "accuracy_scores = cross_val_score(pipeline, X, y, cv=skf, scoring='accuracy', n_jobs=-1) # n_jobs=-1 uses all processors\n",
    "\n",
    "# ROC AUC Scores\n",
    "# Note: cross_val_score calculates ROC AUC based on predict_proba internally\n",
    "roc_auc_scores = cross_val_score(pipeline, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(\"Cross-Validation finished.\")\n",
    "\n",
    "# --- Report Results ---\n",
    "print(\"\\n--- Cross-Validation Results ---\")\n",
    "print(f\"Accuracy Scores per Fold: {accuracy_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Std Dev Accuracy: {np.std(accuracy_scores):.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"ROC AUC Scores per Fold: {roc_auc_scores}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Std Dev ROC AUC: {np.std(roc_auc_scores):.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Note on Final Training ---\n",
    "print(\"\\nNOTE: The scores above are estimates of generalization performance.\")\n",
    "print(\"For the final submission, you should train the pipeline on the *entire* training set (X, y)\")\n",
    "print(\"and then predict on the preprocessed test set.\")\n",
    "print(\"Example final training step (run this *after* CV and hyperparameter tuning):\")\n",
    "print(\"# pipeline.fit(X, y)\")\n",
    "print(\"# test_predictions = pipeline.predict(X_test) # Assuming X_test is preprocessed test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b825432d-4310-4f85-afed-46fddb49cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing complete.\n",
      "\n",
      "Starting RandomizedSearchCV with 50 iterations for roc_auc...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "RandomizedSearchCV finished.\n",
      "\n",
      "--- Hyperparameter Tuning Results ---\n",
      "Best Score (roc_auc): 0.9239\n",
      "Best Parameters:\n",
      "  classifier__subsample: 0.6\n",
      "  classifier__n_estimators: 200\n",
      "  classifier__min_samples_split: 20\n",
      "  classifier__min_samples_leaf: 20\n",
      "  classifier__max_features: sqrt\n",
      "  classifier__max_depth: 2\n",
      "  classifier__learning_rate: 0.08\n",
      "\n",
      "Training final model on the entire training set using best parameters...\n",
      "Final model training complete.\n",
      "Predicting on test data using the tuned model...\n",
      "Prediction complete.\n",
      "\n",
      "Submission file 'submission_tuned_gbc.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution:\n",
      "Target\n",
      "0    0.619614\n",
      "1    0.380386\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Tuned Model Training Set Evaluation ---\n",
      "Accuracy: 0.8666\n",
      "ROC AUC: 0.9539\n",
      "(Compare these to the initial overfit scores and the CV scores)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\") # Needed for final submission ID mapping\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (same as before) ---\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years & Calculate Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except:\n",
    "        reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Process Dt_Customer & Calculate Customer_Lifetime\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1) # Define global latest date from train set\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else: # Fallback if test is processed first somehow (shouldn't happen here)\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        print(f\"Warning: Using fallback latest date: {latest_date_to_use}\")\n",
    "\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True) # Keep originals for now\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing ---\n",
    "train_df_processed = preprocess_data(train_df.copy(), is_train=True)\n",
    "# Preprocess test data using the date derived from training data\n",
    "test_df_processed = preprocess_data(test_df.copy(), is_train=False, latest_date=global_latest_date)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data ---\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns after preprocessing (important!)\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# --- Define Preprocessing Steps ---\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Set sparse_output=True for large datasets if memory is an issue\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model ---\n",
    "# We will tune the parameters of this model\n",
    "base_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# --- Create Full Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', base_model) # Placeholder name 'classifier'\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for RandomizedSearchCV ---\n",
    "# Adjust ranges based on previous results and desired exploration\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [50, 80, 100, 150, 200], # Range around potentially good values\n",
    "    'classifier__learning_rate': [0.01, 0.02, 0.05, 0.08, 0.1, 0.15], # Wider range, including lower values\n",
    "    'classifier__max_depth': [2, 3, 4], # Focus on shallower trees to reduce overfitting\n",
    "    'classifier__min_samples_leaf': [5, 10, 15, 20], # Force more samples per leaf\n",
    "    'classifier__min_samples_split': [10, 20, 30], # Force more samples for a split\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9], # Explore subsampling ratios\n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.7, 0.8, None] # Limit features per split\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy (same as before) ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV ---\n",
    "N_ITER = 50 # Number of parameter settings to sample. Increase for more thorough search (e.g., 100), decrease for speed.\n",
    "SCORING_METRIC = 'roc_auc' # Optimize for ROC AUC, common for binary classification\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=N_ITER,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    random_state=42, # For reproducible search results\n",
    "    verbose=1 # Set to 1 or 2 to see progress\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV with {N_ITER} iterations for {SCORING_METRIC}...\")\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best Results ---\n",
    "print(\"\\n--- Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "# Nicely print the best parameters found\n",
    "best_params = random_search.best_params_\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final Model with Best Parameters ---\n",
    "print(\"\\nTraining final model on the entire training set using best parameters...\")\n",
    "# The best estimator found by RandomizedSearchCV is already fitted on the full data\n",
    "# if refit=True (default), but we fit it explicitly for clarity.\n",
    "# Alternatively, you could just use: best_pipeline = random_search.best_estimator_\n",
    "best_pipeline = pipeline # Start with the original pipeline structure\n",
    "best_pipeline.set_params(**best_params) # Set the best parameters found\n",
    "best_pipeline.fit(X, y)\n",
    "print(\"Final model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data ---\n",
    "print(\"Predicting on test data using the tuned model...\")\n",
    "test_predictions = best_pipeline.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions})\n",
    "submission_filename = 'submission_tuned_gbc.csv' # New filename\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename}' created successfully.\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nPredicted target distribution:\\n{submission_df['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* model on the training set\n",
    "train_preds_tuned = best_pipeline.predict(X)\n",
    "train_accuracy_tuned = accuracy_score(y, train_preds_tuned)\n",
    "train_roc_auc_tuned = roc_auc_score(y, best_pipeline.predict_proba(X)[:, 1])\n",
    "print(f\"\\n--- Tuned Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_tuned:.4f}\")\n",
    "print(\"(Compare these to the initial overfit scores and the CV scores)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b186ba-3cd1-48c8-a606-38cb1591633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa2610a1-4eb5-4c1c-a450-e67464349b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing complete.\n",
      "\n",
      "Numerical features (27): ['Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Age', 'Customer_Lifetime', 'Children', 'Total_Mnt', 'Total_Purchases', 'Total_CmpAccepted']\n",
      "Categorical features (2): ['Education', 'Marital_Status']\n",
      "\n",
      "Starting RandomizedSearchCV for XGBoost with 75 iterations for roc_auc...\n",
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n",
      "XGBoost RandomizedSearchCV finished.\n",
      "\n",
      "--- XGBoost Hyperparameter Tuning Results ---\n",
      "Best Score (roc_auc): 0.9236\n",
      "Best Parameters:\n",
      "  classifier__subsample: 0.8\n",
      "  classifier__reg_lambda: 0.5\n",
      "  classifier__reg_alpha: 0.01\n",
      "  classifier__n_estimators: 150\n",
      "  classifier__max_depth: 3\n",
      "  classifier__learning_rate: 0.05\n",
      "  classifier__gamma: 0.1\n",
      "  classifier__colsample_bytree: 0.6\n",
      "\n",
      "Training final XGBoost model on the entire training set using best parameters...\n",
      "Final XGBoost model training complete.\n",
      "Predicting on test data using the tuned XGBoost model...\n",
      "Prediction complete.\n",
      "\n",
      "Submission file 'submission_tuned_xgb.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (XGBoost):\n",
      "Target\n",
      "0    0.627043\n",
      "1    0.372957\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Tuned XGBoost Model Training Set Evaluation ---\n",
      "Accuracy: 0.8890\n",
      "ROC AUC: 0.9655\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer # Ensure make_scorer is imported if needed, though cross_val_score handles it\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "import xgboost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    print(\"XGBoost not found. Please install it using: pip install xgboost\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (Identical to previous step) ---\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years & Calculate Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        # Attempt to get reference year from Dt_Customer\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True) # Drop temporary column\n",
    "    except Exception as e: # Broad exception for safety if Dt_Customer is missing or unparseable\n",
    "        print(f\"Warning: Could not parse Dt_Customer for reference year. Using current year. Error: {e}\")\n",
    "        reference_year = current_year\n",
    "\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Process Dt_Customer & Calculate Customer_Lifetime\n",
    "    # Convert 'Dt_Customer' to datetime, coercing errors\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "\n",
    "    # Find the latest date for calculating tenure if not provided (from training set)\n",
    "    if is_train:\n",
    "        global global_latest_date # Store latest date from training set globally\n",
    "        # Handle case where all Dt_Customer might be NaT after coercion\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            global_latest_date = valid_dates.max() + datetime.timedelta(days=1)\n",
    "        else:\n",
    "            # Fallback if no valid dates found in training set\n",
    "            global_latest_date = datetime.datetime(reference_year + 1, 1, 1)\n",
    "            print(f\"Warning: No valid Dt_Customer found in training set. Using fallback latest date: {global_latest_date}\")\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        # Fallback if called on test set first or global_latest_date isn't set\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        print(f\"Warning: Using fallback latest date for test set: {latest_date_to_use}\")\n",
    "\n",
    "    # Calculate Customer_Lifetime (Tenure in days)\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    # Impute median *before* dropping Dt_Customer to handle NaTs properly\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # Keep original Kidhome/Teenhome for now, might be useful features\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing ---\n",
    "train_df_processed = preprocess_data(train_df.copy(), is_train=True)\n",
    "# Check if global_latest_date was set correctly\n",
    "if 'global_latest_date' not in globals():\n",
    "     print(\"Error: global_latest_date not set during training preprocessing. Exiting.\")\n",
    "     # Handle this case appropriately, maybe define a default or raise error\n",
    "     # For now, let's set a default, but ideally the training data processing should succeed\n",
    "     global_latest_date = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "     print(f\"Using current date as fallback for global_latest_date: {global_latest_date}\")\n",
    "\n",
    "test_df_processed = preprocess_data(test_df.copy(), is_train=False, latest_date=global_latest_date)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data ---\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns after preprocessing\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# --- Define Preprocessing Steps (Same as before) ---\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Set sparse=True for large data if needed\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model: XGBoost ---\n",
    "# Use_label_encoder=False is recommended for newer XGBoost versions\n",
    "# eval_metric='logloss' or 'auc' are common for binary classification\n",
    "base_model_xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# --- Create Full Pipeline with XGBoost ---\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', base_model_xgb) # Step name remains 'classifier'\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for XGBoost RandomizedSearchCV ---\n",
    "# These ranges are starting points; adjust based on results or computational budget\n",
    "param_dist_xgb = {\n",
    "    'classifier__n_estimators': [100, 150, 200, 300, 400], # Number of boosting rounds\n",
    "    'classifier__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15], # Step size shrinkage\n",
    "    'classifier__max_depth': [2, 3, 4, 5], # Maximum depth of a tree\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Fraction of samples used per tree\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Fraction of features used per tree\n",
    "    'classifier__gamma': [0, 0.1, 0.2, 0.5], # Minimum loss reduction required to make a further partition\n",
    "    'classifier__reg_alpha': [0, 0.001, 0.01, 0.1], # L1 regularization term\n",
    "    'classifier__reg_lambda': [0.5, 1, 1.5] # L2 regularization term (default is 1)\n",
    "    # Add 'min_child_weight': [1, 3, 5] if needed (minimum sum of instance weight needed in a child)\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy (same as before) ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for XGBoost ---\n",
    "N_ITER_XGB = 75 # Increase iterations for potentially better results (vs 50 for GBC)\n",
    "SCORING_METRIC = 'roc_auc' # Optimize for ROC AUC\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=pipeline_xgb,\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=N_ITER_XGB,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    random_state=42, # For reproducible search results\n",
    "    verbose=1 # Set to 1 or 2 to see progress\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for XGBoost with {N_ITER_XGB} iterations for {SCORING_METRIC}...\")\n",
    "random_search_xgb.fit(X, y)\n",
    "print(\"XGBoost RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best XGBoost Results ---\n",
    "print(\"\\n--- XGBoost Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_xgb.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_xgb = random_search_xgb.best_params_\n",
    "for param, value in best_params_xgb.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final XGBoost Model with Best Parameters ---\n",
    "print(\"\\nTraining final XGBoost model on the entire training set using best parameters...\")\n",
    "# The best estimator is automatically refit on the whole training data by RandomizedSearchCV\n",
    "best_pipeline_xgb = random_search_xgb.best_estimator_\n",
    "# Explicit refit just to be sure (Optional, default behavior of RS CV is refit=True)\n",
    "# best_pipeline_xgb.fit(X, y)\n",
    "print(\"Final XGBoost model training complete.\")\n",
    "\n",
    "\n",
    "# --- Predict on Test Data with Tuned XGBoost ---\n",
    "print(\"Predicting on test data using the tuned XGBoost model...\")\n",
    "test_predictions_xgb = best_pipeline_xgb.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for XGBoost ---\n",
    "submission_df_xgb = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_xgb})\n",
    "submission_filename_xgb = 'submission_tuned_xgb.csv' # New filename\n",
    "submission_df_xgb.to_csv(submission_filename_xgb, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_xgb}' created successfully.\")\n",
    "print(submission_df_xgb.head())\n",
    "print(f\"\\nPredicted target distribution (XGBoost):\\n{submission_df_xgb['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* XGBoost model on the training set\n",
    "train_preds_xgb_tuned = best_pipeline_xgb.predict(X)\n",
    "train_accuracy_xgb_tuned = accuracy_score(y, train_preds_xgb_tuned)\n",
    "train_roc_auc_xgb_tuned = roc_auc_score(y, best_pipeline_xgb.predict_proba(X)[:, 1])\n",
    "print(f\"\\n--- Tuned XGBoost Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_xgb_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_xgb_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b64d34d-c10d-47d5-b3dc-319304ec13e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "V2 Preprocessing complete.\n",
      "\n",
      "V2 Numerical features (35): ['Income', 'Kidhome', 'Teenhome', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Age', 'Enroll_Month', 'Enroll_Year', 'Enroll_DayOfWeek', 'Customer_Lifetime', 'Children', 'Total_Mnt', 'Total_Purchases', 'Total_CmpAccepted', 'Wine_Ratio', 'Meat_Ratio', 'Fruit_Ratio', 'Income_per_Person', 'Spending_per_Purchase']\n",
      "V2 Categorical features (2): ['Education', 'Marital_Status']\n",
      "\n",
      "Starting RandomizedSearchCV for GBC (V2 Features) with 50 iterations for roc_auc...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "GBC V2 RandomizedSearchCV finished.\n",
      "\n",
      "--- GBC V2 Hyperparameter Tuning Results ---\n",
      "Best Score (roc_auc): 0.9214\n",
      "Best Parameters:\n",
      "  classifier__subsample: 0.7\n",
      "  classifier__n_estimators: 300\n",
      "  classifier__min_samples_split: 20\n",
      "  classifier__min_samples_leaf: 20\n",
      "  classifier__max_features: log2\n",
      "  classifier__max_depth: 2\n",
      "  classifier__learning_rate: 0.05\n",
      "\n",
      "Training final GBC V2 model on the entire training set using best parameters...\n",
      "Final GBC V2 model training complete.\n",
      "Predicting on test data using the tuned GBC V2 model...\n",
      "Prediction complete.\n",
      "\n",
      "Submission file 'submission_tuned_gbc_v2_features.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (GBC V2 Features):\n",
      "Target\n",
      "0    0.609212\n",
      "1    0.390788\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Tuned GBC V2 Model Training Set Evaluation ---\n",
      "Accuracy: 0.8736\n",
      "ROC AUC: 0.9564\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Keep GBC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer # Added FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (MODIFIED) ---\n",
    "def preprocess_data_v2(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering (v2) and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # --- Original Preprocessing ---\n",
    "    # 1. Handle Birth Year & Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not parse Dt_Customer for reference year. Using current year. Error: {e}\")\n",
    "        reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True) # Keep Age\n",
    "\n",
    "    # 2. Process Dt_Customer & Lifetime + Extract Date Features\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v2 # Use a new global var name if running in same session\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            global_latest_date_v2 = valid_dates.max() + datetime.timedelta(days=1)\n",
    "        else:\n",
    "            global_latest_date_v2 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "            print(f\"Warning: No valid Dt_Customer. Using fallback latest date: {global_latest_date_v2}\")\n",
    "        latest_date_to_use = global_latest_date_v2\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "\n",
    "    # --> NEW: Extract Date Features BEFORE calculating lifetime and dropping\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer'].dt.dayofweek\n",
    "    # Impute NaNs in date features (e.g., with mode or median year/month)\n",
    "    df_processed['Enroll_Month'].fillna(df_processed['Enroll_Month'].mode()[0], inplace=True)\n",
    "    df_processed['Enroll_Year'].fillna(df_processed['Enroll_Year'].median(), inplace=True)\n",
    "    df_processed['Enroll_DayOfWeek'].fillna(df_processed['Enroll_DayOfWeek'].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True) # Now drop Dt_Customer\n",
    "\n",
    "    # 3. Simplify Marital Status (Keeping original for now - let's test)\n",
    "    # df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({ ... }) # Keep original\n",
    "\n",
    "    # 4. Simplify Education (Keeping original for now - let's test)\n",
    "    # df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'}) # Keep original\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    # Impute NaNs in spending columns *before* summing (using 0 or median)\n",
    "    for col in mnt_cols:\n",
    "        df_processed[col] = df_processed[col].fillna(0) # Simple imputation with 0 for spending\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # --- NEW Features ---\n",
    "    # Ratio Features (handle division by zero)\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    # Replace inf values that might result from 0/0\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "\n",
    "    # Income related (handle division by zero and potential NaNs in Income)\n",
    "    # Impute Income NaNs *before* using it in calculations\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    # Add 1 to avoid division by zero if Children=0 and partner=1 (or single=1)\n",
    "    num_people = df_processed['Children'] + df_processed['Marital_Status'].apply(lambda x: 1 if x=='Single' else 2) # Simple adult estimate\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / num_people.replace(0, 1)).fillna(0) # Replace 0 people with 1\n",
    "\n",
    "\n",
    "    # Spending per Purchase (handle division by zero)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Total_Purchases'].replace(0, 1)).fillna(0)\n",
    "\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply NEW Preprocessing ---\n",
    "train_df_processed_v2 = preprocess_data_v2(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v2' not in globals(): # Check the new global var\n",
    "     global_latest_date_v2 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "     print(f\"Error: global_latest_date_v2 not set. Using fallback: {global_latest_date_v2}\")\n",
    "test_df_processed_v2 = preprocess_data_v2(test_df.copy(), is_train=False, latest_date=global_latest_date_v2)\n",
    "print(\"V2 Preprocessing complete.\")\n",
    "\n",
    "# --- Prepare Data (using v2 processed data) ---\n",
    "X_v2 = train_df_processed_v2.drop(['ID', 'Target'], axis=1)\n",
    "y_v2 = train_df_processed_v2['Target'] # Target remains the same\n",
    "X_test_v2 = test_df_processed_v2.drop('ID', axis=1)\n",
    "\n",
    "# Align columns after V2 preprocessing\n",
    "train_cols_v2 = X_v2.columns\n",
    "test_cols_v2 = X_test_v2.columns\n",
    "\n",
    "missing_in_test_v2 = set(train_cols_v2) - set(test_cols_v2)\n",
    "for c in missing_in_test_v2:\n",
    "    X_test_v2[c] = 0\n",
    "missing_in_train_v2 = set(test_cols_v2) - set(train_cols_v2)\n",
    "for c in missing_in_train_v2:\n",
    "    X_v2[c] = 0\n",
    "\n",
    "X_test_v2 = X_test_v2[train_cols_v2] # Ensure order is the same\n",
    "\n",
    "# --- Define Preprocessing Steps (Potentially updated if features changed type) ---\n",
    "numerical_features_v2 = X_v2.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2 = X_v2.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nV2 Numerical features ({len(numerical_features_v2)}): {numerical_features_v2}\")\n",
    "print(f\"V2 Categorical features ({len(categorical_features_v2)}): {categorical_features_v2}\")\n",
    "\n",
    "\n",
    "# Log transformer function\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=False) # validate=False to handle 0s after log1p\n",
    "\n",
    "# Update Numerical Pipeline to include Log Transform for specific skewed features\n",
    "# Identify potentially skewed features (Income, Spending)\n",
    "skewed_num_features = ['Income', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
    "                       'MntSweetProducts', 'MntGoldProds', 'Total_Mnt', 'Income_per_Person', 'Spending_per_Purchase']\n",
    "# Make sure these features actually exist after preprocessing\n",
    "skewed_num_features = [f for f in skewed_num_features if f in numerical_features_v2]\n",
    "other_num_features = [f for f in numerical_features_v2 if f not in skewed_num_features]\n",
    "\n",
    "\n",
    "numerical_pipeline_v2 = Pipeline([\n",
    "    # Impute FIRST\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    # Apply log transform only to skewed columns (using ColumnTransformer within Pipeline - tricky!)\n",
    "    # Easier approach: Apply log transform in preprocess_data_v2 or handle separately if needed.\n",
    "    # For simplicity here, let's apply StandardScaler to all imputed numericals.\n",
    "    # Consider log transform within preprocess_data_v2 if it proves beneficial.\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline_v2 = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Update Preprocessor\n",
    "preprocessor_v2 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2, numerical_features_v2), # Apply updated pipeline to all numerical\n",
    "    ('cat', categorical_pipeline_v2, categorical_features_v2)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model: Gradient Boosting (Retuning this one) ---\n",
    "base_model_gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# --- Create Full Pipeline with GBC V2 ---\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('classifier', base_model_gbc)\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for GBC RandomizedSearchCV (Centered around previous best) ---\n",
    "param_dist_gbc_v2 = {\n",
    "    'classifier__n_estimators': [150, 200, 250, 300], # Explore higher values slightly\n",
    "    'classifier__learning_rate': [0.02, 0.05, 0.08, 0.1], # Narrower range around 0.08\n",
    "    'classifier__max_depth': [2, 3], # Keep focusing on shallow trees\n",
    "    'classifier__min_samples_leaf': [15, 20, 25], # Stay around the previous best\n",
    "    'classifier__min_samples_split': [15, 20, 30], # Stay around the previous best\n",
    "    'classifier__subsample': [0.5, 0.6, 0.7], # Explore around 0.6\n",
    "    'classifier__max_features': ['sqrt', 'log2'] # Keep simpler options\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy (same as before) ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for GBC V2 ---\n",
    "N_ITER_GBC_V2 = 50 # Number of iterations for retuning\n",
    "SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "random_search_gbc_v2 = RandomizedSearchCV(\n",
    "    estimator=pipeline_gbc_v2,\n",
    "    param_distributions=param_dist_gbc_v2,\n",
    "    n_iter=N_ITER_GBC_V2,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for GBC (V2 Features) with {N_ITER_GBC_V2} iterations for {SCORING_METRIC}...\")\n",
    "random_search_gbc_v2.fit(X_v2, y_v2) # Use V2 features and original target\n",
    "print(\"GBC V2 RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best GBC V2 Results ---\n",
    "print(\"\\n--- GBC V2 Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_gbc_v2.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_gbc_v2 = random_search_gbc_v2.best_params_\n",
    "for param, value in best_params_gbc_v2.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final GBC V2 Model with Best Parameters ---\n",
    "print(\"\\nTraining final GBC V2 model on the entire training set using best parameters...\")\n",
    "best_pipeline_gbc_v2 = random_search_gbc_v2.best_estimator_\n",
    "print(\"Final GBC V2 model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Tuned GBC V2 ---\n",
    "print(\"Predicting on test data using the tuned GBC V2 model...\")\n",
    "test_predictions_gbc_v2 = best_pipeline_gbc_v2.predict(X_test_v2) # Use V2 test features\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for GBC V2 ---\n",
    "submission_df_gbc_v2 = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_gbc_v2})\n",
    "submission_filename_gbc_v2 = 'submission_tuned_gbc_v2_features.csv' # New filename\n",
    "submission_df_gbc_v2.to_csv(submission_filename_gbc_v2, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_gbc_v2}' created successfully.\")\n",
    "print(submission_df_gbc_v2.head())\n",
    "print(f\"\\nPredicted target distribution (GBC V2 Features):\\n{submission_df_gbc_v2['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* GBC V2 model on the training set\n",
    "train_preds_gbc_v2_tuned = best_pipeline_gbc_v2.predict(X_v2)\n",
    "train_accuracy_gbc_v2_tuned = accuracy_score(y_v2, train_preds_gbc_v2_tuned)\n",
    "train_roc_auc_gbc_v2_tuned = roc_auc_score(y_v2, best_pipeline_gbc_v2.predict_proba(X_v2)[:, 1])\n",
    "print(f\"\\n--- Tuned GBC V2 Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_gbc_v2_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v2_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a048a4e-5af5-4994-8a12-e92a278e5d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "V1 Preprocessing complete.\n",
      "V2 Preprocessing complete.\n",
      "Training Model 1 (GBC V1)...\n",
      "Model 1 training complete.\n",
      "Training Model 2 (GBC V2)...\n",
      "Model 2 training complete.\n",
      "Predicting probabilities...\n",
      "Averaging predictions...\n",
      "\n",
      "Submission file 'submission_ensemble_gbc_v1_v2.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (Ensemble):\n",
      "Target\n",
      "0    0.616642\n",
      "1    0.383358\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Model 1 (GBC V1) Training Set Eval ---\n",
      "ROC AUC: 0.9547\n",
      "\n",
      "--- Model 2 (GBC V2) Training Set Eval ---\n",
      "ROC AUC: 0.9569\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold # Keep for reference if needed later\n",
    "# Removed RandomizedSearchCV as we are using pre-found params\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions (V1 and V2) ---\n",
    "\n",
    "# Function V1 (leading to 0.845 score)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    df_processed = df.copy()\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0) # Impute before sum\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    # Impute Income (might be needed if not done before FE)\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    # Impute Age (might be needed if Year_Birth had NaNs)\n",
    "    age_median = df_processed['Age'].median()\n",
    "    df_processed['Age'].fillna(age_median, inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# Function V2 (leading to 0.848 score) - simplified, assuming it's the same as last run\n",
    "def preprocess_data_v2(df, is_train=True, latest_date=None):\n",
    "    df_processed = df.copy()\n",
    "    # --- Previous steps: Age, Lifetime, Date Features ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v2\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        global_latest_date_v2 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v2\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer'].dt.month.fillna(df_processed['Dt_Customer'].dt.month.mode()[0])\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer'].dt.year.fillna(df_processed['Dt_Customer'].dt.year.median())\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer'].dt.dayofweek.fillna(df_processed['Dt_Customer'].dt.dayofweek.mode()[0])\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # --- V2 specific additions / kept originals ---\n",
    "    # Marital_Status kept original\n",
    "    # Education kept original\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0)\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    num_people = df_processed['Children'] + df_processed['Marital_Status'].apply(lambda x: 1 if x in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / num_people.replace(0, 1)).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Total_Purchases'].replace(0, 1)).fillna(0)\n",
    "    # Impute Age (might be needed if Year_Birth had NaNs)\n",
    "    age_median = df_processed['Age'].median()\n",
    "    df_processed['Age'].fillna(age_median, inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing V1 ---\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "\n",
    "# --- Apply Preprocessing V2 ---\n",
    "train_df_processed_v2 = preprocess_data_v2(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v2' not in globals(): global_latest_date_v2 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v2 = preprocess_data_v2(test_df.copy(), is_train=False, latest_date=global_latest_date_v2)\n",
    "print(\"V2 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V1 ---\n",
    "X_v1 = train_df_processed_v1.drop(['ID', 'Target'], axis=1)\n",
    "y_v1 = train_df_processed_v1['Target']\n",
    "X_test_v1 = test_df_processed_v1.drop('ID', axis=1)\n",
    "# Align\n",
    "train_cols_v1 = X_v1.columns\n",
    "missing_in_test_v1 = set(train_cols_v1) - set(X_test_v1.columns)\n",
    "for c in missing_in_test_v1: X_test_v1[c] = 0\n",
    "missing_in_train_v1 = set(X_test_v1.columns) - set(train_cols_v1)\n",
    "for c in missing_in_train_v1: X_v1[c] = 0\n",
    "X_test_v1 = X_test_v1[train_cols_v1]\n",
    "\n",
    "# --- Prepare Data V2 ---\n",
    "X_v2 = train_df_processed_v2.drop(['ID', 'Target'], axis=1)\n",
    "y_v2 = train_df_processed_v2['Target']\n",
    "X_test_v2 = test_df_processed_v2.drop('ID', axis=1)\n",
    "# Align\n",
    "train_cols_v2 = X_v2.columns\n",
    "missing_in_test_v2 = set(train_cols_v2) - set(X_test_v2.columns)\n",
    "for c in missing_in_test_v2: X_test_v2[c] = 0\n",
    "missing_in_train_v2 = set(X_test_v2.columns) - set(train_cols_v2)\n",
    "for c in missing_in_train_v2: X_v2[c] = 0\n",
    "X_test_v2 = X_test_v2[train_cols_v2]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines (Need separate ones for V1 and V2 features) ---\n",
    "\n",
    "# Pipeline V1 Definition\n",
    "numerical_features_v1 = X_v1.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v1 = X_v1.select_dtypes(exclude=np.number).columns.tolist()\n",
    "numerical_pipeline_v1 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v1 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v1 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v1, numerical_features_v1),\n",
    "    ('cat', categorical_pipeline_v1, categorical_features_v1)], remainder='passthrough')\n",
    "\n",
    "# Pipeline V2 Definition\n",
    "numerical_features_v2 = X_v2.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2 = X_v2.select_dtypes(exclude=np.number).columns.tolist()\n",
    "numerical_pipeline_v2 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v2 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v2 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2, numerical_features_v2),\n",
    "    ('cat', categorical_pipeline_v2, categorical_features_v2)], remainder='passthrough')\n",
    "\n",
    "# --- Define BEST Hyperparameters found previously ---\n",
    "\n",
    "# Best parameters for GBC with V1 features (resulted in 0.845 Kaggle score)\n",
    "# Note: These are the params *you reported* finding previously. Double-check if needed.\n",
    "best_params_gbc_v1 = {\n",
    "    'classifier__subsample': 0.6,\n",
    "    'classifier__n_estimators': 200,\n",
    "    'classifier__min_samples_split': 20,\n",
    "    'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'sqrt',\n",
    "    'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.08\n",
    "}\n",
    "\n",
    "# Best parameters for GBC with V2 features (resulted in 0.848 Kaggle score)\n",
    "best_params_gbc_v2 = {\n",
    "    'classifier__subsample': 0.7,\n",
    "    'classifier__n_estimators': 300,\n",
    "    'classifier__min_samples_split': 20,\n",
    "    'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'log2',\n",
    "    'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.05\n",
    "}\n",
    "\n",
    "\n",
    "# --- Build and Train Model 1 (GBC V1) ---\n",
    "print(\"Training Model 1 (GBC V1)...\")\n",
    "pipeline_gbc_v1 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v1),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42)) # Base model\n",
    "])\n",
    "pipeline_gbc_v1.set_params(**best_params_gbc_v1) # Apply best params\n",
    "pipeline_gbc_v1.fit(X_v1, y_v1)\n",
    "print(\"Model 1 training complete.\")\n",
    "\n",
    "# --- Build and Train Model 2 (GBC V2) ---\n",
    "print(\"Training Model 2 (GBC V2)...\")\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42)) # Base model\n",
    "])\n",
    "pipeline_gbc_v2.set_params(**best_params_gbc_v2) # Apply best params\n",
    "pipeline_gbc_v2.fit(X_v2, y_v2)\n",
    "print(\"Model 2 training complete.\")\n",
    "\n",
    "\n",
    "# --- Predict Probabilities on Test Set ---\n",
    "print(\"Predicting probabilities...\")\n",
    "# IMPORTANT: Use the correctly preprocessed test set for each model!\n",
    "probs_gbc_v1 = pipeline_gbc_v1.predict_proba(X_test_v1)[:, 1]\n",
    "probs_gbc_v2 = pipeline_gbc_v2.predict_proba(X_test_v2)[:, 1]\n",
    "\n",
    "# --- Ensemble Averaging ---\n",
    "print(\"Averaging predictions...\")\n",
    "# Simple average (you could also try weighted average if desired)\n",
    "avg_probs = (probs_gbc_v1 + probs_gbc_v2) / 2\n",
    "\n",
    "# Convert probabilities to 0/1 using 0.5 threshold\n",
    "final_predictions = (avg_probs >= 0.5).astype(int)\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df_ensemble = pd.DataFrame({'ID': test_df['ID'], 'Target': final_predictions})\n",
    "submission_filename_ensemble = 'submission_ensemble_gbc_v1_v2.csv'\n",
    "submission_df_ensemble.to_csv(submission_filename_ensemble, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_ensemble}' created successfully.\")\n",
    "print(submission_df_ensemble.head())\n",
    "print(f\"\\nPredicted target distribution (Ensemble):\\n{submission_df_ensemble['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate component models on training data (as a rough check)\n",
    "train_preds_gbc_v1 = pipeline_gbc_v1.predict(X_v1)\n",
    "train_roc_auc_gbc_v1 = roc_auc_score(y_v1, pipeline_gbc_v1.predict_proba(X_v1)[:, 1])\n",
    "print(f\"\\n--- Model 1 (GBC V1) Training Set Eval ---\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v1:.4f}\")\n",
    "\n",
    "train_preds_gbc_v2 = pipeline_gbc_v2.predict(X_v2)\n",
    "train_roc_auc_gbc_v2 = roc_auc_score(y_v2, pipeline_gbc_v2.predict_proba(X_v2)[:, 1])\n",
    "print(f\"\\n--- Model 2 (GBC V2) Training Set Eval ---\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcbff6-6ab7-4696-bb75-006ec0c16a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
