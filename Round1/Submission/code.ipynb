{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96208800-22cf-4090-9174-9071d646af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed (e.g., from one-hot encoder)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Often related to sparse output default\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# --- Feature Engineering & Preprocessing ---\n",
    "\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years\n",
    "    current_year = datetime.datetime.now().year\n",
    "    # Use a reasonable reference year based on Dt_Customer if available, otherwise current year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except: # Handle cases where Dt_Customer might not exist or be parseable easily\n",
    "        reference_year = current_year\n",
    "\n",
    "    # Replace very old birth years (e.g., < 1910) with NaN to be imputed later\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "\n",
    "    # Calculate Age (handle potential NaNs in Year_Birth temporarily)\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 2. Process Dt_Customer\n",
    "    # Convert 'Dt_Customer' to datetime, coercing errors\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "\n",
    "    # Find the latest date for calculating tenure if not provided (from training set)\n",
    "    if is_train:\n",
    "        global global_latest_date # Store latest date from training set globally\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1)\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        # Fallback if called on test set first or global_latest_date isn't set\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1) # Use start of next year as reference\n",
    "        print(f\"Warning: Using fallback latest date: {latest_date_to_use}\")\n",
    "\n",
    "\n",
    "    # Calculate Customer_Lifetime (Tenure in days)\n",
    "    # Handle potential NaT dates resulting from coerce errors\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    df_processed['Customer_Lifetime'].fillna(df_processed['Customer_Lifetime'].median(), inplace=True) # Impute NaNs created by NaT\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    # Consolidate categories\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner',\n",
    "        'Together': 'Partner',\n",
    "        'Absurd': 'Single',\n",
    "        'Alone': 'Single',\n",
    "        'YOLO': 'Single',\n",
    "        'Widow': 'Single',\n",
    "        'Divorced':'Single'\n",
    "         }) # Grouping Married/Together and others into Single for simplicity\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'}) # Group '2n Cycle' with 'Master'\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # Optionally drop original columns if 'Children' is deemed sufficient\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns (identified during EDA)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore') # errors='ignore' in case they were already dropped\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# Preprocess training data\n",
    "train_df_processed = preprocess_data(train_df, is_train=True)\n",
    "# Preprocess test data using the latest date from training data\n",
    "test_df_processed = preprocess_data(test_df, is_train=False, latest_date=global_latest_date)\n",
    "\n",
    "print(\"Feature engineering complete.\")\n",
    "print(\"\\nTrain Data Info after processing:\")\n",
    "train_df_processed.info()\n",
    "print(\"\\nTest Data Info after processing:\")\n",
    "test_df_processed.info()\n",
    "\n",
    "\n",
    "# --- Model Training ---\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns - crucial after feature engineering if columns were added/dropped differently (shouldn't happen here but good practice)\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0 # Add missing columns to test set with default value (0)\n",
    "\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0 # Add missing columns to train set with default value (0) - less likely\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types for preprocessing\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')), # Impute missing numericals (Age, Income, Customer_Lifetime)\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')), # Impute missing categoricals (if any)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Use sparse_output=False for easier debugging if needed\n",
    "])\n",
    "\n",
    "# Create a column transformer to apply different pipelines to different columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough') # Keep any columns not specified (though there shouldn't be any here)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "# GradientBoostingClassifier often works well. random_state for reproducibility.\n",
    "# Consider tuning hyperparameters later using GridSearchCV or RandomizedSearchCV\n",
    "model = GradientBoostingClassifier(n_estimators=150, # Increased slightly\n",
    "                                 learning_rate=0.08, # Slightly decreased\n",
    "                                 max_depth=4,       # Increased slightly\n",
    "                                 subsample=0.8,     # Added subsampling\n",
    "                                 random_state=42)\n",
    "\n",
    "# Create the full pipeline: preprocess + model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training dataset\n",
    "print(\"\\nTraining the model...\")\n",
    "pipeline.fit(X, y)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Prediction ---\n",
    "\n",
    "# Predict on the preprocessed test data\n",
    "print(\"Predicting on test data...\")\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Submission File Generation ---\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = 'submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename}' created successfully.\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nPredicted target distribution:\\n{submission_df['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate on the training set (for sanity check, not a true performance measure)\n",
    "train_preds = pipeline.predict(X)\n",
    "train_accuracy = accuracy_score(y, train_preds)\n",
    "train_roc_auc = roc_auc_score(y, pipeline.predict_proba(X)[:, 1]) # Use probabilities for AUC\n",
    "print(f\"\\n--- Training Set Evaluation (Sanity Check) ---\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc:.4f}\")\n",
    "# print(\"Classification Report:\\n\", classification_report(y, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b1ffc-1bb9-4f55-aa9b-586aa725681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed (e.g., from one-hot encoder)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Often related to sparse output default\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    # test_df = pd.read_csv(\"test.csv\") # Not needed for CV evaluation\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Training data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (same as before) ---\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years & Calculate Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except:\n",
    "        reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Process Dt_Customer & Calculate Customer_Lifetime\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1)\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    # Impute median *before* dropping Dt_Customer to handle NaTs properly\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median() # Calculate median only once\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True) # Keep originals for now\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing to Training Data ---\n",
    "train_df_processed = preprocess_data(train_df.copy(), is_train=True) # Use copy to be safe\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- Prepare Data for CV ---\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "\n",
    "# Identify column types (ensure this happens *after* preprocessing)\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Check if Customer_Lifetime needs explicit imputation placeholder if it wasn't numeric initially\n",
    "if 'Customer_Lifetime' in numerical_features:\n",
    "     print(\"Customer_Lifetime treated as numerical.\")\n",
    "else:\n",
    "     print(\"Warning: Customer_Lifetime might not be numerical after preprocessing.\")\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Steps ---\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# --- Define Model (Using the same parameters as your previous run) ---\n",
    "# You can adjust these parameters later based on CV results\n",
    "model = GradientBoostingClassifier(n_estimators=150,\n",
    "                                 learning_rate=0.08,\n",
    "                                 max_depth=4,\n",
    "                                 subsample=0.8,\n",
    "                                 random_state=42)\n",
    "\n",
    "# --- Create Full Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# --- Set up K-Fold Cross-Validation ---\n",
    "N_SPLITS = 5 # Number of folds (5 or 10 are common)\n",
    "RANDOM_STATE_KFOLD = 42 # For reproducible splits\n",
    "\n",
    "# Use StratifiedKFold to maintain target class distribution in each fold\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "print(f\"\\nStarting {N_SPLITS}-Fold Cross-Validation...\")\n",
    "\n",
    "# --- Perform Cross-Validation and Calculate Scores ---\n",
    "\n",
    "# Accuracy Scores\n",
    "accuracy_scores = cross_val_score(pipeline, X, y, cv=skf, scoring='accuracy', n_jobs=-1) # n_jobs=-1 uses all processors\n",
    "\n",
    "# ROC AUC Scores\n",
    "# Note: cross_val_score calculates ROC AUC based on predict_proba internally\n",
    "roc_auc_scores = cross_val_score(pipeline, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(\"Cross-Validation finished.\")\n",
    "\n",
    "# --- Report Results ---\n",
    "print(\"\\n--- Cross-Validation Results ---\")\n",
    "print(f\"Accuracy Scores per Fold: {accuracy_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Std Dev Accuracy: {np.std(accuracy_scores):.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"ROC AUC Scores per Fold: {roc_auc_scores}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Std Dev ROC AUC: {np.std(roc_auc_scores):.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Note on Final Training ---\n",
    "print(\"\\nNOTE: The scores above are estimates of generalization performance.\")\n",
    "print(\"For the final submission, you should train the pipeline on the *entire* training set (X, y)\")\n",
    "print(\"and then predict on the preprocessed test set.\")\n",
    "print(\"Example final training step (run this *after* CV and hyperparameter tuning):\")\n",
    "print(\"# pipeline.fit(X, y)\")\n",
    "print(\"# test_predictions = pipeline.predict(X_test) # Assuming X_test is preprocessed test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825432d-4310-4f85-afed-46fddb49cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\") # Needed for final submission ID mapping\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (same as before) ---\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years & Calculate Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except:\n",
    "        reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Process Dt_Customer & Calculate Customer_Lifetime\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1) # Define global latest date from train set\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else: # Fallback if test is processed first somehow (shouldn't happen here)\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        print(f\"Warning: Using fallback latest date: {latest_date_to_use}\")\n",
    "\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True) # Keep originals for now\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing ---\n",
    "train_df_processed = preprocess_data(train_df.copy(), is_train=True)\n",
    "# Preprocess test data using the date derived from training data\n",
    "test_df_processed = preprocess_data(test_df.copy(), is_train=False, latest_date=global_latest_date)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data ---\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns after preprocessing (important!)\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# --- Define Preprocessing Steps ---\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Set sparse_output=True for large datasets if memory is an issue\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model ---\n",
    "# We will tune the parameters of this model\n",
    "base_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# --- Create Full Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', base_model) # Placeholder name 'classifier'\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for RandomizedSearchCV ---\n",
    "# Adjust ranges based on previous results and desired exploration\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [50, 80, 100, 150, 200], # Range around potentially good values\n",
    "    'classifier__learning_rate': [0.01, 0.02, 0.05, 0.08, 0.1, 0.15], # Wider range, including lower values\n",
    "    'classifier__max_depth': [2, 3, 4], # Focus on shallower trees to reduce overfitting\n",
    "    'classifier__min_samples_leaf': [5, 10, 15, 20], # Force more samples per leaf\n",
    "    'classifier__min_samples_split': [10, 20, 30], # Force more samples for a split\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9], # Explore subsampling ratios\n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.7, 0.8, None] # Limit features per split\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy (same as before) ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV ---\n",
    "N_ITER = 50 # Number of parameter settings to sample. Increase for more thorough search (e.g., 100), decrease for speed.\n",
    "SCORING_METRIC = 'roc_auc' # Optimize for ROC AUC, common for binary classification\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=N_ITER,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    random_state=42, # For reproducible search results\n",
    "    verbose=1 # Set to 1 or 2 to see progress\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV with {N_ITER} iterations for {SCORING_METRIC}...\")\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best Results ---\n",
    "print(\"\\n--- Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "# Nicely print the best parameters found\n",
    "best_params = random_search.best_params_\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final Model with Best Parameters ---\n",
    "print(\"\\nTraining final model on the entire training set using best parameters...\")\n",
    "# The best estimator found by RandomizedSearchCV is already fitted on the full data\n",
    "# if refit=True (default), but we fit it explicitly for clarity.\n",
    "# Alternatively, you could just use: best_pipeline = random_search.best_estimator_\n",
    "best_pipeline = pipeline # Start with the original pipeline structure\n",
    "best_pipeline.set_params(**best_params) # Set the best parameters found\n",
    "best_pipeline.fit(X, y)\n",
    "print(\"Final model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data ---\n",
    "print(\"Predicting on test data using the tuned model...\")\n",
    "test_predictions = best_pipeline.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions})\n",
    "submission_filename = 'submission_tuned_gbc.csv' # New filename\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename}' created successfully.\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nPredicted target distribution:\\n{submission_df['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* model on the training set\n",
    "train_preds_tuned = best_pipeline.predict(X)\n",
    "train_accuracy_tuned = accuracy_score(y, train_preds_tuned)\n",
    "train_roc_auc_tuned = roc_auc_score(y, best_pipeline.predict_proba(X)[:, 1])\n",
    "print(f\"\\n--- Tuned Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_tuned:.4f}\")\n",
    "print(\"(Compare these to the initial overfit scores and the CV scores)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b186ba-3cd1-48c8-a606-38cb1591633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2610a1-4eb5-4c1c-a450-e67464349b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer # Ensure make_scorer is imported if needed, though cross_val_score handles it\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "import xgboost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    print(\"XGBoost not found. Please install it using: pip install xgboost\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (Identical to previous step) ---\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years & Calculate Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        # Attempt to get reference year from Dt_Customer\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True) # Drop temporary column\n",
    "    except Exception as e: # Broad exception for safety if Dt_Customer is missing or unparseable\n",
    "        print(f\"Warning: Could not parse Dt_Customer for reference year. Using current year. Error: {e}\")\n",
    "        reference_year = current_year\n",
    "\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Process Dt_Customer & Calculate Customer_Lifetime\n",
    "    # Convert 'Dt_Customer' to datetime, coercing errors\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "\n",
    "    # Find the latest date for calculating tenure if not provided (from training set)\n",
    "    if is_train:\n",
    "        global global_latest_date # Store latest date from training set globally\n",
    "        # Handle case where all Dt_Customer might be NaT after coercion\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            global_latest_date = valid_dates.max() + datetime.timedelta(days=1)\n",
    "        else:\n",
    "            # Fallback if no valid dates found in training set\n",
    "            global_latest_date = datetime.datetime(reference_year + 1, 1, 1)\n",
    "            print(f\"Warning: No valid Dt_Customer found in training set. Using fallback latest date: {global_latest_date}\")\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        # Fallback if called on test set first or global_latest_date isn't set\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        print(f\"Warning: Using fallback latest date for test set: {latest_date_to_use}\")\n",
    "\n",
    "    # Calculate Customer_Lifetime (Tenure in days)\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    # Impute median *before* dropping Dt_Customer to handle NaTs properly\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # Keep original Kidhome/Teenhome for now, might be useful features\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing ---\n",
    "train_df_processed = preprocess_data(train_df.copy(), is_train=True)\n",
    "# Check if global_latest_date was set correctly\n",
    "if 'global_latest_date' not in globals():\n",
    "     print(\"Error: global_latest_date not set during training preprocessing. Exiting.\")\n",
    "     # Handle this case appropriately, maybe define a default or raise error\n",
    "     # For now, let's set a default, but ideally the training data processing should succeed\n",
    "     global_latest_date = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "     print(f\"Using current date as fallback for global_latest_date: {global_latest_date}\")\n",
    "\n",
    "test_df_processed = preprocess_data(test_df.copy(), is_train=False, latest_date=global_latest_date)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data ---\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns after preprocessing\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# --- Define Preprocessing Steps (Same as before) ---\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Set sparse=True for large data if needed\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model: XGBoost ---\n",
    "# Use_label_encoder=False is recommended for newer XGBoost versions\n",
    "# eval_metric='logloss' or 'auc' are common for binary classification\n",
    "base_model_xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# --- Create Full Pipeline with XGBoost ---\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', base_model_xgb) # Step name remains 'classifier'\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for XGBoost RandomizedSearchCV ---\n",
    "# These ranges are starting points; adjust based on results or computational budget\n",
    "param_dist_xgb = {\n",
    "    'classifier__n_estimators': [100, 150, 200, 300, 400], # Number of boosting rounds\n",
    "    'classifier__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15], # Step size shrinkage\n",
    "    'classifier__max_depth': [2, 3, 4, 5], # Maximum depth of a tree\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Fraction of samples used per tree\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Fraction of features used per tree\n",
    "    'classifier__gamma': [0, 0.1, 0.2, 0.5], # Minimum loss reduction required to make a further partition\n",
    "    'classifier__reg_alpha': [0, 0.001, 0.01, 0.1], # L1 regularization term\n",
    "    'classifier__reg_lambda': [0.5, 1, 1.5] # L2 regularization term (default is 1)\n",
    "    # Add 'min_child_weight': [1, 3, 5] if needed (minimum sum of instance weight needed in a child)\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy (same as before) ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for XGBoost ---\n",
    "N_ITER_XGB = 75 # Increase iterations for potentially better results (vs 50 for GBC)\n",
    "SCORING_METRIC = 'roc_auc' # Optimize for ROC AUC\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=pipeline_xgb,\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=N_ITER_XGB,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    random_state=42, # For reproducible search results\n",
    "    verbose=1 # Set to 1 or 2 to see progress\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for XGBoost with {N_ITER_XGB} iterations for {SCORING_METRIC}...\")\n",
    "random_search_xgb.fit(X, y)\n",
    "print(\"XGBoost RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best XGBoost Results ---\n",
    "print(\"\\n--- XGBoost Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_xgb.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_xgb = random_search_xgb.best_params_\n",
    "for param, value in best_params_xgb.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final XGBoost Model with Best Parameters ---\n",
    "print(\"\\nTraining final XGBoost model on the entire training set using best parameters...\")\n",
    "# The best estimator is automatically refit on the whole training data by RandomizedSearchCV\n",
    "best_pipeline_xgb = random_search_xgb.best_estimator_\n",
    "# Explicit refit just to be sure (Optional, default behavior of RS CV is refit=True)\n",
    "# best_pipeline_xgb.fit(X, y)\n",
    "print(\"Final XGBoost model training complete.\")\n",
    "\n",
    "\n",
    "# --- Predict on Test Data with Tuned XGBoost ---\n",
    "print(\"Predicting on test data using the tuned XGBoost model...\")\n",
    "test_predictions_xgb = best_pipeline_xgb.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for XGBoost ---\n",
    "submission_df_xgb = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_xgb})\n",
    "submission_filename_xgb = 'submission_tuned_xgb.csv' # New filename\n",
    "submission_df_xgb.to_csv(submission_filename_xgb, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_xgb}' created successfully.\")\n",
    "print(submission_df_xgb.head())\n",
    "print(f\"\\nPredicted target distribution (XGBoost):\\n{submission_df_xgb['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* XGBoost model on the training set\n",
    "train_preds_xgb_tuned = best_pipeline_xgb.predict(X)\n",
    "train_accuracy_xgb_tuned = accuracy_score(y, train_preds_xgb_tuned)\n",
    "train_roc_auc_xgb_tuned = roc_auc_score(y, best_pipeline_xgb.predict_proba(X)[:, 1])\n",
    "print(f\"\\n--- Tuned XGBoost Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_xgb_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_xgb_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64d34d-c10d-47d5-b3dc-319304ec13e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Keep GBC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer # Added FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (MODIFIED) ---\n",
    "def preprocess_data_v2(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering (v2) and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # --- Original Preprocessing ---\n",
    "    # 1. Handle Birth Year & Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not parse Dt_Customer for reference year. Using current year. Error: {e}\")\n",
    "        reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True) # Keep Age\n",
    "\n",
    "    # 2. Process Dt_Customer & Lifetime + Extract Date Features\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v2 # Use a new global var name if running in same session\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            global_latest_date_v2 = valid_dates.max() + datetime.timedelta(days=1)\n",
    "        else:\n",
    "            global_latest_date_v2 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "            print(f\"Warning: No valid Dt_Customer. Using fallback latest date: {global_latest_date_v2}\")\n",
    "        latest_date_to_use = global_latest_date_v2\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "\n",
    "    # --> NEW: Extract Date Features BEFORE calculating lifetime and dropping\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer'].dt.dayofweek\n",
    "    # Impute NaNs in date features (e.g., with mode or median year/month)\n",
    "    df_processed['Enroll_Month'].fillna(df_processed['Enroll_Month'].mode()[0], inplace=True)\n",
    "    df_processed['Enroll_Year'].fillna(df_processed['Enroll_Year'].median(), inplace=True)\n",
    "    df_processed['Enroll_DayOfWeek'].fillna(df_processed['Enroll_DayOfWeek'].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True) # Now drop Dt_Customer\n",
    "\n",
    "    # 3. Simplify Marital Status (Keeping original for now - let's test)\n",
    "    # df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({ ... }) # Keep original\n",
    "\n",
    "    # 4. Simplify Education (Keeping original for now - let's test)\n",
    "    # df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'}) # Keep original\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    # Impute NaNs in spending columns *before* summing (using 0 or median)\n",
    "    for col in mnt_cols:\n",
    "        df_processed[col] = df_processed[col].fillna(0) # Simple imputation with 0 for spending\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # --- NEW Features ---\n",
    "    # Ratio Features (handle division by zero)\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    # Replace inf values that might result from 0/0\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "\n",
    "    # Income related (handle division by zero and potential NaNs in Income)\n",
    "    # Impute Income NaNs *before* using it in calculations\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    # Add 1 to avoid division by zero if Children=0 and partner=1 (or single=1)\n",
    "    num_people = df_processed['Children'] + df_processed['Marital_Status'].apply(lambda x: 1 if x=='Single' else 2) # Simple adult estimate\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / num_people.replace(0, 1)).fillna(0) # Replace 0 people with 1\n",
    "\n",
    "\n",
    "    # Spending per Purchase (handle division by zero)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Total_Purchases'].replace(0, 1)).fillna(0)\n",
    "\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply NEW Preprocessing ---\n",
    "train_df_processed_v2 = preprocess_data_v2(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v2' not in globals(): # Check the new global var\n",
    "     global_latest_date_v2 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "     print(f\"Error: global_latest_date_v2 not set. Using fallback: {global_latest_date_v2}\")\n",
    "test_df_processed_v2 = preprocess_data_v2(test_df.copy(), is_train=False, latest_date=global_latest_date_v2)\n",
    "print(\"V2 Preprocessing complete.\")\n",
    "\n",
    "# --- Prepare Data (using v2 processed data) ---\n",
    "X_v2 = train_df_processed_v2.drop(['ID', 'Target'], axis=1)\n",
    "y_v2 = train_df_processed_v2['Target'] # Target remains the same\n",
    "X_test_v2 = test_df_processed_v2.drop('ID', axis=1)\n",
    "\n",
    "# Align columns after V2 preprocessing\n",
    "train_cols_v2 = X_v2.columns\n",
    "test_cols_v2 = X_test_v2.columns\n",
    "\n",
    "missing_in_test_v2 = set(train_cols_v2) - set(test_cols_v2)\n",
    "for c in missing_in_test_v2:\n",
    "    X_test_v2[c] = 0\n",
    "missing_in_train_v2 = set(test_cols_v2) - set(train_cols_v2)\n",
    "for c in missing_in_train_v2:\n",
    "    X_v2[c] = 0\n",
    "\n",
    "X_test_v2 = X_test_v2[train_cols_v2] # Ensure order is the same\n",
    "\n",
    "# --- Define Preprocessing Steps (Potentially updated if features changed type) ---\n",
    "numerical_features_v2 = X_v2.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2 = X_v2.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nV2 Numerical features ({len(numerical_features_v2)}): {numerical_features_v2}\")\n",
    "print(f\"V2 Categorical features ({len(categorical_features_v2)}): {categorical_features_v2}\")\n",
    "\n",
    "\n",
    "# Log transformer function\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=False) # validate=False to handle 0s after log1p\n",
    "\n",
    "# Update Numerical Pipeline to include Log Transform for specific skewed features\n",
    "# Identify potentially skewed features (Income, Spending)\n",
    "skewed_num_features = ['Income', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
    "                       'MntSweetProducts', 'MntGoldProds', 'Total_Mnt', 'Income_per_Person', 'Spending_per_Purchase']\n",
    "# Make sure these features actually exist after preprocessing\n",
    "skewed_num_features = [f for f in skewed_num_features if f in numerical_features_v2]\n",
    "other_num_features = [f for f in numerical_features_v2 if f not in skewed_num_features]\n",
    "\n",
    "\n",
    "numerical_pipeline_v2 = Pipeline([\n",
    "    # Impute FIRST\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    # Apply log transform only to skewed columns (using ColumnTransformer within Pipeline - tricky!)\n",
    "    # Easier approach: Apply log transform in preprocess_data_v2 or handle separately if needed.\n",
    "    # For simplicity here, let's apply StandardScaler to all imputed numericals.\n",
    "    # Consider log transform within preprocess_data_v2 if it proves beneficial.\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline_v2 = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Update Preprocessor\n",
    "preprocessor_v2 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2, numerical_features_v2), # Apply updated pipeline to all numerical\n",
    "    ('cat', categorical_pipeline_v2, categorical_features_v2)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model: Gradient Boosting (Retuning this one) ---\n",
    "base_model_gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# --- Create Full Pipeline with GBC V2 ---\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('classifier', base_model_gbc)\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for GBC RandomizedSearchCV (Centered around previous best) ---\n",
    "param_dist_gbc_v2 = {\n",
    "    'classifier__n_estimators': [150, 200, 250, 300], # Explore higher values slightly\n",
    "    'classifier__learning_rate': [0.02, 0.05, 0.08, 0.1], # Narrower range around 0.08\n",
    "    'classifier__max_depth': [2, 3], # Keep focusing on shallow trees\n",
    "    'classifier__min_samples_leaf': [15, 20, 25], # Stay around the previous best\n",
    "    'classifier__min_samples_split': [15, 20, 30], # Stay around the previous best\n",
    "    'classifier__subsample': [0.5, 0.6, 0.7], # Explore around 0.6\n",
    "    'classifier__max_features': ['sqrt', 'log2'] # Keep simpler options\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy (same as before) ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for GBC V2 ---\n",
    "N_ITER_GBC_V2 = 50 # Number of iterations for retuning\n",
    "SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "random_search_gbc_v2 = RandomizedSearchCV(\n",
    "    estimator=pipeline_gbc_v2,\n",
    "    param_distributions=param_dist_gbc_v2,\n",
    "    n_iter=N_ITER_GBC_V2,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for GBC (V2 Features) with {N_ITER_GBC_V2} iterations for {SCORING_METRIC}...\")\n",
    "random_search_gbc_v2.fit(X_v2, y_v2) # Use V2 features and original target\n",
    "print(\"GBC V2 RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best GBC V2 Results ---\n",
    "print(\"\\n--- GBC V2 Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_gbc_v2.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_gbc_v2 = random_search_gbc_v2.best_params_\n",
    "for param, value in best_params_gbc_v2.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final GBC V2 Model with Best Parameters ---\n",
    "print(\"\\nTraining final GBC V2 model on the entire training set using best parameters...\")\n",
    "best_pipeline_gbc_v2 = random_search_gbc_v2.best_estimator_\n",
    "print(\"Final GBC V2 model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Tuned GBC V2 ---\n",
    "print(\"Predicting on test data using the tuned GBC V2 model...\")\n",
    "test_predictions_gbc_v2 = best_pipeline_gbc_v2.predict(X_test_v2) # Use V2 test features\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for GBC V2 ---\n",
    "submission_df_gbc_v2 = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_gbc_v2})\n",
    "submission_filename_gbc_v2 = 'submission_tuned_gbc_v2_features.csv' # New filename\n",
    "submission_df_gbc_v2.to_csv(submission_filename_gbc_v2, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_gbc_v2}' created successfully.\")\n",
    "print(submission_df_gbc_v2.head())\n",
    "print(f\"\\nPredicted target distribution (GBC V2 Features):\\n{submission_df_gbc_v2['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* GBC V2 model on the training set\n",
    "train_preds_gbc_v2_tuned = best_pipeline_gbc_v2.predict(X_v2)\n",
    "train_accuracy_gbc_v2_tuned = accuracy_score(y_v2, train_preds_gbc_v2_tuned)\n",
    "train_roc_auc_gbc_v2_tuned = roc_auc_score(y_v2, best_pipeline_gbc_v2.predict_proba(X_v2)[:, 1])\n",
    "print(f\"\\n--- Tuned GBC V2 Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_gbc_v2_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v2_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64d632-491e-4577-9784-13c0eb33cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming best_pipeline_gbc_v2 is your fitted V2 GBC pipeline\n",
    "# Get feature names after one-hot encoding\n",
    "ohe_feature_names = best_pipeline_gbc_v2.named_steps['preprocessor'] \\\n",
    "                    .named_transformers_['cat'] \\\n",
    "                    .named_steps['onehot'] \\\n",
    "                    .get_feature_names_out(categorical_features_v2)\n",
    "all_feature_names = numerical_features_v2 + list(ohe_feature_names)\n",
    "\n",
    "# Get importances\n",
    "importances = best_pipeline_gbc_v2.named_steps['classifier'].feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'feature': all_feature_names, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "print(\"\\nTop 20 Feature Importances (GBC V2):\")\n",
    "print(feature_importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a048a4e-5af5-4994-8a12-e92a278e5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold # Keep for reference if needed later\n",
    "# Removed RandomizedSearchCV as we are using pre-found params\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions (V1 and V2) ---\n",
    "\n",
    "# Function V1 (leading to 0.845 score)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    df_processed = df.copy()\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0) # Impute before sum\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    # Impute Income (might be needed if not done before FE)\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    # Impute Age (might be needed if Year_Birth had NaNs)\n",
    "    age_median = df_processed['Age'].median()\n",
    "    df_processed['Age'].fillna(age_median, inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# Function V2 (leading to 0.848 score) - simplified, assuming it's the same as last run\n",
    "def preprocess_data_v2(df, is_train=True, latest_date=None):\n",
    "    df_processed = df.copy()\n",
    "    # --- Previous steps: Age, Lifetime, Date Features ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v2\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        global_latest_date_v2 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v2\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer'].dt.month.fillna(df_processed['Dt_Customer'].dt.month.mode()[0])\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer'].dt.year.fillna(df_processed['Dt_Customer'].dt.year.median())\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer'].dt.dayofweek.fillna(df_processed['Dt_Customer'].dt.dayofweek.mode()[0])\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # --- V2 specific additions / kept originals ---\n",
    "    # Marital_Status kept original\n",
    "    # Education kept original\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0)\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    num_people = df_processed['Children'] + df_processed['Marital_Status'].apply(lambda x: 1 if x in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / num_people.replace(0, 1)).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Total_Purchases'].replace(0, 1)).fillna(0)\n",
    "    # Impute Age (might be needed if Year_Birth had NaNs)\n",
    "    age_median = df_processed['Age'].median()\n",
    "    df_processed['Age'].fillna(age_median, inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing V1 ---\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "\n",
    "# --- Apply Preprocessing V2 ---\n",
    "train_df_processed_v2 = preprocess_data_v2(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v2' not in globals(): global_latest_date_v2 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v2 = preprocess_data_v2(test_df.copy(), is_train=False, latest_date=global_latest_date_v2)\n",
    "print(\"V2 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V1 ---\n",
    "X_v1 = train_df_processed_v1.drop(['ID', 'Target'], axis=1)\n",
    "y_v1 = train_df_processed_v1['Target']\n",
    "X_test_v1 = test_df_processed_v1.drop('ID', axis=1)\n",
    "# Align\n",
    "train_cols_v1 = X_v1.columns\n",
    "missing_in_test_v1 = set(train_cols_v1) - set(X_test_v1.columns)\n",
    "for c in missing_in_test_v1: X_test_v1[c] = 0\n",
    "missing_in_train_v1 = set(X_test_v1.columns) - set(train_cols_v1)\n",
    "for c in missing_in_train_v1: X_v1[c] = 0\n",
    "X_test_v1 = X_test_v1[train_cols_v1]\n",
    "\n",
    "# --- Prepare Data V2 ---\n",
    "X_v2 = train_df_processed_v2.drop(['ID', 'Target'], axis=1)\n",
    "y_v2 = train_df_processed_v2['Target']\n",
    "X_test_v2 = test_df_processed_v2.drop('ID', axis=1)\n",
    "# Align\n",
    "train_cols_v2 = X_v2.columns\n",
    "missing_in_test_v2 = set(train_cols_v2) - set(X_test_v2.columns)\n",
    "for c in missing_in_test_v2: X_test_v2[c] = 0\n",
    "missing_in_train_v2 = set(X_test_v2.columns) - set(train_cols_v2)\n",
    "for c in missing_in_train_v2: X_v2[c] = 0\n",
    "X_test_v2 = X_test_v2[train_cols_v2]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines (Need separate ones for V1 and V2 features) ---\n",
    "\n",
    "# Pipeline V1 Definition\n",
    "numerical_features_v1 = X_v1.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v1 = X_v1.select_dtypes(exclude=np.number).columns.tolist()\n",
    "numerical_pipeline_v1 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v1 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v1 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v1, numerical_features_v1),\n",
    "    ('cat', categorical_pipeline_v1, categorical_features_v1)], remainder='passthrough')\n",
    "\n",
    "# Pipeline V2 Definition\n",
    "numerical_features_v2 = X_v2.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2 = X_v2.select_dtypes(exclude=np.number).columns.tolist()\n",
    "numerical_pipeline_v2 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v2 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v2 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2, numerical_features_v2),\n",
    "    ('cat', categorical_pipeline_v2, categorical_features_v2)], remainder='passthrough')\n",
    "\n",
    "# --- Define BEST Hyperparameters found previously ---\n",
    "\n",
    "# Best parameters for GBC with V1 features (resulted in 0.845 Kaggle score)\n",
    "# Note: These are the params *you reported* finding previously. Double-check if needed.\n",
    "best_params_gbc_v1 = {\n",
    "    'classifier__subsample': 0.6,\n",
    "    'classifier__n_estimators': 200,\n",
    "    'classifier__min_samples_split': 20,\n",
    "    'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'sqrt',\n",
    "    'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.08\n",
    "}\n",
    "\n",
    "# Best parameters for GBC with V2 features (resulted in 0.848 Kaggle score)\n",
    "best_params_gbc_v2 = {\n",
    "    'classifier__subsample': 0.7,\n",
    "    'classifier__n_estimators': 300,\n",
    "    'classifier__min_samples_split': 20,\n",
    "    'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'log2',\n",
    "    'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.05\n",
    "}\n",
    "\n",
    "\n",
    "# --- Build and Train Model 1 (GBC V1) ---\n",
    "print(\"Training Model 1 (GBC V1)...\")\n",
    "pipeline_gbc_v1 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v1),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42)) # Base model\n",
    "])\n",
    "pipeline_gbc_v1.set_params(**best_params_gbc_v1) # Apply best params\n",
    "pipeline_gbc_v1.fit(X_v1, y_v1)\n",
    "print(\"Model 1 training complete.\")\n",
    "\n",
    "# --- Build and Train Model 2 (GBC V2) ---\n",
    "print(\"Training Model 2 (GBC V2)...\")\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42)) # Base model\n",
    "])\n",
    "pipeline_gbc_v2.set_params(**best_params_gbc_v2) # Apply best params\n",
    "pipeline_gbc_v2.fit(X_v2, y_v2)\n",
    "print(\"Model 2 training complete.\")\n",
    "\n",
    "\n",
    "# --- Predict Probabilities on Test Set ---\n",
    "print(\"Predicting probabilities...\")\n",
    "# IMPORTANT: Use the correctly preprocessed test set for each model!\n",
    "probs_gbc_v1 = pipeline_gbc_v1.predict_proba(X_test_v1)[:, 1]\n",
    "probs_gbc_v2 = pipeline_gbc_v2.predict_proba(X_test_v2)[:, 1]\n",
    "\n",
    "# --- Ensemble Averaging ---\n",
    "print(\"Averaging predictions...\")\n",
    "# Simple average (you could also try weighted average if desired)\n",
    "avg_probs = (probs_gbc_v1 + probs_gbc_v2) / 2\n",
    "\n",
    "# Convert probabilities to 0/1 using 0.5 threshold\n",
    "final_predictions = (avg_probs >= 0.5).astype(int)\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df_ensemble = pd.DataFrame({'ID': test_df['ID'], 'Target': final_predictions})\n",
    "submission_filename_ensemble = 'submission_ensemble_gbc_v1_v2.csv'\n",
    "submission_df_ensemble.to_csv(submission_filename_ensemble, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_ensemble}' created successfully.\")\n",
    "print(submission_df_ensemble.head())\n",
    "print(f\"\\nPredicted target distribution (Ensemble):\\n{submission_df_ensemble['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate component models on training data (as a rough check)\n",
    "train_preds_gbc_v1 = pipeline_gbc_v1.predict(X_v1)\n",
    "train_roc_auc_gbc_v1 = roc_auc_score(y_v1, pipeline_gbc_v1.predict_proba(X_v1)[:, 1])\n",
    "print(f\"\\n--- Model 1 (GBC V1) Training Set Eval ---\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v1:.4f}\")\n",
    "\n",
    "train_preds_gbc_v2 = pipeline_gbc_v2.predict(X_v2)\n",
    "train_roc_auc_gbc_v2 = roc_auc_score(y_v2, pipeline_gbc_v2.predict_proba(X_v2)[:, 1])\n",
    "print(f\"\\n--- Model 2 (GBC V2) Training Set Eval ---\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0cbc33-b907-403b-b388-d24d944277dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcbff6-6ab7-4696-bb75-006ec0c16a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "import lightgbm\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    print(\"LightGBM not found. Please install it using: pip install lightgbm\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function V2 (from previous step) ---\n",
    "def preprocess_data_v2(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering (v2) and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    # --- Original Preprocessing ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v2\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        global_latest_date_v2 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v2\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "\n",
    "    # Extract Date Features BEFORE calculating lifetime and dropping\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer'].dt.month.fillna(df_processed['Dt_Customer'].dt.month.mode()[0])\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer'].dt.year.fillna(df_processed['Dt_Customer'].dt.year.median())\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer'].dt.dayofweek.fillna(df_processed['Dt_Customer'].dt.dayofweek.mode()[0])\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # Keep Original Marital_Status & Education for V2\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0) # Impute spending NaNs with 0\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # --- V2 Features ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    # Estimate adults based on Marital_Status original values\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if x in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / num_people.replace(0, 1)).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Total_Purchases'].replace(0, 1)).fillna(0)\n",
    "    age_median = df_processed['Age'].median()\n",
    "    df_processed['Age'].fillna(age_median, inplace=True)\n",
    "    # Drop Num_Adults helper column\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing V2 ---\n",
    "train_df_processed_v2 = preprocess_data_v2(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v2' not in globals(): global_latest_date_v2 = datetime.datetime.now() + datetime.timedelta(days=1); print(f\"Error: global_latest_date_v2 not set. Using fallback: {global_latest_date_v2}\")\n",
    "test_df_processed_v2 = preprocess_data_v2(test_df.copy(), is_train=False, latest_date=global_latest_date_v2)\n",
    "print(\"V2 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V2 ---\n",
    "X_v2 = train_df_processed_v2.drop(['ID', 'Target'], axis=1)\n",
    "y_v2 = train_df_processed_v2['Target']\n",
    "X_test_v2 = test_df_processed_v2.drop('ID', axis=1)\n",
    "# Align columns\n",
    "train_cols_v2 = X_v2.columns\n",
    "test_cols_v2 = X_test_v2.columns\n",
    "missing_in_test_v2 = set(train_cols_v2) - set(test_cols_v2)\n",
    "for c in missing_in_test_v2: X_test_v2[c] = 0\n",
    "missing_in_train_v2 = set(test_cols_v2) - set(train_cols_v2)\n",
    "for c in missing_in_train_v2: X_v2[c] = 0\n",
    "X_test_v2 = X_test_v2[train_cols_v2]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Steps (Using V2 Features) ---\n",
    "numerical_features_v2 = X_v2.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2 = X_v2.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# Ensure 'Age' and 'Customer_Lifetime' are correctly identified if they exist\n",
    "print(f\"\\nV2 Numerical features ({len(numerical_features_v2)}): {numerical_features_v2}\")\n",
    "print(f\"V2 Categorical features ({len(categorical_features_v2)}): {categorical_features_v2}\")\n",
    "\n",
    "numerical_pipeline_v2 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v2 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v2 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2, numerical_features_v2),\n",
    "    ('cat', categorical_pipeline_v2, categorical_features_v2)], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model: LightGBM ---\n",
    "base_model_lgbm = LGBMClassifier(random_state=42, objective='binary') # objective='binary' is good practice\n",
    "\n",
    "# --- Create Full Pipeline with LightGBM ---\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('classifier', base_model_lgbm) # Step name remains 'classifier'\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for LightGBM RandomizedSearchCV ---\n",
    "param_dist_lgbm = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'classifier__learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 4, 5, 7, -1], # -1 means no limit\n",
    "    'classifier__num_leaves': [10, 15, 20, 31, 40], # Should be < 2^max_depth\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Alias: bagging_fraction\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Alias: feature_fraction\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1, 0.5, 1.0], # L1\n",
    "    'classifier__reg_lambda': [0, 0.1, 0.5, 1.0, 2.0], # L2\n",
    "    'classifier__min_child_samples': [10, 20, 30, 50] # Min data in leaf\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for LightGBM ---\n",
    "N_ITER_LGBM = 75 # Number of iterations\n",
    "SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "random_search_lgbm = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgbm,\n",
    "    param_distributions=param_dist_lgbm,\n",
    "    n_iter=N_ITER_LGBM,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for LightGBM with {N_ITER_LGBM} iterations for {SCORING_METRIC}...\")\n",
    "random_search_lgbm.fit(X_v2, y_v2) # Use V2 features\n",
    "print(\"LightGBM RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best LightGBM Results ---\n",
    "print(\"\\n--- LightGBM Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_lgbm.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_lgbm = random_search_lgbm.best_params_\n",
    "for param, value in best_params_lgbm.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final LightGBM Model with Best Parameters ---\n",
    "print(\"\\nTraining final LightGBM model on the entire training set using best parameters...\")\n",
    "best_pipeline_lgbm = random_search_lgbm.best_estimator_\n",
    "# best_pipeline_lgbm.fit(X_v2, y_v2) # Already refit by default\n",
    "print(\"Final LightGBM model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Tuned LightGBM ---\n",
    "print(\"Predicting on test data using the tuned LightGBM model...\")\n",
    "test_predictions_lgbm = best_pipeline_lgbm.predict(X_test_v2) # Use V2 test features\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for LightGBM ---\n",
    "submission_df_lgbm = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_lgbm})\n",
    "submission_filename_lgbm = 'submission_tuned_lgbm_v2_features.csv' # New filename\n",
    "submission_df_lgbm.to_csv(submission_filename_lgbm, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_lgbm}' created successfully.\")\n",
    "print(submission_df_lgbm.head())\n",
    "print(f\"\\nPredicted target distribution (LightGBM):\\n{submission_df_lgbm['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* LightGBM model on the training set\n",
    "train_preds_lgbm_tuned = best_pipeline_lgbm.predict(X_v2)\n",
    "train_accuracy_lgbm_tuned = accuracy_score(y_v2, train_preds_lgbm_tuned)\n",
    "train_roc_auc_lgbm_tuned = roc_auc_score(y_v2, best_pipeline_lgbm.predict_proba(X_v2)[:, 1])\n",
    "print(f\"\\n--- Tuned LightGBM Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_lgbm_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_lgbm_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6d14f-6262-49a2-ac6f-4174d748b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ae0e1-c330-4681-a5c8-f4646a290f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError:\n",
    "    print(\"CatBoost not found. Please install it using: pip install catboost\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function V4 (Corrected Target Exclusion) ---\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    \"\"\" V2 features, keeps original categoricals, handles imputation+scaling manually \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    target_col = 'Target' # Define target column name\n",
    "\n",
    "    # --- Step 1: Initial Feature Creation ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1)\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Identify initial feature types *excluding Target and ID*\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    # --- Step 2: Imputation ---\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        # Fit only on the selected features from the training set\n",
    "        imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        # Fit only on the selected features from the training set\n",
    "        imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "        print(\"Imputers fitted on training data (excluding Target).\")\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers must be provided for test data\")\n",
    "        try:\n",
    "            # Transform using the same feature list used in fit\n",
    "            df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "            df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "        except Exception as e:\n",
    "             print(f\"Error during imputation transformation: {e}\")\n",
    "             print(\"Columns being imputed (Num):\", initial_numerical_features)\n",
    "             print(\"Columns being imputed (Cat):\", initial_categorical_features)\n",
    "             print(\"Columns available in df:\", df_processed.columns.tolist())\n",
    "             raise e\n",
    "        print(\"Test data imputed using fitted imputers.\")\n",
    "\n",
    "    # Convert imputed arrays back to DataFrame -> Important: Keep Target if it exists!\n",
    "    original_cols = df_processed.columns # Store original columns before potential array conversion\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    # Add back ID and Target if they exist\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed # Overwrite df_processed with the correctly columned DataFrame\n",
    "\n",
    "    # --- Step 3: Create Derived Features ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    # --- Step 4: Scaling ---\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        # Fit only on numerical features (excluding Target)\n",
    "        scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "        print(\"Scaler fitted on training data (excluding Target).\")\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler must be provided for test data\")\n",
    "        try:\n",
    "             # Transform using the same feature list used in fit\n",
    "             df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "        except Exception as e:\n",
    "             print(f\"Error during scaling transformation: {e}\")\n",
    "             print(\"Columns being scaled:\", final_numerical_features)\n",
    "             print(\"Columns available in df:\", df_processed.columns.tolist())\n",
    "             raise e\n",
    "        print(\"Test data scaled using fitted scaler.\")\n",
    "\n",
    "    # --- Step 5: Final Type Conversion for CatBoost ---\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features:\n",
    "        df_processed[col] = df_processed[col].astype(str)\n",
    "\n",
    "    # Ensure all column names (except Target maybe) are strings\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: # Rename target back if needed\n",
    "         df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "\n",
    "\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V4 (Manual - Corrected) ---\n",
    "# Need error handling around these calls as well\n",
    "try:\n",
    "    train_df_processed_v4, fitted_imputers, fitted_scaler = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "    if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1); print(f\"Error: global_latest_date_v4 not set. Using fallback: {global_latest_date_v4}\")\n",
    "    test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers, fit_scaler=fitted_scaler)\n",
    "    print(\"V4 Preprocessing (Manual Impute/Scale - Corrected) complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during preprocessing: {e}\")\n",
    "    exit() # Exit if preprocessing fails\n",
    "\n",
    "# --- Prepare Data V4 ---\n",
    "try:\n",
    "    X_v4 = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "    y_v4 = train_df_processed_v4['Target'] # Target should exist here\n",
    "    X_test_v4 = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "\n",
    "    # Align columns\n",
    "    train_cols_v4 = X_v4.columns\n",
    "    test_cols_v4 = X_test_v4.columns\n",
    "    missing_in_test_v4 = set(train_cols_v4) - set(test_cols_v4)\n",
    "    for c in missing_in_test_v4: X_test_v4[c] = 0\n",
    "    missing_in_train_v4 = set(test_cols_v4) - set(train_cols_v4)\n",
    "    for c in missing_in_train_v4: X_v4[c] = 0\n",
    "    X_test_v4 = X_test_v4[train_cols_v4]\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError during data preparation after preprocessing: {e}\")\n",
    "    print(\"Columns in train_df_processed_v4:\", train_df_processed_v4.columns)\n",
    "    print(\"Columns in test_df_processed_v4:\", test_df_processed_v4.columns)\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during data preparation: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Identify Categorical Feature *Names* for CatBoost AFTER all processing\n",
    "categorical_features_v4_names = X_v4.select_dtypes(include='object').columns.tolist()\n",
    "print(f\"\\nV4 Categorical features by name: {categorical_features_v4_names}\")\n",
    "\n",
    "\n",
    "# --- Define Base Model: CatBoost (No Pipeline Needed) ---\n",
    "base_model_catboost = CatBoostClassifier(\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    cat_features=categorical_features_v4_names # Pass NAMES\n",
    ")\n",
    "\n",
    "# --- Define Parameter Grid for CatBoost RandomizedSearchCV ---\n",
    "param_dist_catboost = {\n",
    "    'iterations': [100, 200, 300, 500, 700, 900],\n",
    "    'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.07],\n",
    "    'depth': [4, 5, 6, 7],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 64, 128],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for CatBoost (No Pipeline) ---\n",
    "N_ITER_CATBOOST = 75\n",
    "SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "random_search_catboost = RandomizedSearchCV(\n",
    "    estimator=base_model_catboost,\n",
    "    param_distributions=param_dist_catboost,\n",
    "    n_iter=N_ITER_CATBOOST,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for CatBoost (Manual Preprocessing) with {N_ITER_CATBOOST} iterations for {SCORING_METRIC}...\")\n",
    "# Fit directly on the preprocessed dataframes\n",
    "random_search_catboost.fit(X_v4, y_v4)\n",
    "print(\"CatBoost RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best CatBoost Results ---\n",
    "print(\"\\n--- CatBoost Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_catboost.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_catboost = random_search_catboost.best_params_\n",
    "for param, value in best_params_catboost.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final CatBoost Model with Best Parameters ---\n",
    "print(\"\\nTraining final CatBoost model on the entire training set using best parameters...\")\n",
    "best_model_catboost = random_search_catboost.best_estimator_\n",
    "print(\"Final CatBoost model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Tuned CatBoost ---\n",
    "print(\"Predicting on test data using the tuned CatBoost model...\")\n",
    "test_predictions_catboost = best_model_catboost.predict(X_test_v4)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for CatBoost ---\n",
    "submission_df_catboost = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_catboost})\n",
    "submission_filename_catboost = 'submission_tuned_catboost_v4_manualprep.csv' # New filename\n",
    "submission_df_catboost.to_csv(submission_filename_catboost, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_catboost}' created successfully.\")\n",
    "print(submission_df_catboost.head())\n",
    "print(f\"\\nPredicted target distribution (CatBoost V4):\\n{submission_df_catboost['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* CatBoost model on the training set\n",
    "train_preds_catboost_tuned = best_model_catboost.predict(X_v4)\n",
    "train_accuracy_catboost_tuned = accuracy_score(y_v4, train_preds_catboost_tuned)\n",
    "train_roc_auc_catboost_tuned = roc_auc_score(y_v4, best_model_catboost.predict_proba(X_v4)[:, 1])\n",
    "print(f\"\\n--- Tuned CatBoost V4 Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_catboost_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_catboost_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f37fb-975c-49cc-b222-9c36473558b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# No CV needed here, just loading/training final models\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression # Added for meta-model if doing stacking later\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError:\n",
    "    print(\"CatBoost not found. Please install it using: pip install catboost\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions (V1 and V4 needed) ---\n",
    "\n",
    "# Function V1 (Correct definition for GBC V1 - returns only DataFrame)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    \"\"\" V1 features with simplified categoricals, no manual impute/scale return \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    target_col = 'Target'\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1 # Keep distinct global date var\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        # Use try-except for safety if no valid dates exist\n",
    "        try:\n",
    "            global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: # Handle case where valid_dates might be empty or cause issues\n",
    "             global_latest_date_v1 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "             print(f\"Warning: Error setting global_latest_date_v1. Using fallback: {global_latest_date_v1}\")\n",
    "\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # V1 Simplifications\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0) # Simple imputation\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Simple median imputation for remaining numericals (before pipeline)\n",
    "    cols_to_impute_num = ['Age', 'Income', 'Customer_Lifetime', 'Total_Mnt'] # Add others if needed\n",
    "    for col in cols_to_impute_num:\n",
    "         if col in df_processed.columns and df_processed[col].isnull().any():\n",
    "              median_val = df_processed[col].median() # Calculate median before filling\n",
    "              df_processed[col].fillna(median_val, inplace=True)\n",
    "\n",
    "    # Simple mode imputation for remaining categoricals (before pipeline)\n",
    "    cat_cols_simple = ['Marital_Status', 'Education']\n",
    "    for col in cat_cols_simple:\n",
    "        if col in df_processed.columns and df_processed[col].isnull().any():\n",
    "            mode_val = df_processed[col].mode()[0] # Calculate mode before filling\n",
    "            df_processed[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "    return df_processed # Only return the DataFrame\n",
    "\n",
    "\n",
    "# Function V4 (for CatBoost - Manual Impute/Scale) - Keep as is from previous correct version\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    # ... (Keep the full V4 function definition from the previous working block) ...\n",
    "    df_processed = df.copy()\n",
    "    target_col = 'Target' # Define target column name\n",
    "    # --- Step 1: Initial Feature Creation (before imputation/scaling) ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth'] # Age calculated, may have NaNs\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True) # Keep intermediate\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        # Use try-except for safety if no valid dates exist\n",
    "        try:\n",
    "            global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: # Handle case where valid_dates might be empty or cause issues\n",
    "             global_latest_date_v4 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "             print(f\"Warning: Error setting global_latest_date_v4. Using fallback: {global_latest_date_v4}\")\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month # Keep as number initially for imputation\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek # Keep as number initially\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days # May have NaNs\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore') # Drop original and intermediate date cols\n",
    "    # Keep Original Marital_Status & Education\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1) # Calculated before imputation, may include NaNs if components are NaN\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    # Don't create ratio/derived features yet\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1) # Denominator stored temporarily\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Identify initial feature types *excluding Target and ID*\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    # --- Step 2: Imputation ---\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "        # print(\"Imputers fitted on training data (excluding Target).\") # Optional print\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers must be provided for test data\")\n",
    "        try:\n",
    "            df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "            df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "        except Exception as e:\n",
    "             print(f\"Error during imputation transformation: {e}\")\n",
    "             print(\"Columns available in df:\", df_processed.columns.tolist())\n",
    "             raise e\n",
    "        # print(\"Test data imputed using fitted imputers.\") # Optional print\n",
    "\n",
    "    # Reconstruct DataFrame -> Important: Keep Target if it exists!\n",
    "    original_cols = df_processed.columns\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "\n",
    "    # --- Step 3: Create Derived Features ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    # --- Step 4: Scaling ---\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "        # print(\"Scaler fitted on training data (excluding Target).\") # Optional\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler must be provided for test data\")\n",
    "        try:\n",
    "             df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "        except Exception as e:\n",
    "             print(f\"Error during scaling transformation: {e}\")\n",
    "             raise e\n",
    "        # print(\"Test data scaled using fitted scaler.\") # Optional\n",
    "\n",
    "    # --- Step 5: Final Type Conversion for CatBoost ---\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V1 (Corrected Call) ---\n",
    "print(\"Preprocessing V1...\")\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True) # Now expects only 1 return value\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "\n",
    "# --- Apply Preprocessing V4 (for CatBoost - Manual Impute/Scale) ---\n",
    "print(\"\\nPreprocessing V4 (for CatBoost)...\")\n",
    "train_df_processed_v4, fitted_imputers_v4, fitted_scaler_v4 = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers_v4, fit_scaler=fitted_scaler_v4)\n",
    "print(\"V4 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V1 (for GBC V1) ---\n",
    "# ... (rest of data prep V1 is likely fine) ...\n",
    "X_v1 = train_df_processed_v1.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v1 = train_df_processed_v1['Target']\n",
    "X_test_v1 = test_df_processed_v1.drop('ID', axis=1, errors='ignore')\n",
    "train_cols_v1 = X_v1.columns; test_cols_v1 = X_test_v1.columns\n",
    "missing_in_test_v1 = set(train_cols_v1) - set(test_cols_v1)\n",
    "for c in missing_in_test_v1: X_test_v1[c] = 0\n",
    "missing_in_train_v1 = set(test_cols_v1) - set(train_cols_v1)\n",
    "for c in missing_in_train_v1: X_v1[c] = 0\n",
    "X_test_v1 = X_test_v1[train_cols_v1]\n",
    "\n",
    "# --- Prepare Data V2 (for GBC V2 - using V4 preproc function for consistency, but applying OHE) ---\n",
    "X_v2_like = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v2_like = train_df_processed_v4['Target'] # Target comes from V4 processed train\n",
    "X_test_v2_like = test_df_processed_v4.drop('ID', axis=1, errors='ignore') # Features come from V4 processed test\n",
    "# Align V2_like\n",
    "train_cols_v2_like = X_v2_like.columns; test_cols_v2_like = X_test_v2_like.columns\n",
    "missing_in_test_v2_like = set(train_cols_v2_like) - set(test_cols_v2_like)\n",
    "for c in missing_in_test_v2_like: X_test_v2_like[c] = 0\n",
    "missing_in_train_v2_like = set(test_cols_v2_like) - set(train_cols_v2_like)\n",
    "for c in missing_in_train_v2_like: X_v2_like[c] = 0\n",
    "X_test_v2_like = X_test_v2_like[train_cols_v2_like]\n",
    "\n",
    "\n",
    "# --- Prepare Data V4 (for CatBoost) ---\n",
    "X_v4 = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v4 = train_df_processed_v4['Target']\n",
    "X_test_v4 = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "# Align V4 (redundant if V2_like is aligned, but safe)\n",
    "train_cols_v4 = X_v4.columns\n",
    "test_cols_v4 = X_test_v4.columns\n",
    "missing_in_test_v4 = set(train_cols_v4) - set(test_cols_v4)\n",
    "for c in missing_in_test_v4: X_test_v4[c] = 0\n",
    "missing_in_train_v4 = set(test_cols_v4) - set(train_cols_v4)\n",
    "for c in missing_in_train_v4: X_v4[c] = 0\n",
    "X_test_v4 = X_test_v4[train_cols_v4]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines V1 (GBC V1 requires OHE) ---\n",
    "numerical_features_v1 = X_v1.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v1 = X_v1.select_dtypes(exclude=np.number).columns.tolist()\n",
    "numerical_pipeline_v1 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v1 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v1 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v1, numerical_features_v1),\n",
    "    ('cat', categorical_pipeline_v1, categorical_features_v1)], remainder='passthrough')\n",
    "\n",
    "# --- Define Preprocessing Pipelines V2 (GBC V2 requires OHE on V4 features) ---\n",
    "numerical_features_v2_like = X_v2_like.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2_like = X_v2_like.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# Define pipelines assuming imputation happened in preprocess_data_v4\n",
    "numerical_pipeline_v2_like = Pipeline([('scaler', StandardScaler())]) # Scaling only\n",
    "categorical_pipeline_v2_like = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]) # OHE only\n",
    "preprocessor_v2_like = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2_like, numerical_features_v2_like),\n",
    "    ('cat', categorical_pipeline_v2_like, categorical_features_v2_like)], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define BEST Hyperparameters ---\n",
    "# ... (keep best params definitions) ...\n",
    "best_params_gbc_v1 = {\n",
    "    'classifier__subsample': 0.6, 'classifier__n_estimators': 200,\n",
    "    'classifier__min_samples_split': 20, 'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'sqrt', 'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.08}\n",
    "best_params_gbc_v2 = { # Params that gave 0.848\n",
    "    'classifier__subsample': 0.7, 'classifier__n_estimators': 300,\n",
    "    'classifier__min_samples_split': 20, 'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'log2', 'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.05}\n",
    "best_params_catboost_v4 = { # Params found for CatBoost V4\n",
    "    'subsample': 0.9, 'learning_rate': 0.07, 'l2_leaf_reg': 3,\n",
    "    'iterations': 100, 'depth': 4, 'border_count': 64}\n",
    "\n",
    "# --- Build and Train Model 1 (GBC V1) ---\n",
    "print(\"\\nTraining Model 1 (GBC V1)...\")\n",
    "pipeline_gbc_v1 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v1),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "pipeline_gbc_v1.set_params(**best_params_gbc_v1)\n",
    "pipeline_gbc_v1.fit(X_v1, y_v1)\n",
    "print(\"Model 1 training complete.\")\n",
    "\n",
    "# --- Build and Train Model 2 (GBC V2 features + OHE) ---\n",
    "print(\"Training Model 2 (GBC V2 Features + OHE)...\")\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2_like),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "pipeline_gbc_v2.set_params(**best_params_gbc_v2)\n",
    "pipeline_gbc_v2.fit(X_v2_like, y_v2_like)\n",
    "print(\"Model 2 training complete.\")\n",
    "\n",
    "# --- Build and Train Model 3 (CatBoost V4 features - Manual Prep) ---\n",
    "print(\"Training Model 3 (CatBoost V4 Features - Manual Prep)...\")\n",
    "# Identify final features for CatBoost training\n",
    "categorical_features_v4_names = X_v4.select_dtypes(include='object').columns.tolist() # Names needed for CatBoost model init\n",
    "\n",
    "model_catboost_final = CatBoostClassifier(\n",
    "    random_state=42, verbose=0, loss_function='Logloss', eval_metric='AUC',\n",
    "    cat_features=categorical_features_v4_names, **best_params_catboost_v4 # Apply best params\n",
    ")\n",
    "# Train directly on the V4 data (already imputed and scaled in preprocess function)\n",
    "model_catboost_final.fit(X_v4, y_v4)\n",
    "print(\"Model 3 training complete.\")\n",
    "\n",
    "# --- Predict Probabilities on Test Set ---\n",
    "print(\"Predicting probabilities...\")\n",
    "probs_gbc_v1 = pipeline_gbc_v1.predict_proba(X_test_v1)[:, 1]\n",
    "probs_gbc_v2 = pipeline_gbc_v2.predict_proba(X_test_v2_like)[:, 1]\n",
    "probs_catboost = model_catboost_final.predict_proba(X_test_v4)[:, 1] # Predict on manually prepped test data\n",
    "\n",
    "# --- Ensemble Averaging ---\n",
    "print(\"Averaging predictions...\")\n",
    "avg_probs_3 = (probs_gbc_v1 + probs_gbc_v2 + probs_catboost) / 3\n",
    "final_predictions = (avg_probs_3 >= 0.5).astype(int)\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df_ensemble3 = pd.DataFrame({'ID': test_df['ID'], 'Target': final_predictions})\n",
    "submission_filename_ensemble3 = 'submission_ensemble_3model_avg.csv'\n",
    "submission_df_ensemble3.to_csv(submission_filename_ensemble3, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_ensemble3}' created successfully.\")\n",
    "print(submission_df_ensemble3.head())\n",
    "print(f\"\\nPredicted target distribution (3-Model Ensemble):\\n{submission_df_ensemble3['Target'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8d069-758d-4f75-aeda-1d9474293b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV # Using Lasso for feature selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function V5 (Adds Poly/Interaction) ---\n",
    "def preprocess_data_v5_poly_interact(df, is_train=True, latest_date=None, fit_imputers=None, poly_feature_names=None):\n",
    "    \"\"\" V4 + Polynomial/Interaction features, returns df before scaling \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    target_col = 'Target'\n",
    "    # --- Initial Feature Creation (Same as V4) ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v5 # Use distinct name\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        global_latest_date_v5 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v5\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1)\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # --- Imputation (Manual) ---\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed for test\")\n",
    "        df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    # Reconstruct DataFrame\n",
    "    original_cols = df_processed.columns\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "\n",
    "    # --- Create Derived Features (Post-Imputation) ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    # --- Add Interaction & Polynomial Features ---\n",
    "    # Select top numerical features based on previous importance analysis\n",
    "    poly_cols = ['Recency', 'MntWines', 'Total_CmpAccepted', 'Total_Purchases', 'NumWebPurchases', 'Customer_Lifetime', 'Total_Mnt', 'Age']\n",
    "    # Ensure these columns exist after previous steps\n",
    "    poly_cols = [col for col in poly_cols if col in df_processed.columns]\n",
    "\n",
    "    if is_train:\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False) # Try full quadratic\n",
    "        poly_features = poly.fit_transform(df_processed[poly_cols])\n",
    "        # Get new feature names\n",
    "        poly_feature_names = poly.get_feature_names_out(poly_cols)\n",
    "        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_processed.index)\n",
    "        # Drop original columns used in poly features to avoid duplication? Optional, keep for now.\n",
    "        # df_processed = df_processed.drop(columns=poly_cols)\n",
    "        df_processed = pd.concat([df_processed, poly_df.drop(columns=poly_cols)], axis=1) # Add only new terms\n",
    "        print(f\"Added {len(poly_feature_names) - len(poly_cols)} polynomial/interaction features.\")\n",
    "    else:\n",
    "        if poly_feature_names is None:\n",
    "             raise ValueError(\"Polynomial feature names must be provided for test data transformation.\")\n",
    "        # Need to apply the *same* transformation. Requires fitted poly object or careful reconstruction.\n",
    "        # Easier approach for now: Recalculate on test, assuming same features are generated.\n",
    "        # THIS IS NOT IDEAL for production but simpler for testing feature impact here.\n",
    "        # A robust solution uses the fitted poly object from training.\n",
    "        poly_test = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "        poly_features_test = poly_test.fit_transform(df_processed[poly_cols])\n",
    "        poly_feature_names_test = poly_test.get_feature_names_out(poly_cols) # Get names from test fit\n",
    "        poly_df_test = pd.DataFrame(poly_features_test, columns=poly_feature_names_test, index=df_processed.index)\n",
    "\n",
    "        # Align columns based on training names (important!)\n",
    "        poly_df_test_aligned = pd.DataFrame(index=poly_df_test.index) # Empty df with same index\n",
    "        for col_name in poly_feature_names: # Iterate through names FROM TRAINING\n",
    "            if col_name in poly_df_test.columns and col_name not in poly_cols: # Check if exists in test and is a new term\n",
    "                poly_df_test_aligned[col_name] = poly_df_test[col_name]\n",
    "            elif col_name not in poly_cols: # If missing in test, add column of zeros\n",
    "                poly_df_test_aligned[col_name] = 0\n",
    "\n",
    "        df_processed = pd.concat([df_processed, poly_df_test_aligned], axis=1)\n",
    "\n",
    "    # --- Manual Interaction Example ---\n",
    "    if 'Recency' in df_processed.columns and 'Total_CmpAccepted' in df_processed.columns:\n",
    "         df_processed['Recency_x_Cmp'] = df_processed['Recency'] * df_processed['Total_CmpAccepted']\n",
    "\n",
    "\n",
    "    # Convert final categoricals to string (important BEFORE pipeline)\n",
    "    final_categorical_features = df_processed.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features:\n",
    "        df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str) # Ensure all column names are strings\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "\n",
    "\n",
    "    return df_processed, fit_imputers, poly_feature_names # Return poly names for test set\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V5 ---\n",
    "print(\"Preprocessing V5 (Poly/Interaction Features)...\")\n",
    "train_df_processed_v5, fitted_imputers_v5, poly_names_v5 = preprocess_data_v5_poly_interact(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v5' not in globals(): global_latest_date_v5 = datetime.datetime.now() + datetime.timedelta(days=1); print(f\"Error: global_latest_date_v5 not set. Using fallback: {global_latest_date_v5}\")\n",
    "test_df_processed_v5, _, _ = preprocess_data_v5_poly_interact(test_df.copy(), is_train=False, latest_date=global_latest_date_v5, fit_imputers=fitted_imputers_v5, poly_feature_names=poly_names_v5)\n",
    "print(\"V5 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V5 ---\n",
    "X_v5 = train_df_processed_v5.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v5 = train_df_processed_v5['Target']\n",
    "X_test_v5 = test_df_processed_v5.drop('ID', axis=1, errors='ignore')\n",
    "# Align columns\n",
    "train_cols_v5 = X_v5.columns\n",
    "test_cols_v5 = X_test_v5.columns\n",
    "missing_in_test_v5 = set(train_cols_v5) - set(test_cols_v5)\n",
    "for c in missing_in_test_v5: X_test_v5[c] = 0\n",
    "missing_in_train_v5 = set(test_cols_v5) - set(train_cols_v5)\n",
    "for c in missing_in_train_v5: X_v5[c] = 0\n",
    "X_test_v5 = X_test_v5[train_cols_v5] # Ensure order\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipeline V5 (OHE + Scale) ---\n",
    "numerical_features_v5 = X_v5.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v5 = X_v5.select_dtypes(exclude=np.number).columns.tolist()\n",
    "print(f\"\\nV5 Numerical features ({len(numerical_features_v5)}): {len(numerical_features_v5)} features\") # Print count due to length\n",
    "# print(numerical_features_v5) # Optionally print list\n",
    "print(f\"V5 Categorical features ({len(categorical_features_v5)}): {categorical_features_v5}\")\n",
    "\n",
    "\n",
    "# Pipeline for GBC (Scale Numerics, OHE Categoricals)\n",
    "# Imputation already done in preprocess_data_v5\n",
    "numerical_pipeline_v5 = Pipeline([('scaler', StandardScaler())])\n",
    "categorical_pipeline_v5 = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v5 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v5, numerical_features_v5),\n",
    "    ('cat', categorical_pipeline_v5, categorical_features_v5)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Feature Selector ---\n",
    "# Using LassoCV for selection based on linear importance\n",
    "# Threshold='median' means select features with importance > median importance\n",
    "# Can also use a float like '1.25*mean' or a specific value\n",
    "selector = SelectFromModel(\n",
    "    estimator=LassoCV(cv=3, random_state=42, max_iter=2000), # LassoCV finds best alpha\n",
    "    threshold='median', # Select features with importance above the median\n",
    "    prefit=False # Estimator will be fit automatically\n",
    ")\n",
    "\n",
    "# --- Define Base Model: GBC ---\n",
    "base_model_gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# --- Create Full Pipeline with Selection ---\n",
    "pipeline_gbc_v5_select = Pipeline([\n",
    "    ('preprocessor', preprocessor_v5),\n",
    "    ('selector', selector), # Add feature selection step\n",
    "    ('classifier', base_model_gbc)\n",
    "])\n",
    "\n",
    "\n",
    "# --- Define Parameter Grid for GBC V5 (Focus on GBC Params) ---\n",
    "# Use previous best GBC V2 params as a guide\n",
    "param_dist_gbc_v5 = {\n",
    "    'classifier__n_estimators': [200, 300, 400],\n",
    "    'classifier__learning_rate': [0.03, 0.05, 0.07, 0.1],\n",
    "    'classifier__max_depth': [2, 3],\n",
    "    'classifier__min_samples_leaf': [15, 20, 25],\n",
    "    'classifier__min_samples_split': [20, 30, 40],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8],\n",
    "    'classifier__max_features': ['sqrt', 'log2', None]\n",
    "    # Can also tune selector threshold if needed: 'selector__threshold': ['median', 'mean']\n",
    "}\n",
    "\n",
    "\n",
    "# --- Set up K-Fold Strategy ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for GBC V5 ---\n",
    "N_ITER_GBC_V5 = 60 # Adjust iterations based on time constraints\n",
    "SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "random_search_gbc_v5 = RandomizedSearchCV(\n",
    "    estimator=pipeline_gbc_v5_select,\n",
    "    param_distributions=param_dist_gbc_v5,\n",
    "    n_iter=N_ITER_GBC_V5,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for GBC V5 (Poly/Interact + Selection) with {N_ITER_GBC_V5} iterations...\")\n",
    "random_search_gbc_v5.fit(X_v5, y_v5)\n",
    "print(\"GBC V5 RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best GBC V5 Results ---\n",
    "print(\"\\n--- GBC V5 (Poly/Interact + Selection) Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_gbc_v5.best_score_:.5f}\") # More precision\n",
    "print(\"Best Parameters:\")\n",
    "best_params_gbc_v5 = random_search_gbc_v5.best_params_\n",
    "for param, value in best_params_gbc_v5.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final GBC V5 Model with Best Parameters ---\n",
    "print(\"\\nTraining final GBC V5 model on the entire training set...\")\n",
    "best_pipeline_gbc_v5 = random_search_gbc_v5.best_estimator_\n",
    "print(\"Final GBC V5 model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Tuned GBC V5 ---\n",
    "print(\"Predicting on test data using the tuned GBC V5 model...\")\n",
    "test_predictions_gbc_v5 = best_pipeline_gbc_v5.predict(X_test_v5)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for GBC V5 ---\n",
    "submission_df_gbc_v5 = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_gbc_v5})\n",
    "submission_filename_gbc_v5 = 'submission_tuned_gbc_v5_poly_select.csv'\n",
    "submission_df_gbc_v5.to_csv(submission_filename_gbc_v5, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_gbc_v5}' created successfully.\")\n",
    "print(submission_df_gbc_v5.head())\n",
    "print(f\"\\nPredicted target distribution (GBC V5):\\n{submission_df_gbc_v5['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* GBC V5 model on the training set\n",
    "train_preds_gbc_v5_tuned = best_pipeline_gbc_v5.predict(X_v5)\n",
    "train_accuracy_gbc_v5_tuned = accuracy_score(y_v5, train_preds_gbc_v5_tuned)\n",
    "train_roc_auc_gbc_v5_tuned = roc_auc_score(y_v5, best_pipeline_gbc_v5.predict_proba(X_v5)[:, 1])\n",
    "print(f\"\\n--- Tuned GBC V5 Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_gbc_v5_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v5_tuned:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34369f-7e2d-478c-afdf-da9eee1abbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier # Import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression # Meta-model\n",
    "import datetime\n",
    "import warnings\n",
    "try:\n",
    "    from catboost import CatBoostClassifier # Keep for potential future use\n",
    "except ImportError:\n",
    "    print(\"CatBoost not found.\")\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions ---\n",
    "# Using V1 function (for GBC V1) and V4 function (as base for GBC V2 structure)\n",
    "\n",
    "# Function V1 (Correct definition for GBC V1 - returns only DataFrame)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    \"\"\" V1 features with simplified categoricals, basic imputation \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'; current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v1 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0)\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    cols_to_impute_num = ['Age', 'Income', 'Customer_Lifetime', 'Total_Mnt']\n",
    "    for col in cols_to_impute_num:\n",
    "         if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    cat_cols_simple = ['Marital_Status', 'Education']\n",
    "    for col in cat_cols_simple:\n",
    "        if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# Function V4 (Manual Impute/Scale) - Used as basis for GBC V2 features\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    \"\"\" V2 features, keeps original categoricals, handles imputation+scaling manually \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v4 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1)\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "    # Identify initial features *before* imputation\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # Imputation\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median'); imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent'); imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed\")\n",
    "        df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    # Reconstruct DataFrame\n",
    "    original_cols = df_processed.columns\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "    # Create Derived Features\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    # Scaling\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler needed\")\n",
    "        df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "    # Final Type Conversion\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V1 ---\n",
    "print(\"Preprocessing V1...\")\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "\n",
    "# --- Apply Preprocessing V4 (for GBC V2 Base) ---\n",
    "print(\"\\nPreprocessing V4 (for GBC V2 base)...\")\n",
    "train_df_processed_v4, fitted_imputers_v4, fitted_scaler_v4 = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers_v4, fit_scaler=fitted_scaler_v4)\n",
    "print(\"V4 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V1 (Target for GBC V1) ---\n",
    "X_v1 = train_df_processed_v1.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v1 = train_df_processed_v1['Target']\n",
    "X_test_v1 = test_df_processed_v1.drop('ID', axis=1, errors='ignore')\n",
    "train_cols_v1 = X_v1.columns; test_cols_v1 = X_test_v1.columns\n",
    "missing_in_test_v1 = set(train_cols_v1) - set(test_cols_v1)\n",
    "for c in missing_in_test_v1: X_test_v1[c] = 0\n",
    "missing_in_train_v1 = set(test_cols_v1) - set(train_cols_v1)\n",
    "for c in missing_in_train_v1: X_v1[c] = 0\n",
    "X_test_v1 = X_test_v1[train_cols_v1]\n",
    "\n",
    "# --- Prepare Data V4 (Target for GBC V2) ---\n",
    "# StackingClassifier needs the *same* X for all base estimators usually.\n",
    "# So we use V4 data structure for BOTH GBC models in stacking.\n",
    "X_stack = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_stack = train_df_processed_v4['Target'] # Target is the same\n",
    "X_test_stack = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "# Align Stacking Data\n",
    "train_cols_stack = X_stack.columns; test_cols_stack = X_test_stack.columns\n",
    "missing_in_test_stack = set(train_cols_stack) - set(test_cols_stack)\n",
    "for c in missing_in_test_stack: X_test_stack[c] = 0\n",
    "missing_in_train_stack = set(test_cols_stack) - set(train_cols_stack)\n",
    "for c in missing_in_train_stack: X_stack[c] = 0\n",
    "X_test_stack = X_test_stack[train_cols_stack]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines for Base Models in Stacking ---\n",
    "\n",
    "# Preprocessor for GBC V1 base model (Simplified Cats + OHE + Scale)\n",
    "# Needs to operate on the common X_stack structure now\n",
    "numerical_features_stack = X_stack.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_stack_orig = ['Education', 'Marital_Status'] # Original cats in V4 structure\n",
    "# Apply V1 simplification logic here before OHE\n",
    "def simplify_cats_v1(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Marital_Status'] = df_copy['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "    df_copy['Education'] = df_copy['Education'].replace({'2n Cycle': 'Master'})\n",
    "    return df_copy\n",
    "\n",
    "# Need FunctionTransformer if simplifying within pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "simplifier_v1 = FunctionTransformer(simplify_cats_v1, validate=False)\n",
    "\n",
    "# Updated Preprocessor V1 for Stacking\n",
    "numerical_pipeline_stack = Pipeline([('scaler', StandardScaler())]) # Assumes imputation done\n",
    "categorical_pipeline_v1_stack = Pipeline([\n",
    "    # ('simplifier', simplifier_v1), # Apply simplification - tricky with FunctionTransformer and column names\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "# Need to select *original* cat columns for OHE in V1 pipeline\n",
    "preprocessor_v1_stack = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_stack, numerical_features_stack),\n",
    "    ('cat', categorical_pipeline_v1_stack, categorical_features_stack_orig)], # OHE original cats\n",
    "    remainder='passthrough') # Pass through other V4 features\n",
    "\n",
    "\n",
    "# Preprocessor for GBC V2 base model (Original Cats + Date Cats + OHE + Scale)\n",
    "categorical_features_v4_names = X_stack.select_dtypes(include='object').columns.tolist() # All object columns in V4 structure\n",
    "numerical_pipeline_stack_v2 = Pipeline([('scaler', StandardScaler())]) # Assumes imputation done\n",
    "categorical_pipeline_v2_stack = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v2_stack = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_stack_v2, numerical_features_stack),\n",
    "    ('cat', categorical_pipeline_v2_stack, categorical_features_v4_names)], # OHE *all* object cols\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define BEST Hyperparameters (from previous runs) ---\n",
    "best_params_gbc_v1_direct = { # No 'classifier__' prefix needed when setting on model directly\n",
    "    'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'sqrt', 'max_depth': 2,\n",
    "    'learning_rate': 0.08, 'random_state':42}\n",
    "\n",
    "best_params_gbc_v2_direct = { # Params that gave 0.848\n",
    "    'subsample': 0.7, 'n_estimators': 300, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'log2', 'max_depth': 2,\n",
    "    'learning_rate': 0.05, 'random_state':42}\n",
    "\n",
    "# --- Define Base Estimator Pipelines for Stacking ---\n",
    "# Base Estimator 1: GBC with V1-style preprocessing\n",
    "base_gbc_v1 = Pipeline([\n",
    "    # Need to handle simplification OR use V1 data. Using V4 data + OHE original cats is simpler for stacking input consistency.\n",
    "    # Let's redefine GBC V1 slightly: V4 features, but OHE only original cats, use V1 params.\n",
    "    ('preprocessor', preprocessor_v1_stack), # Scales all numerics, OHEs 'Education', 'Marital_Status'\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v1_direct))\n",
    "])\n",
    "\n",
    "# Base Estimator 2: GBC with V2-style preprocessing (all cats OHE'd)\n",
    "base_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2_stack), # Scales all numerics, OHEs all object columns\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v2_direct))\n",
    "])\n",
    "\n",
    "# List of base estimators for StackingClassifier\n",
    "base_estimators = [\n",
    "    ('gbc_v1_style', base_gbc_v1),\n",
    "    ('gbc_v2_style', base_gbc_v2)\n",
    "]\n",
    "\n",
    "# --- Define Meta-Model ---\n",
    "# Logistic Regression is a common choice, C=1 is default regularization\n",
    "meta_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "# --- Define Cross-Validation Strategy for Meta-Model ---\n",
    "# StackingClassifier uses this CV to generate the out-of-fold predictions for training the meta-model\n",
    "N_SPLITS_STACK = 5\n",
    "RANDOM_STATE_STACK = 42\n",
    "cv_stack = StratifiedKFold(n_splits=N_SPLITS_STACK, shuffle=True, random_state=RANDOM_STATE_STACK)\n",
    "\n",
    "# --- Create Stacking Classifier ---\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=cv_stack,\n",
    "    stack_method='predict_proba', # Use probabilities as input to meta-model\n",
    "    n_jobs=-1,\n",
    "    passthrough=False # Do not pass original features to final estimator\n",
    ")\n",
    "\n",
    "# --- Train Stacking Classifier ---\n",
    "print(\"\\nTraining Stacking Classifier...\")\n",
    "# Fit on the V4 data structure, the pipelines within handle specifics\n",
    "stacking_clf.fit(X_stack, y_stack)\n",
    "print(\"Stacking Classifier training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Stacking Classifier ---\n",
    "print(\"Predicting on test data using the Stacking Classifier...\")\n",
    "# Predict using the same V4 test data structure\n",
    "test_predictions_stacking = stacking_clf.predict(X_test_stack)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for Stacking ---\n",
    "submission_df_stacking = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_stacking})\n",
    "submission_filename_stacking = 'submission_stacking_gbc1_gbc2.csv'\n",
    "submission_df_stacking.to_csv(submission_filename_stacking, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_stacking}' created successfully.\")\n",
    "print(submission_df_stacking.head())\n",
    "print(f\"\\nPredicted target distribution (Stacking):\\n{submission_df_stacking['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate Stacking model on training data (less reliable due to OOF nature)\n",
    "train_preds_stacking = stacking_clf.predict(X_stack)\n",
    "train_accuracy_stacking = accuracy_score(y_stack, train_preds_stacking)\n",
    "# Getting reliable ROC AUC requires predict_proba on OOF preds or separate validation\n",
    "try:\n",
    "    train_roc_auc_stacking = roc_auc_score(y_stack, stacking_clf.predict_proba(X_stack)[:, 1])\n",
    "    print(f\"\\n--- Stacking Model Training Set Evaluation ---\")\n",
    "    print(f\"Accuracy: {train_accuracy_stacking:.4f}\")\n",
    "    print(f\"ROC AUC: {train_roc_auc_stacking:.5f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not calculate ROC AUC for stacking on training set: {e}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy_stacking:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abf392-78a3-4d9b-9e7a-8480f8b901f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function V6 (Refined V4) ---\n",
    "def preprocess_data_v6_refined(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    \"\"\" V4 features + Targeted Interactions - Manual Impute/Scale \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    target_col = 'Target'\n",
    "    # --- Step 1: Initial Feature Creation (Same as V4) ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v6 # Use distinct name\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v6 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v6 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v6\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    # df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek # Dropping this based on hypothetical low importance\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    # Drop Kidhome? Keep for now as Children uses it.\n",
    "    # df_processed.drop(['Kidhome'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1)\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # --- Identify initial feature types *before* imputation ---\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    # --- Step 2: Imputation ---\n",
    "    # ... (Imputation code same as V4) ...\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median'); imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent'); imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed\")\n",
    "        df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    # Reconstruct DataFrame\n",
    "    original_cols = df_processed.columns\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "\n",
    "    # --- Step 3: Create Derived Features (Post-Imputation) ---\n",
    "    # Ratios (Keep Wine, Meat, maybe drop Fruit?)\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    # df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0) # Drop based on low importance?\n",
    "\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    # --- Targeted Interactions (Post-Imputation) ---\n",
    "    # Only interact top features\n",
    "    df_processed['Recency_x_TotalPurch'] = df_processed['Recency'] * df_processed['Total_Purchases']\n",
    "    df_processed['Recency_x_MntWines'] = df_processed['Recency'] * df_processed['MntWines']\n",
    "    df_processed['MntWines_x_TotalCmp'] = df_processed['MntWines'] * df_processed['Total_CmpAccepted']\n",
    "    df_processed['TotalPurch_x_TotalCmp'] = df_processed['Total_Purchases'] * df_processed['Total_CmpAccepted']\n",
    "    df_processed['WebPurch_x_WebVisits'] = df_processed['NumWebPurchases'] * df_processed['NumWebVisitsMonth'] # Example\n",
    "\n",
    "\n",
    "    # --- Step 4: Scaling ---\n",
    "    # ... (Scaling code same as V4) ...\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler needed\")\n",
    "        df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "\n",
    "    # --- Step 5: Final Type Conversion ---\n",
    "    # ... (Type conversion same as V4) ...\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V6 ---\n",
    "print(\"\\nPreprocessing V6 (Refined Features)...\")\n",
    "train_df_processed_v6, fitted_imputers_v6, fitted_scaler_v6 = preprocess_data_v6_refined(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v6' not in globals(): global_latest_date_v6 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v6, _, _ = preprocess_data_v6_refined(test_df.copy(), is_train=False, latest_date=global_latest_date_v6, fit_imputers=fitted_imputers_v6, fit_scaler=fitted_scaler_v6)\n",
    "print(\"V6 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V6 ---\n",
    "X_v6 = train_df_processed_v6.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v6 = train_df_processed_v6['Target']\n",
    "X_test_v6 = test_df_processed_v6.drop('ID', axis=1, errors='ignore')\n",
    "# Align columns\n",
    "train_cols_v6 = X_v6.columns; test_cols_v6 = X_test_v6.columns\n",
    "missing_in_test_v6 = set(train_cols_v6) - set(test_cols_v6)\n",
    "for c in missing_in_test_v6: X_test_v6[c] = 0\n",
    "missing_in_train_v6 = set(test_cols_v6) - set(train_cols_v6)\n",
    "for c in missing_in_train_v6: X_v6[c] = 0\n",
    "X_test_v6 = X_test_v6[train_cols_v6]\n",
    "\n",
    "# --- Retune GBC on V6 Features ---\n",
    "print(\"\\nRetuning GBC on V6 features...\")\n",
    "# Define GBC V6 Preprocessor (OHE)\n",
    "numerical_features_v6 = X_v6.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v6 = X_v6.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# Pipeline assumes imputation/scaling done in preprocess_data_v6, just needs OHE\n",
    "categorical_pipeline_v6 = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_gbc_v6 = ColumnTransformer([\n",
    "    ('cat', categorical_pipeline_v6, categorical_features_v6)], # Only apply OHE to categoricals\n",
    "    remainder='passthrough') # Numericals are already scaled\n",
    "\n",
    "pipeline_gbc_v6 = Pipeline([\n",
    "    ('preprocessor', preprocessor_gbc_v6),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Use similar param grid as GBC V2\n",
    "param_dist_gbc_v6 = {\n",
    "    'classifier__n_estimators': [200, 300, 400],\n",
    "    'classifier__learning_rate': [0.03, 0.05, 0.07], # Slightly adjust maybe\n",
    "    'classifier__max_depth': [2, 3],\n",
    "    'classifier__min_samples_leaf': [15, 20, 25],\n",
    "    'classifier__min_samples_split': [20, 30, 40],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8],\n",
    "    'classifier__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "N_ITER_GBC_V6 = 50 # Fewer iterations maybe? Or keep 75\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search_gbc_v6 = RandomizedSearchCV(\n",
    "    estimator=pipeline_gbc_v6, param_distributions=param_dist_gbc_v6,\n",
    "    n_iter=N_ITER_GBC_V6, scoring='roc_auc', cv=skf, n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "random_search_gbc_v6.fit(X_v6, y_v6)\n",
    "print(\"GBC V6 Tuning Results:\")\n",
    "print(f\"Best Score: {random_search_gbc_v6.best_score_:.5f}\")\n",
    "print(f\"Best Params: {random_search_gbc_v6.best_params_}\")\n",
    "best_pipeline_gbc_v6 = random_search_gbc_v6.best_estimator_\n",
    "\n",
    "\n",
    "# --- Retune CatBoost on V6 Features ---\n",
    "print(\"\\nRetuning CatBoost on V6 features...\")\n",
    "categorical_features_v6_names = X_v6.select_dtypes(include='object').columns.tolist()\n",
    "model_catboost_v6 = CatBoostClassifier(\n",
    "    random_state=42, verbose=0, loss_function='Logloss', eval_metric='AUC',\n",
    "    cat_features=categorical_features_v6_names\n",
    ")\n",
    "# Use similar param grid as CatBoost V4\n",
    "param_dist_catboost_v6 = {\n",
    "    'iterations': [100, 200, 300, 500], # Maybe fewer iterations needed now?\n",
    "    'learning_rate': [0.03, 0.05, 0.07, 0.1],\n",
    "    'depth': [4, 5, 6],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "    'border_count': [32, 64],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "}\n",
    "N_ITER_CATBOOST_V6 = 50\n",
    "random_search_catboost_v6 = RandomizedSearchCV(\n",
    "    estimator=model_catboost_v6, param_distributions=param_dist_catboost_v6,\n",
    "    n_iter=N_ITER_CATBOOST_V6, scoring='roc_auc', cv=skf, n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "random_search_catboost_v6.fit(X_v6, y_v6) # Fit directly on manually prepped V6 data\n",
    "print(\"CatBoost V6 Tuning Results:\")\n",
    "print(f\"Best Score: {random_search_catboost_v6.best_score_:.5f}\")\n",
    "print(f\"Best Params: {random_search_catboost_v6.best_params_}\")\n",
    "best_model_catboost_v6 = random_search_catboost_v6.best_estimator_\n",
    "\n",
    "\n",
    "# --- Ensemble Averaging (GBC V6 + CatBoost V6) ---\n",
    "print(\"\\nCreating ensemble of retuned GBC V6 and CatBoost V6...\")\n",
    "probs_gbc_v6 = best_pipeline_gbc_v6.predict_proba(X_test_v6)[:, 1]\n",
    "probs_catboost_v6 = best_model_catboost_v6.predict_proba(X_test_v6)[:, 1]\n",
    "\n",
    "avg_probs_v6_ensemble = (probs_gbc_v6 + probs_catboost_v6) / 2\n",
    "final_predictions_v6_ensemble = (avg_probs_v6_ensemble >= 0.5).astype(int)\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df_ensemble_v6 = pd.DataFrame({'ID': test_df['ID'], 'Target': final_predictions_v6_ensemble})\n",
    "submission_filename_ensemble_v6 = 'submission_ensemble_v6_features.csv'\n",
    "submission_df_ensemble_v6.to_csv(submission_filename_ensemble_v6, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_ensemble_v6}' created successfully.\")\n",
    "print(submission_df_ensemble_v6.head())\n",
    "print(f\"\\nPredicted target distribution (Ensemble V6):\\n{submission_df_ensemble_v6['Target'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230d32f6-8499-4e49-afb7-4c83726c7076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing V1...\n",
      "V1 Preprocessing complete.\n",
      "\n",
      "Preprocessing V4 (for GBC V2)...\n",
      "V4 Preprocessing complete.\n",
      "\n",
      "Training Model 1 (GBC V1)...\n",
      "Model 1 training complete.\n",
      "Training Model 2 (GBC V4 Features + OHE)...\n",
      "Model 2 training complete.\n",
      "Predicting probabilities...\n",
      "Probability prediction complete.\n",
      "Averaging predictions with weights: GBC_V1=0.65, GBC_V2=0.35...\n",
      "\n",
      "Submission file 'submission_ensemble_gbc_weighted_45_55.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (Weighted Ensemble 45/55):\n",
      "Target\n",
      "0    0.6211\n",
      "1    0.3789\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# No CV needed here, just loading/training final models\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Removed: StackingClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "# Removed CatBoost import\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions ---\n",
    "\n",
    "# Function V1 (Correct definition for GBC V1 - returns only DataFrame)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    \"\"\" V1 features with simplified categoricals, basic imputation \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'; current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v1 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0)\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    cols_to_impute_num = ['Age', 'Income', 'Customer_Lifetime', 'Total_Mnt']\n",
    "    for col in cols_to_impute_num:\n",
    "         if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    cat_cols_simple = ['Marital_Status', 'Education']\n",
    "    for col in cat_cols_simple:\n",
    "        if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# Function V4 (Manual Impute/Scale) - Used as basis for GBC V2 features\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    \"\"\" V2 features, keeps original categoricals, handles imputation+scaling manually \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v4 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1)\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "    # Identify initial features *before* imputation\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # Imputation\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median'); imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent'); imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed\")\n",
    "        df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    # Reconstruct DataFrame\n",
    "    original_cols = df_processed.columns\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "    # Create Derived Features\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    # Scaling\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler needed\")\n",
    "        df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "    # Final Type Conversion\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V1 ---\n",
    "print(\"Preprocessing V1...\")\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "\n",
    "# --- Apply Preprocessing V4 (for GBC V2) ---\n",
    "print(\"\\nPreprocessing V4 (for GBC V2)...\")\n",
    "train_df_processed_v4, fitted_imputers_v4, fitted_scaler_v4 = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers_v4, fit_scaler=fitted_scaler_v4)\n",
    "print(\"V4 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V1 (for GBC V1) ---\n",
    "X_v1 = train_df_processed_v1.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v1 = train_df_processed_v1['Target']\n",
    "X_test_v1 = test_df_processed_v1.drop('ID', axis=1, errors='ignore')\n",
    "# Align V1\n",
    "train_cols_v1 = X_v1.columns; test_cols_v1 = X_test_v1.columns\n",
    "missing_in_test_v1 = set(train_cols_v1) - set(test_cols_v1); [X_test_v1.insert(loc=X_v1.columns.get_loc(c), column=c, value=0) for c in missing_in_test_v1] # Insert missing cols at correct pos\n",
    "missing_in_train_v1 = set(test_cols_v1) - set(train_cols_v1); [X_v1.insert(loc=X_test_v1.columns.get_loc(c), column=c, value=0) for c in missing_in_train_v1]\n",
    "X_test_v1 = X_test_v1[train_cols_v1]\n",
    "\n",
    "\n",
    "# --- Prepare Data V4 (for GBC V2) ---\n",
    "X_v4 = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v4 = train_df_processed_v4['Target'] # Target is the same for both\n",
    "X_test_v4 = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "# Align V4\n",
    "train_cols_v4 = X_v4.columns; test_cols_v4 = X_test_v4.columns\n",
    "missing_in_test_v4 = set(train_cols_v4) - set(test_cols_v4); [X_test_v4.insert(loc=X_v4.columns.get_loc(c), column=c, value=0) for c in missing_in_test_v4]\n",
    "missing_in_train_v4 = set(test_cols_v4) - set(train_cols_v4); [X_v4.insert(loc=X_test_v4.columns.get_loc(c), column=c, value=0) for c in missing_in_train_v4]\n",
    "X_test_v4 = X_test_v4[train_cols_v4]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines V1 (GBC V1 requires OHE on simplified cats) ---\n",
    "numerical_features_v1 = X_v1.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v1 = X_v1.select_dtypes(exclude=np.number).columns.tolist() # Should be simplified cats\n",
    "numerical_pipeline_v1 = Pipeline([('scaler', StandardScaler())]) # Imputed in func\n",
    "categorical_pipeline_v1 = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]) # Imputed in func\n",
    "preprocessor_v1 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v1, numerical_features_v1),\n",
    "    ('cat', categorical_pipeline_v1, categorical_features_v1)], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines V4 (GBC V2 requires OHE on original cats + dates) ---\n",
    "numerical_features_v4 = X_v4.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v4 = X_v4.select_dtypes(exclude=np.number).columns.tolist() # Original cats + dates as str\n",
    "# V4 data is already imputed and scaled, just need OHE for GBC V2\n",
    "categorical_pipeline_v4_ohe = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v4_ohe = ColumnTransformer([\n",
    "    ('cat', categorical_pipeline_v4_ohe, categorical_features_v4)], # OHE the string columns\n",
    "    remainder='passthrough') # Pass through already scaled numericals\n",
    "\n",
    "\n",
    "# --- Define BEST Hyperparameters ---\n",
    "best_params_gbc_v1 = { # For V1 features\n",
    "    'classifier__subsample': 0.6, 'classifier__n_estimators': 200, 'classifier__min_samples_split': 20,\n",
    "    'classifier__min_samples_leaf': 20, 'classifier__max_features': 'sqrt', 'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.08}\n",
    "\n",
    "best_params_gbc_v2 = { # For V4 features + OHE\n",
    "    'classifier__subsample': 0.7, 'classifier__n_estimators': 300, 'classifier__min_samples_split': 20,\n",
    "    'classifier__min_samples_leaf': 20, 'classifier__max_features': 'log2', 'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.05}\n",
    "\n",
    "\n",
    "# --- Build and Train Model 1 (GBC V1) ---\n",
    "print(\"\\nTraining Model 1 (GBC V1)...\")\n",
    "pipeline_gbc_v1 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v1),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "pipeline_gbc_v1.set_params(**best_params_gbc_v1)\n",
    "pipeline_gbc_v1.fit(X_v1, y_v1) # Train on V1 data\n",
    "print(\"Model 1 training complete.\")\n",
    "\n",
    "# --- Build and Train Model 2 (GBC V4 features + OHE) ---\n",
    "print(\"Training Model 2 (GBC V4 Features + OHE)...\")\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v4_ohe), # Use the OHE preprocessor for V4 data\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "pipeline_gbc_v2.set_params(**best_params_gbc_v2)\n",
    "pipeline_gbc_v2.fit(X_v4, y_v4) # Train on V4 data\n",
    "print(\"Model 2 training complete.\")\n",
    "\n",
    "\n",
    "# --- Predict Probabilities on Test Set ---\n",
    "print(\"Predicting probabilities...\")\n",
    "probs_gbc_v1 = pipeline_gbc_v1.predict_proba(X_test_v1)[:, 1] # Predict on V1 test data\n",
    "probs_gbc_v2 = pipeline_gbc_v2.predict_proba(X_test_v4)[:, 1] # Predict on V4 test data\n",
    "print(\"Probability prediction complete.\")\n",
    "\n",
    "# --- Ensemble Weighted Averaging ---\n",
    "# Give slightly more weight to GBC V2 based on its individual LB score being better\n",
    "weight_v1 = 0.65 # Weight for GBC V1 (Simplified Features)\n",
    "weight_v2 = 0.35 # Weight for GBC V2 (V4 Features + OHE)\n",
    "print(f\"Averaging predictions with weights: GBC_V1={weight_v1}, GBC_V2={weight_v2}...\")\n",
    "\n",
    "weighted_avg_probs = (weight_v1 * probs_gbc_v1 + weight_v2 * probs_gbc_v2)\n",
    "\n",
    "# Convert probabilities to 0/1 using 0.5 threshold\n",
    "final_predictions = (weighted_avg_probs >= 0.5).astype(int)\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df_ensemble_weighted = pd.DataFrame({'ID': test_df['ID'], 'Target': final_predictions})\n",
    "# Use a new filename to track weights\n",
    "submission_filename_ensemble_weighted = 'submission_ensemble_gbc_weighted_45_55.csv'\n",
    "submission_df_ensemble_weighted.to_csv(submission_filename_ensemble_weighted, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_ensemble_weighted}' created successfully.\")\n",
    "print(submission_df_ensemble_weighted.head())\n",
    "print(f\"\\nPredicted target distribution (Weighted Ensemble 45/55):\\n{submission_df_ensemble_weighted['Target'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8520df-603b-462c-9ebd-946f3d275e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function V7 ---\n",
    "def preprocess_data_v7(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None, simplify_cats=False):\n",
    "    \"\"\" V4 + Log Transforms + New Interactions, optional cat simplification \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'\n",
    "    # --- Step 1: Initial Feature Creation ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        # Use unique global name for date\n",
    "        global global_latest_date_v7\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v7 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v7 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v7\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1)\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed['Spend_Habits_Denom'] = df_processed['Recency'].replace(0, 1) # Avoid /0\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # --- Optional Categorical Simplification ---\n",
    "    if simplify_cats:\n",
    "        df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "        df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # --- Identify initial feature types *before* imputation ---\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    # --- Step 2: Imputation ---\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median'); imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent'); imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed\")\n",
    "        df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    # Reconstruct DataFrame\n",
    "    original_cols = df_processed.columns\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "\n",
    "    # --- Step 3: Create Derived Features (Post-Imputation) ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    # df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0) # Maybe keep dropped\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed['Spending_Habits'] = (df_processed['Total_Mnt'] / df_processed['Spend_Habits_Denom']).fillna(0) # New\n",
    "    df_processed['Loyalty_Value'] = df_processed['Customer_Lifetime'] * df_processed['Total_Purchases'] # New\n",
    "    df_processed['Income_x_Age'] = df_processed['Income'] * df_processed['Age'] # New\n",
    "    df_processed['WineLover_Interact'] = df_processed['MntWines'] * df_processed['Total_CmpAccepted'] # New\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom', 'Spend_Habits_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    # --- Step 4: Log Transformation (Post-Imputation, Pre-Scaling) ---\n",
    "    log_cols = ['Income', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
    "                'MntSweetProducts', 'MntGoldProds', 'Total_Mnt', 'Income_per_Person',\n",
    "                'Spending_per_Purchase', 'Spending_Habits', 'Loyalty_Value', 'Income_x_Age',\n",
    "                'WineLover_Interact'] # Add new interaction terms here\n",
    "    for col in log_cols:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[col] = np.log1p(df_processed[col]) # Use log1p to handle zeros\n",
    "\n",
    "    # --- Step 5: Scaling ---\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler needed\")\n",
    "        df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "\n",
    "    # --- Step 6: Final Type Conversion for Categoricals ---\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V7 (Original Cats) ---\n",
    "print(\"\\nPreprocessing V7 (Original Cats + New Features)...\")\n",
    "train_df_v7_orig, imp_v7_orig, scaler_v7_orig = preprocess_data_v7(train_df.copy(), is_train=True, simplify_cats=False)\n",
    "if 'global_latest_date_v7' not in globals(): global_latest_date_v7 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_v7_orig, _, _ = preprocess_data_v7(test_df.copy(), is_train=False, latest_date=global_latest_date_v7, fit_imputers=imp_v7_orig, fit_scaler=scaler_v7_orig, simplify_cats=False)\n",
    "print(\"V7 Orig Cats Preprocessing complete.\")\n",
    "\n",
    "# --- Apply Preprocessing V7 (Simplified Cats) ---\n",
    "print(\"\\nPreprocessing V7 (Simplified Cats + New Features)...\")\n",
    "train_df_v7_simp, imp_v7_simp, scaler_v7_simp = preprocess_data_v7(train_df.copy(), is_train=True, simplify_cats=True)\n",
    "# Use the same global date\n",
    "test_df_v7_simp, _, _ = preprocess_data_v7(test_df.copy(), is_train=False, latest_date=global_latest_date_v7, fit_imputers=imp_v7_simp, fit_scaler=scaler_v7_simp, simplify_cats=True)\n",
    "print(\"V7 Simplified Cats Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data & Tune GBC for BOTH V7 versions ---\n",
    "\n",
    "results = {}\n",
    "\n",
    "for version_name, train_processed, test_processed in [\n",
    "    ('V7_Orig_Cats', train_df_v7_orig, test_df_v7_orig),\n",
    "    ('V7_Simp_Cats', train_df_v7_simp, test_df_v7_simp)\n",
    "]:\n",
    "    print(f\"\\n--- Processing and Tuning for: {version_name} ---\")\n",
    "    try:\n",
    "        X = train_processed.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "        y = train_processed['Target']\n",
    "        X_test = test_processed.drop('ID', axis=1, errors='ignore')\n",
    "\n",
    "        # Align columns\n",
    "        train_cols = X.columns; test_cols = X_test.columns\n",
    "        missing_in_test = set(train_cols) - set(test_cols); [X_test.insert(loc=X.columns.get_loc(c), column=c, value=0) for c in missing_in_test]\n",
    "        missing_in_train = set(test_cols) - set(train_cols); [X.insert(loc=X_test.columns.get_loc(c), column=c, value=0) for c in missing_in_train]\n",
    "        X_test = X_test[train_cols]\n",
    "\n",
    "        # Define Preprocessor (OHE + Scaling already done) -> Just need OHE now\n",
    "        numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "        print(f\"Features for {version_name} - Num: {len(numerical_features)}, Cat: {len(categorical_features)}\")\n",
    "\n",
    "        # Preprocessor just applies OHE to string cols, passes through scaled numericals\n",
    "        cat_pipeline = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "        preprocessor = ColumnTransformer([\n",
    "             ('cat', cat_pipeline, categorical_features)],\n",
    "             remainder='passthrough') # Pass scaled numericals through\n",
    "\n",
    "        # GBC Pipeline\n",
    "        pipeline_gbc = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "        ])\n",
    "\n",
    "        # Tuning Grid (reuse V6 grid maybe)\n",
    "        param_dist_gbc = {\n",
    "            'classifier__n_estimators': [200, 300, 400, 500], # More estimators might be needed\n",
    "            'classifier__learning_rate': [0.02, 0.03, 0.05, 0.07],\n",
    "            'classifier__max_depth': [2, 3, 4], # Allow slightly deeper?\n",
    "            'classifier__min_samples_leaf': [15, 20, 25, 30],\n",
    "            'classifier__min_samples_split': [30, 40, 50],\n",
    "            'classifier__subsample': [0.7, 0.8, 0.9],\n",
    "            'classifier__max_features': ['sqrt', 'log2', 0.7] # Add fraction\n",
    "        }\n",
    "        N_ITER_GBC = 75 # More iterations for tuning\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        random_search_gbc = RandomizedSearchCV(\n",
    "            estimator=pipeline_gbc, param_distributions=param_dist_gbc,\n",
    "            n_iter=N_ITER_GBC, scoring='roc_auc', cv=skf, n_jobs=-1, random_state=42, verbose=1\n",
    "        )\n",
    "\n",
    "        print(f\"Starting RandomizedSearchCV for GBC ({version_name})...\")\n",
    "        random_search_gbc.fit(X, y)\n",
    "        print(f\"GBC {version_name} Tuning Results:\")\n",
    "        print(f\"  Best Score: {random_search_gbc.best_score_:.5f}\")\n",
    "        print(f\"  Best Params: {random_search_gbc.best_params_}\")\n",
    "\n",
    "        # Store results\n",
    "        results[version_name] = {\n",
    "            'best_score': random_search_gbc.best_score_,\n",
    "            'best_params': random_search_gbc.best_params_,\n",
    "            'best_estimator': random_search_gbc.best_estimator_,\n",
    "            'X_test': X_test # Store the corresponding test set\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error processing {version_name}: {e}\")\n",
    "        results[version_name] = {'best_score': -1, 'best_params': None, 'best_estimator': None, 'X_test': None}\n",
    "\n",
    "\n",
    "# --- Compare Results and Generate Best Submission ---\n",
    "best_version = None\n",
    "highest_cv_score = -1\n",
    "\n",
    "print(\"\\n--- Comparison of V7 Feature Sets ---\")\n",
    "for version, data in results.items():\n",
    "    print(f\"{version}: CV ROC AUC = {data['best_score']:.5f}\")\n",
    "    if data['best_score'] > highest_cv_score:\n",
    "        highest_cv_score = data['best_score']\n",
    "        best_version = version\n",
    "\n",
    "if best_version:\n",
    "    print(f\"\\nSelecting '{best_version}' based on CV score.\")\n",
    "    best_pipeline = results[best_version]['best_estimator']\n",
    "    X_test_final = results[best_version]['X_test']\n",
    "\n",
    "    # --- Predict on Test Data ---\n",
    "    print(f\"Predicting on test data using the best model ({best_version})...\")\n",
    "    test_predictions_final = best_pipeline.predict(X_test_final)\n",
    "    print(\"Prediction complete.\")\n",
    "\n",
    "    # --- Generate Submission File ---\n",
    "    submission_df_final = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_final})\n",
    "    submission_filename_final = f'submission_gbc_{best_version}_tuned.csv'\n",
    "    submission_df_final.to_csv(submission_filename_final, index=False)\n",
    "\n",
    "    print(f\"\\nSubmission file '{submission_filename_final}' created successfully.\")\n",
    "    print(submission_df_final.head())\n",
    "    print(f\"\\nPredicted target distribution ({best_version}):\\n{submission_df_final['Target'].value_counts(normalize=True)}\")\n",
    "else:\n",
    "    print(\"\\nNo successful model tuning runs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1eb5a1-241f-4903-bdd6-3e52afc61926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing V1...\n",
      "V1 Preprocessing complete.\n",
      "\n",
      "Preprocessing V4 (for GBC V2 base)...\n",
      "V4 Preprocessing complete.\n",
      "\n",
      "Starting GridSearchCV for GBC V1...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "GBC V1 GridSearchCV finished.\n",
      "  Best Score: 0.92415\n",
      "  Best Params: {'classifier__learning_rate': 0.08, 'classifier__max_depth': 2, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 18, 'classifier__min_samples_split': 20, 'classifier__n_estimators': 200, 'classifier__subsample': 0.6}\n",
      "\n",
      "Starting GridSearchCV for GBC V2 (on V4 features)...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "GBC V2 GridSearchCV finished.\n",
      "  Best Score: 0.92102\n",
      "  Best Params: {'classifier__learning_rate': 0.05, 'classifier__max_depth': 2, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 22, 'classifier__min_samples_split': 20, 'classifier__n_estimators': 325, 'classifier__subsample': 0.7}\n",
      "\n",
      "Predicting probabilities with GridSearch Tuned models...\n",
      "Probability prediction complete.\n",
      "Averaging predictions with 50/50 weights...\n",
      "\n",
      "Submission file 'submission_ensemble_gbc_gridtuned_imputed.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (Grid Tuned Ensemble Imputed):\n",
      "Target\n",
      "0    0.627043\n",
      "1    0.372957\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions (V1 and V4 Corrected) ---\n",
    "\n",
    "# Function V1 (Correct definition for GBC V1 - returns only DataFrame)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    \"\"\" V1 features with simplified categoricals, basic imputation \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'; current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max(); reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan; df_processed['Age'] = reference_year - df_processed['Year_Birth']; df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v1 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt']); df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days; df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0)\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1); df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1); df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    cols_to_impute_num = ['Age', 'Income', 'Customer_Lifetime', 'Total_Mnt']\n",
    "    for col in cols_to_impute_num:\n",
    "         if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    cat_cols_simple = ['Marital_Status', 'Education']\n",
    "    for col in cat_cols_simple:\n",
    "        if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# Function V4 (Corrected Imputation Order)\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    \"\"\" V2 features, keeps original categoricals, handles imputation+scaling manually, adds final derived feat imputation \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'\n",
    "    # --- Step 1: Initial Feature Creation ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max(); reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan; df_processed['Age'] = reference_year - df_processed['Year_Birth']; df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v4 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month; df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year; df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt']); df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days; df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1); df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1); df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']; df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1); df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1); df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "    # Identify initial features *before* imputation\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist(); initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # --- Step 2: Imputation ---\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median'); imputer_num.fit(df_processed[initial_numerical_features]); df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent'); imputer_cat.fit(df_processed[initial_categorical_features]); df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed\"); df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features]); df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    # Reconstruct DataFrame\n",
    "    original_cols = df_processed.columns; imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "    # --- Step 3: Create Derived Features (Post-Imputation) ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1))\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1))\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1))\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom'])\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom'])\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    # --- Step 3.5: Impute NaNs/Infs from Derived Features ---\n",
    "    derived_cols = ['Wine_Ratio', 'Meat_Ratio', 'Fruit_Ratio', 'Income_per_Person', 'Spending_per_Purchase']\n",
    "    for col in derived_cols:\n",
    "        if col in df_processed.columns:\n",
    "             df_processed[col] = df_processed[col].replace([np.inf, -np.inf], np.nan)\n",
    "             # Fill NaNs - using 0 is simple, median might be better if distribution exists\n",
    "             col_median = df_processed[col].median()\n",
    "             df_processed[col] = df_processed[col].fillna(col_median if pd.notna(col_median) else 0) # Fill with median or 0 if median is NaN\n",
    "\n",
    "    # --- Step 4: Scaling ---\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler(); scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler needed\")\n",
    "        df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "    # --- Step 5: Final Type Conversion ---\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V1 & V4 ---\n",
    "print(\"Preprocessing V1...\")\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "\n",
    "print(\"\\nPreprocessing V4 (for GBC V2 base)...\")\n",
    "train_df_processed_v4, fitted_imputers_v4, fitted_scaler_v4 = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers_v4, fit_scaler=fitted_scaler_v4)\n",
    "print(\"V4 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V1 & V4 ---\n",
    "X_v1 = train_df_processed_v1.drop(['ID', 'Target'], axis=1, errors='ignore'); y_v1 = train_df_processed_v1['Target']\n",
    "X_test_v1 = test_df_processed_v1.drop('ID', axis=1, errors='ignore')\n",
    "train_cols_v1 = X_v1.columns; test_cols_v1 = X_test_v1.columns\n",
    "missing_in_test_v1 = set(train_cols_v1) - set(test_cols_v1); [X_test_v1.insert(loc=X_v1.columns.get_loc(c), column=c, value=0) for c in missing_in_test_v1]\n",
    "missing_in_train_v1 = set(test_cols_v1) - set(train_cols_v1); [X_v1.insert(loc=X_test_v1.columns.get_loc(c), column=c, value=0) for c in missing_in_train_v1]\n",
    "X_test_v1 = X_test_v1[train_cols_v1]\n",
    "\n",
    "X_v4 = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore'); y_v4 = train_df_processed_v4['Target']\n",
    "X_test_v4 = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "train_cols_v4 = X_v4.columns; test_cols_v4 = X_test_v4.columns\n",
    "missing_in_test_v4 = set(train_cols_v4) - set(test_cols_v4); [X_test_v4.insert(loc=X_v4.columns.get_loc(c), column=c, value=0) for c in missing_in_test_v4]\n",
    "missing_in_train_v4 = set(test_cols_v4) - set(train_cols_v4); [X_v4.insert(loc=X_test_v4.columns.get_loc(c), column=c, value=0) for c in missing_in_train_v4]\n",
    "X_test_v4 = X_test_v4[train_cols_v4]\n",
    "\n",
    "# --- Define Preprocessing Pipelines V1 & V4 (as used before) ---\n",
    "numerical_features_v1 = X_v1.select_dtypes(include=np.number).columns.tolist(); categorical_features_v1 = X_v1.select_dtypes(exclude=np.number).columns.tolist()\n",
    "numerical_pipeline_v1 = Pipeline([('scaler', StandardScaler())]); categorical_pipeline_v1 = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v1 = ColumnTransformer([('num', numerical_pipeline_v1, numerical_features_v1),('cat', categorical_pipeline_v1, categorical_features_v1)], remainder='passthrough')\n",
    "\n",
    "numerical_features_v4 = X_v4.select_dtypes(include=np.number).columns.tolist(); categorical_features_v4 = X_v4.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# Preprocessor for V4 data structure -> OHE categoricals, pass through numericals\n",
    "categorical_pipeline_v4_ohe = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "# Keep numericals separate for now to apply imputer after transformer if needed\n",
    "preprocessor_v4_ohe = ColumnTransformer([\n",
    "    ('cat', categorical_pipeline_v4_ohe, categorical_features_v4)],\n",
    "    remainder='passthrough') # Pass through numericals first\n",
    "\n",
    "\n",
    "# --- Define Base GBC Models (placeholder) ---\n",
    "base_gbc_v1 = GradientBoostingClassifier(random_state=42)\n",
    "base_gbc_v2 = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# --- Create Full Pipelines ---\n",
    "pipeline_gbc_v1 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v1),\n",
    "    # Add final imputer here too, just in case\n",
    "    ('final_imputer', SimpleImputer(strategy='median', missing_values=np.nan, add_indicator=False)),\n",
    "    ('classifier', base_gbc_v1)\n",
    "])\n",
    "\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v4_ohe), # This outputs OHE columns first, then numericals\n",
    "    # Add a final imputer step AFTER the ColumnTransformer\n",
    "    # This will impute NaNs in the final array passed to the classifier\n",
    "    ('final_imputer', SimpleImputer(strategy='median', missing_values=np.nan, add_indicator=False)), # add_indicator=False to avoid changing dimensions\n",
    "    ('classifier', base_gbc_v2)\n",
    "])\n",
    "\n",
    "\n",
    "# --- Define FOCUSED Parameter Grids for GridSearchCV ---\n",
    "# Add prefix for the new imputer step (though it has no params to tune)\n",
    "param_grid_gbc_v1 = {\n",
    "    'classifier__learning_rate': [0.07, 0.08, 0.09], 'classifier__n_estimators': [180, 200, 220],\n",
    "    'classifier__min_samples_leaf': [18, 20, 22], 'classifier__max_depth': [2],\n",
    "    'classifier__min_samples_split': [20], 'classifier__subsample': [0.6], 'classifier__max_features': ['sqrt']}\n",
    "\n",
    "param_grid_gbc_v2 = {\n",
    "    'classifier__learning_rate': [0.04, 0.05, 0.06], 'classifier__n_estimators': [275, 300, 325],\n",
    "    'classifier__min_samples_leaf': [18, 20, 22], 'classifier__max_depth': [2],\n",
    "    'classifier__min_samples_split': [20], 'classifier__subsample': [0.7], 'classifier__max_features': ['log2']}\n",
    "\n",
    "\n",
    "# --- Set up K-Fold Strategy ---\n",
    "N_SPLITS = 5; RANDOM_STATE_KFOLD = 42; skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD); SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "\n",
    "# --- Run GridSearchCV for GBC V1 ---\n",
    "print(f\"\\nStarting GridSearchCV for GBC V1...\")\n",
    "grid_search_gbc_v1 = GridSearchCV(estimator=pipeline_gbc_v1, param_grid=param_grid_gbc_v1, scoring=SCORING_METRIC, cv=skf, n_jobs=-1, verbose=1)\n",
    "grid_search_gbc_v1.fit(X_v1, y_v1)\n",
    "print(\"GBC V1 GridSearchCV finished.\"); print(f\"  Best Score: {grid_search_gbc_v1.best_score_:.5f}\"); print(f\"  Best Params: {grid_search_gbc_v1.best_params_}\")\n",
    "best_pipeline_gbc_v1_tuned = grid_search_gbc_v1.best_estimator_\n",
    "\n",
    "# --- Run GridSearchCV for GBC V2 ---\n",
    "print(f\"\\nStarting GridSearchCV for GBC V2 (on V4 features)...\")\n",
    "grid_search_gbc_v2 = GridSearchCV(estimator=pipeline_gbc_v2, param_grid=param_grid_gbc_v2, scoring=SCORING_METRIC, cv=skf, n_jobs=-1, verbose=1)\n",
    "grid_search_gbc_v2.fit(X_v4, y_v4) # Fit on V4 data\n",
    "print(\"GBC V2 GridSearchCV finished.\"); print(f\"  Best Score: {grid_search_gbc_v2.best_score_:.5f}\"); print(f\"  Best Params: {grid_search_gbc_v2.best_params_}\")\n",
    "best_pipeline_gbc_v2_tuned = grid_search_gbc_v2.best_estimator_\n",
    "\n",
    "\n",
    "# --- Predict Probabilities on Test Set using BEST Tuned Models ---\n",
    "print(\"\\nPredicting probabilities with GridSearch Tuned models...\")\n",
    "try:\n",
    "    probs_gbc_v1_tuned = best_pipeline_gbc_v1_tuned.predict_proba(X_test_v1)[:, 1]\n",
    "    probs_gbc_v2_tuned = best_pipeline_gbc_v2_tuned.predict_proba(X_test_v4)[:, 1] # Use V4 test data\n",
    "    print(\"Probability prediction complete.\")\n",
    "except ValueError as e:\n",
    "     print(f\"!!! Error during predict_proba: {e}\")\n",
    "     print(\"Investigate NaNs in X_test_v1 or X_test_v4 *after* preprocessing steps within the pipeline.\")\n",
    "     # Add code here to investigate Xt inside the pipeline if needed\n",
    "     exit()\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during prediction: {e}\")\n",
    "     exit()\n",
    "\n",
    "\n",
    "# --- Ensemble 50/50 Averaging ---\n",
    "print(\"Averaging predictions with 50/50 weights...\")\n",
    "avg_probs_tuned = (probs_gbc_v1_tuned + probs_gbc_v2_tuned) / 2\n",
    "final_predictions_tuned = (avg_probs_tuned >= 0.5).astype(int)\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df_ensemble_tuned = pd.DataFrame({'ID': test_df['ID'], 'Target': final_predictions_tuned})\n",
    "submission_filename_ensemble_tuned = 'submission_ensemble_gbc_gridtuned_imputed.csv' # New name\n",
    "submission_df_ensemble_tuned.to_csv(submission_filename_ensemble_tuned, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_ensemble_tuned}' created successfully.\")\n",
    "print(submission_df_ensemble_tuned.head())\n",
    "print(f\"\\nPredicted target distribution (Grid Tuned Ensemble Imputed):\\n{submission_df_ensemble_tuned['Target'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1df42349-4017-4031-aba6-76e188175575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing V1...\n",
      "V1 Preprocessing complete.\n",
      "\n",
      "Preprocessing V4 (Base for Stacking)...\n",
      "V4 Preprocessing complete.\n",
      "\n",
      "Preprocessing for Stacking Base Models...\n",
      "Preprocessing for base models complete.\n",
      "\n",
      "Training Stacking Classifier (GBC V1, GBC V2, LogReg)...\n",
      "Stacking Classifier training complete.\n",
      "Predicting on test data using the Stacking Classifier...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nGradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 291\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# --- Predict on Test Data with Stacking Classifier ---\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting on test data using the Stacking Classifier...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m test_predictions_stacking \u001b[38;5;241m=\u001b[39m stacking_clf\u001b[38;5;241m.\u001b[39mpredict(X_test_stack) \u001b[38;5;66;03m# Predict on V4 test data structure\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# --- Generate Submission File for Stacking ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:695\u001b[0m, in \u001b[0;36mStackingClassifier.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;129m@available_if\u001b[39m(_estimator_has(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params):\n\u001b[0;32m    676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict target for X.\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m        Predicted targets.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 695\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# Handle the multilabel-indicator case\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m    699\u001b[0m             [\n\u001b[0;32m    700\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder[target_idx]\u001b[38;5;241m.\u001b[39minverse_transform(target)\n\u001b[0;32m    701\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m target_idx, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    702\u001b[0m             ]\n\u001b[0;32m    703\u001b[0m         )\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:382\u001b[0m, in \u001b[0;36m_BaseStacking.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict target for X.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m    Predicted targets.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_estimator_\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:766\u001b[0m, in \u001b[0;36mStackingClassifier.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return class labels or probabilities for X for each estimator.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m        Prediction outputs for each estimator.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:307\u001b[0m, in \u001b[0;36m_BaseStacking._transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Concatenate and return the predictions of the estimators.\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    306\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(est, meth)(X)\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m ]\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concatenate_predictions(X, predictions)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:721\u001b[0m, in \u001b[0;36mPipeline.predict_proba\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    720\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[1;32m--> 721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict_proba(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    723\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n\u001b[0;32m    724\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:1666\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class probabilities for X.\u001b[39;00m\n\u001b[0;32m   1647\u001b[0m \n\u001b[0;32m   1648\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;124;03m        If the ``loss`` does not support probabilities.\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1666\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[0;32m   1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss\u001b[38;5;241m.\u001b[39mpredict_proba(raw_predictions)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:1564\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[0;32m   1547\u001b[0m \n\u001b[0;32m   1548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;124;03m        array of shape (n_samples,).\u001b[39;00m\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1564\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1565\u001b[0m         X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1566\u001b[0m     )\n\u001b[0;32m   1567\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\n\u001b[0;32m   1568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1065\u001b[0m         array,\n\u001b[0;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    124\u001b[0m     X,\n\u001b[0;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    130\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nGradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression # Import LogReg\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions (V1 and V4) ---\n",
    "\n",
    "# Function V1 (Correct definition for GBC V1 - returns only DataFrame)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    df_processed = df.copy(); target_col = 'Target'; current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max(); reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan; df_processed['Age'] = reference_year - df_processed['Year_Birth']; df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v1 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt']); df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days; df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0)\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1); df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1); df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    cols_to_impute_num = ['Age', 'Income', 'Customer_Lifetime', 'Total_Mnt']\n",
    "    for col in cols_to_impute_num:\n",
    "         if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    cat_cols_simple = ['Marital_Status', 'Education']\n",
    "    for col in cat_cols_simple:\n",
    "        if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# Function V4 (Manual Impute/Scale) - Used as basis for GBC V2 features\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    df_processed = df.copy(); target_col = 'Target'\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max(); reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan; df_processed['Age'] = reference_year - df_processed['Year_Birth']; df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v4 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month; df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year; df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt']); df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days; df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1); df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1); df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']; df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1); df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1); df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist(); initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median'); imputer_num.fit(df_processed[initial_numerical_features]); df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent'); imputer_cat.fit(df_processed[initial_categorical_features]); df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed\"); df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features]); df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    original_cols = df_processed.columns; imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)); df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)); df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1))\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']); df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom'])\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    derived_cols = ['Wine_Ratio', 'Meat_Ratio', 'Fruit_Ratio', 'Income_per_Person', 'Spending_per_Purchase'];\n",
    "    for col in derived_cols:\n",
    "        if col in df_processed.columns: df_processed[col] = df_processed[col].replace([np.inf, -np.inf], np.nan); col_median = df_processed[col].median(); df_processed[col] = df_processed[col].fillna(col_median if pd.notna(col_median) else 0)\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]; final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler(); scaler.fit(df_processed[final_numerical_features]); df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features]); fit_scaler = scaler\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler needed\"); df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "# --- Apply Preprocessing V1 & V4 ---\n",
    "print(\"Preprocessing V1...\")\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "print(\"\\nPreprocessing V4 (Base for Stacking)...\")\n",
    "train_df_processed_v4, fitted_imputers_v4, fitted_scaler_v4 = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers_v4, fit_scaler=fitted_scaler_v4)\n",
    "print(\"V4 Preprocessing complete.\")\n",
    "\n",
    "# --- Prepare Data V4 (Common Input for Stacking) ---\n",
    "X_stack = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_stack = train_df_processed_v4['Target']\n",
    "X_test_stack = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "# Align Stacking Data\n",
    "train_cols_stack = X_stack.columns; test_cols_stack = X_test_stack.columns\n",
    "missing_in_test_stack = set(train_cols_stack) - set(test_cols_stack); [X_test_stack.insert(loc=X_stack.columns.get_loc(c), column=c, value=0) for c in missing_in_test_stack]\n",
    "missing_in_train_stack = set(test_cols_stack) - set(train_cols_stack); [X_stack.insert(loc=X_test_stack.columns.get_loc(c), column=c, value=0) for c in missing_in_train_stack]\n",
    "X_test_stack = X_test_stack[train_cols_stack]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines for Base Models ---\n",
    "# NOTE: V4 data is already imputed and scaled. Pipelines just need OHE.\n",
    "\n",
    "# Preprocessor for GBC V1 Style (OHE Simplified Cats on V4 structure)\n",
    "categorical_features_v1_like = ['Education', 'Marital_Status'] # Columns to simplify and OHE\n",
    "other_features_v1 = [col for col in X_stack.columns if col not in categorical_features_v1_like]\n",
    "# Simplification Function\n",
    "def simplify_cats_v1(df):\n",
    "    df_copy = df.copy()\n",
    "    # Ensure columns exist before modifying\n",
    "    if 'Marital_Status' in df_copy.columns:\n",
    "        df_copy['Marital_Status'] = df_copy['Marital_Status'].astype(str).replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "    if 'Education' in df_copy.columns:\n",
    "        df_copy['Education'] = df_copy['Education'].astype(str).replace({'2n Cycle': 'Master'})\n",
    "    return df_copy\n",
    "simplifier_v1_tf = FunctionTransformer(simplify_cats_v1, validate=False)\n",
    "# V1 Preprocessor Pipeline\n",
    "pipeline_preprocess_v1_for_stack = Pipeline([\n",
    "    ('simplifier', simplifier_v1_tf),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Apply OHE after simplifying\n",
    "])\n",
    "preprocessor_v1_stack = ColumnTransformer([\n",
    "    ('cat', pipeline_preprocess_v1_for_stack, categorical_features_v1_like)], # Apply simplify+OHE\n",
    "    remainder='passthrough') # Pass through all other (already scaled numerical + other string date) features\n",
    "\n",
    "\n",
    "# Preprocessor for GBC V2 Style (OHE All String Cats on V4 structure)\n",
    "numerical_features_v4 = X_stack.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v4 = X_stack.select_dtypes(exclude=np.number).columns.tolist()\n",
    "categorical_pipeline_v4_ohe = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v2_stack = ColumnTransformer([\n",
    "    ('cat', categorical_pipeline_v4_ohe, categorical_features_v4)], # OHE the string columns\n",
    "    remainder='passthrough') # Pass through scaled numericals\n",
    "\n",
    "# Preprocessor for Logistic Regression (Scale Numerics, OHE All Cats)\n",
    "# Same as preprocessor_v2_stack in this case\n",
    "preprocessor_logreg_stack = preprocessor_v2_stack\n",
    "\n",
    "\n",
    "# --- Use Previously Found BEST Hyperparameters ---\n",
    "# Use the params from the RandomizedSearch runs that contributed to the 0.85082 ensemble\n",
    "best_params_gbc_v1_direct = {\n",
    "    'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'sqrt', 'max_depth': 2,\n",
    "    'learning_rate': 0.08, 'random_state':42}\n",
    "\n",
    "best_params_gbc_v2_direct = {\n",
    "    'subsample': 0.7, 'n_estimators': 300, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'log2', 'max_depth': 2,\n",
    "    'learning_rate': 0.05, 'random_state':42}\n",
    "\n",
    "\n",
    "# --- Define Base Estimator Pipelines for Stacking ---\n",
    "# Base Estimator 1: GBC with V1-style preprocessing\n",
    "# IMPORTANT: Ensure this pipeline structure correctly handles the V4 input data\n",
    "# It seems simpler to define the preprocessing outside for stacking base models\n",
    "# Let's preprocess first, then define base models without internal preprocessing steps.\n",
    "\n",
    "# --- Preprocess Data Separately for each base model logic ---\n",
    "print(\"\\nPreprocessing for Stacking Base Models...\")\n",
    "# GBC V1 Style: Simplify, OHE Cats, Scale Nums\n",
    "X_v1_simp = simplify_cats_v1(X_stack) # Simplify first\n",
    "cat_features_v1_simp = ['Education', 'Marital_Status']\n",
    "num_features_v1_simp = [col for col in X_v1_simp.columns if col not in cat_features_v1_simp]\n",
    "preprocessor_gbc_v1_stack = ColumnTransformer([\n",
    "     ('num', StandardScaler(), num_features_v1_simp),\n",
    "     ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features_v1_simp)],\n",
    "     remainder='passthrough').fit(X_v1_simp) # Fit on simplified data\n",
    "X_stack_gbc_v1 = preprocessor_gbc_v1_stack.transform(X_v1_simp)\n",
    "X_test_stack_gbc_v1 = preprocessor_gbc_v1_stack.transform(simplify_cats_v1(X_test_stack)) # Apply simplify before transform\n",
    "\n",
    "# GBC V2 Style: OHE All Cats, Scale Nums\n",
    "cat_features_v4 = X_stack.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_features_v4 = X_stack.select_dtypes(include=np.number).columns.tolist()\n",
    "preprocessor_gbc_v2_stack = ColumnTransformer([\n",
    "     ('num', StandardScaler(), num_features_v4),\n",
    "     ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features_v4)],\n",
    "     remainder='passthrough').fit(X_stack) # Fit on original V4 data\n",
    "X_stack_gbc_v2 = preprocessor_gbc_v2_stack.transform(X_stack)\n",
    "X_test_stack_gbc_v2 = preprocessor_gbc_v2_stack.transform(X_test_stack)\n",
    "\n",
    "# LogReg: Use the same as GBC V2 preprocessing\n",
    "X_stack_logreg = X_stack_gbc_v2\n",
    "X_test_stack_logreg = X_test_stack_gbc_v2\n",
    "\n",
    "print(\"Preprocessing for base models complete.\")\n",
    "\n",
    "# --- Define Base Estimators (using pre-defined best params) ---\n",
    "# Base models now operate on preprocessed NumPy arrays\n",
    "base_model_gbc_v1 = GradientBoostingClassifier(**best_params_gbc_v1_direct)\n",
    "base_model_gbc_v2 = GradientBoostingClassifier(**best_params_gbc_v2_direct)\n",
    "base_model_logreg = LogisticRegression(solver='liblinear', C=1.0, random_state=42) # Default C=1\n",
    "\n",
    "# List of base estimators (name, model)\n",
    "# StackingClassifier will handle fitting these models\n",
    "# It requires estimators that can take the original X, so we need pipelines back\n",
    "# Let's simplify and use the pipelines defined earlier, assuming they work with X_stack input\n",
    "\n",
    "# Redefine Pipelines for StackingClassifier Input\n",
    "pipeline_base_gbc_v1 = Pipeline([\n",
    "    ('preprocess', preprocessor_v1_stack), # Uses V4 data -> Simplifies -> OHE -> Scales Nums\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v1_direct))\n",
    "])\n",
    "pipeline_base_gbc_v2 = Pipeline([\n",
    "    ('preprocess', preprocessor_v2_stack), # Uses V4 data -> OHE -> Scales Nums\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v2_direct))\n",
    "])\n",
    "pipeline_base_logreg = Pipeline([\n",
    "     ('preprocess', preprocessor_logreg_stack), # Same as GBC V2 Preprocessing\n",
    "     ('classifier', LogisticRegression(solver='liblinear', C=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "base_estimators = [\n",
    "    ('gbc_v1_style', pipeline_base_gbc_v1),\n",
    "    ('gbc_v2_style', pipeline_base_gbc_v2),\n",
    "    ('logreg', pipeline_base_logreg)\n",
    "]\n",
    "\n",
    "\n",
    "# --- Define Meta-Model ---\n",
    "meta_model = LogisticRegression(solver='liblinear', random_state=42) # Keep it simple\n",
    "\n",
    "# --- Define Cross-Validation Strategy for Meta-Model ---\n",
    "N_SPLITS_STACK = 5; RANDOM_STATE_STACK = 42\n",
    "cv_stack = StratifiedKFold(n_splits=N_SPLITS_STACK, shuffle=True, random_state=RANDOM_STATE_STACK)\n",
    "\n",
    "# --- Create Stacking Classifier ---\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=cv_stack,\n",
    "    stack_method='predict_proba',\n",
    "    n_jobs=-1, # Use available cores\n",
    "    passthrough=False # Only use base model predictions\n",
    ")\n",
    "\n",
    "# --- Train Stacking Classifier ---\n",
    "print(\"\\nTraining Stacking Classifier (GBC V1, GBC V2, LogReg)...\")\n",
    "# Fit on the V4 data structure, pipelines handle internal preprocessing\n",
    "stacking_clf.fit(X_stack, y_stack)\n",
    "print(\"Stacking Classifier training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Stacking Classifier ---\n",
    "print(\"Predicting on test data using the Stacking Classifier...\")\n",
    "test_predictions_stacking = stacking_clf.predict(X_test_stack) # Predict on V4 test data structure\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for Stacking ---\n",
    "submission_df_stacking = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_stacking})\n",
    "submission_filename_stacking = 'submission_stacking_gbc1_gbc2_logreg.csv' # New filename\n",
    "submission_df_stacking.to_csv(submission_filename_stacking, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_stacking}' created successfully.\")\n",
    "print(submission_df_stacking.head())\n",
    "print(f\"\\nPredicted target distribution (Stacking):\\n{submission_df_stacking['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate Stacking model on training data\n",
    "train_preds_stacking = stacking_clf.predict(X_stack)\n",
    "train_accuracy_stacking = accuracy_score(y_stack, train_preds_stacking)\n",
    "try:\n",
    "    train_roc_auc_stacking = roc_auc_score(y_stack, stacking_clf.predict_proba(X_stack)[:, 1])\n",
    "    print(f\"\\n--- Stacking Model Training Set Evaluation ---\")\n",
    "    print(f\"Accuracy: {train_accuracy_stacking:.4f}\")\n",
    "    print(f\"ROC AUC: {train_roc_auc_stacking:.5f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not calculate ROC AUC for stacking on training set: {e}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy_stacking:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac5c353d-774b-4e54-a42e-9f50aef05353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing V4 (for Pipeline)...\n",
      "V4 Preprocessing complete.\n",
      "\n",
      "Unified Preprocessor - Num Features: 35\n",
      "Unified Preprocessor - Cat Features: 2\n",
      "\n",
      "Training Stacking Classifier (Unified Preprocessing)...\n",
      "Stacking Classifier training complete.\n",
      "Predicting on test data using the Stacking Classifier...\n",
      "Prediction complete.\n",
      "\n",
      "Submission file 'submission_stacking_unified_prep.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (Stacking Unified):\n",
      "Target\n",
      "0    0.624071\n",
      "1    0.375929\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Stacking Model Training Set Evaluation ---\n",
      "Accuracy: 0.8660\n",
      "ROC AUC: 0.95069\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function V4 (Manual Impute/Scale Removed) ---\n",
    "# We will let the scikit-learn pipeline handle imputation and scaling now\n",
    "def preprocess_data_v4_for_pipeline(df, is_train=True, latest_date=None):\n",
    "    \"\"\" V4 features structure, but NO manual impute/scale \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'\n",
    "    # --- Step 1: Initial Feature Creation ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max(); reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan; df_processed['Age'] = reference_year - df_processed['Year_Birth']; df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v4 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month; df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year; df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt']); df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days; df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    # --- Don't impute Mnt cols here ---\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1); df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1); df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    # --- Calculate derived features (can result in NaNs/Infs) ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, np.nan)) # Use NaN for division by zero\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, np.nan))\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, np.nan))\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']; df_processed['Income_per_Person'] = (df_processed['Income'] / num_people.replace(0, 1)) # Use replace for denom 0\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Total_Purchases'].replace(0, 1))\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "    # Replace Inf created during calculations\n",
    "    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # Final type conversion for categoricals (before imputation)\n",
    "    categorical_features_initial = df_processed.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in categorical_features_initial: df_processed[col] = df_processed[col].astype(str)\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing V4 for Pipeline ---\n",
    "print(\"Preprocessing V4 (for Pipeline)...\")\n",
    "train_df_processed_v4p = preprocess_data_v4_for_pipeline(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v4p = preprocess_data_v4_for_pipeline(test_df.copy(), is_train=False, latest_date=global_latest_date_v4)\n",
    "print(\"V4 Preprocessing complete.\")\n",
    "\n",
    "# --- Prepare Data V4 (Common Input for Stacking) ---\n",
    "X_stack = train_df_processed_v4p.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_stack = train_df_processed_v4p['Target']\n",
    "X_test_stack = test_df_processed_v4p.drop('ID', axis=1, errors='ignore')\n",
    "# Align Stacking Data\n",
    "train_cols_stack = X_stack.columns; test_cols_stack = X_test_stack.columns\n",
    "missing_in_test_stack = set(train_cols_stack) - set(test_cols_stack); [X_test_stack.insert(loc=X_stack.columns.get_loc(c), column=c, value=0) for c in missing_in_test_stack]\n",
    "missing_in_train_stack = set(test_cols_stack) - set(train_cols_stack); [X_stack.insert(loc=X_test_stack.columns.get_loc(c), column=c, value=0) for c in missing_in_train_stack]\n",
    "X_test_stack = X_test_stack[train_cols_stack]\n",
    "\n",
    "# --- Define UNIFIED Preprocessing Pipeline ---\n",
    "# Handles all imputation, scaling, OHE\n",
    "numerical_features = X_stack.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_stack.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nUnified Preprocessor - Num Features: {len(numerical_features)}\")\n",
    "print(f\"Unified Preprocessor - Cat Features: {len(categorical_features)}\")\n",
    "\n",
    "# Numerical pipeline: Impute then Scale\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: Impute then OHE\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combined preprocessor\n",
    "unified_preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)],\n",
    "    remainder='passthrough' # Should be nothing left if lists are correct\n",
    ")\n",
    "\n",
    "\n",
    "# --- Use Previously Found BEST Hyperparameters (No classifier__ prefix) ---\n",
    "best_params_gbc_v1_direct = { # Params for GBC V1 (from RandomizedSearch)\n",
    "    'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'sqrt', 'max_depth': 2,\n",
    "    'learning_rate': 0.08, 'random_state':42}\n",
    "\n",
    "best_params_gbc_v2_direct = { # Params for GBC V2 (from RandomizedSearch)\n",
    "    'subsample': 0.7, 'n_estimators': 300, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'log2', 'max_depth': 2,\n",
    "    'learning_rate': 0.05, 'random_state':42}\n",
    "\n",
    "# --- Define Base Estimator Pipelines for Stacking (Using Unified Preprocessor) ---\n",
    "# Base Estimator 1: GBC V1 Params\n",
    "pipeline_base_gbc_v1 = Pipeline([\n",
    "    ('preprocess', unified_preprocessor), # Use the same preprocessor\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v1_direct))\n",
    "])\n",
    "\n",
    "# Base Estimator 2: GBC V2 Params\n",
    "pipeline_base_gbc_v2 = Pipeline([\n",
    "    ('preprocess', unified_preprocessor), # Use the same preprocessor\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v2_direct))\n",
    "])\n",
    "\n",
    "# Base Estimator 3: Logistic Regression\n",
    "pipeline_base_logreg = Pipeline([\n",
    "     ('preprocess', unified_preprocessor), # Use the same preprocessor\n",
    "     ('classifier', LogisticRegression(solver='liblinear', C=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "# List of base estimators\n",
    "base_estimators = [\n",
    "    ('gbc_v1', pipeline_base_gbc_v1),\n",
    "    ('gbc_v2', pipeline_base_gbc_v2),\n",
    "    ('logreg', pipeline_base_logreg)\n",
    "]\n",
    "\n",
    "# --- Define Meta-Model ---\n",
    "meta_model = LogisticRegression(solver='liblinear', C=0.1, random_state=42) # Try slightly stronger regularization for meta\n",
    "\n",
    "# --- Define Cross-Validation Strategy for Meta-Model ---\n",
    "N_SPLITS_STACK = 5; RANDOM_STATE_STACK = 42\n",
    "cv_stack = StratifiedKFold(n_splits=N_SPLITS_STACK, shuffle=True, random_state=RANDOM_STATE_STACK)\n",
    "\n",
    "# --- Create Stacking Classifier ---\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=cv_stack,\n",
    "    stack_method='predict_proba',\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# --- Train Stacking Classifier ---\n",
    "print(\"\\nTraining Stacking Classifier (Unified Preprocessing)...\")\n",
    "stacking_clf.fit(X_stack, y_stack) # Fit on V4 features, pipelines handle internal steps\n",
    "print(\"Stacking Classifier training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Stacking Classifier ---\n",
    "print(\"Predicting on test data using the Stacking Classifier...\")\n",
    "test_predictions_stacking = stacking_clf.predict(X_test_stack) # Predict on V4 test features\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for Stacking ---\n",
    "submission_df_stacking = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_stacking})\n",
    "submission_filename_stacking = 'submission_stacking_unified_prep.csv' # New filename\n",
    "submission_df_stacking.to_csv(submission_filename_stacking, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_stacking}' created successfully.\")\n",
    "print(submission_df_stacking.head())\n",
    "print(f\"\\nPredicted target distribution (Stacking Unified):\\n{submission_df_stacking['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate Stacking model on training data\n",
    "train_preds_stacking = stacking_clf.predict(X_stack)\n",
    "train_accuracy_stacking = accuracy_score(y_stack, train_preds_stacking)\n",
    "try:\n",
    "    # Use decision_function for LogReg meta-model if predict_proba causes issues\n",
    "    # Or directly calculate AUC on OOF predictions if possible\n",
    "    train_roc_auc_stacking = roc_auc_score(y_stack, stacking_clf.predict_proba(X_stack)[:, 1])\n",
    "    print(f\"\\n--- Stacking Model Training Set Evaluation ---\")\n",
    "    print(f\"Accuracy: {train_accuracy_stacking:.4f}\")\n",
    "    print(f\"ROC AUC: {train_roc_auc_stacking:.5f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not calculate ROC AUC for stacking on training set: {e}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy_stacking:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "793c840d-388a-4e3c-bafc-a481c3a98ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing V4 (Base for Stacking)...\n",
      "V4 Preprocessing complete.\n",
      "\n",
      "Training Stacking Classifier (GBC V1, GBC V2, LGBM)...\n",
      "Stacking Classifier training complete.\n",
      "Predicting on test data using the Stacking Classifier...\n",
      "Prediction complete.\n",
      "\n",
      "Submission file 'submission_stacking_gbc1_gbc2_lgbm.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       1\n",
      "\n",
      "Predicted target distribution (Stacking + LGBM):\n",
      "Target\n",
      "1    0.995542\n",
      "0    0.004458\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Stacking Model Training Set Evaluation ---\n",
      "Accuracy: 0.8781\n",
      "ROC AUC: 0.95802\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV # Needed to tune LGBM base\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "import warnings\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier # Import LightGBM\n",
    "except ImportError:\n",
    "    print(\"LightGBM not found. Please install it using: pip install lightgbm\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions (V1 and V4) ---\n",
    "# ... (Keep V1 and V4 function definitions exactly as in the previous working block) ...\n",
    "# Function V1 (Correct definition for GBC V1 - returns only DataFrame)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    df_processed = df.copy(); target_col = 'Target'; current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max(); reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan; df_processed['Age'] = reference_year - df_processed['Year_Birth']; df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v1 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt']); df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days; df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0)\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1); df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1); df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    cols_to_impute_num = ['Age', 'Income', 'Customer_Lifetime', 'Total_Mnt']\n",
    "    for col in cols_to_impute_num:\n",
    "         if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    cat_cols_simple = ['Marital_Status', 'Education']\n",
    "    for col in cat_cols_simple:\n",
    "        if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# Function V4 (Manual Impute/Scale) - Used as basis for GBC V2 features\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    df_processed = df.copy(); target_col = 'Target'\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max(); reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan; df_processed['Age'] = reference_year - df_processed['Year_Birth']; df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v4 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month; df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year; df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt']); df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days; df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1); df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1); df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']; df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1); df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1); df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist(); initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median'); imputer_num.fit(df_processed[initial_numerical_features]); df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent'); imputer_cat.fit(df_processed[initial_categorical_features]); df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed\"); df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features]); df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    original_cols = df_processed.columns; imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)); df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)); df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1))\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']); df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom'])\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    derived_cols = ['Wine_Ratio', 'Meat_Ratio', 'Fruit_Ratio', 'Income_per_Person', 'Spending_per_Purchase'];\n",
    "    for col in derived_cols:\n",
    "        if col in df_processed.columns: df_processed[col] = df_processed[col].replace([np.inf, -np.inf], np.nan); col_median = df_processed[col].median(); df_processed[col] = df_processed[col].fillna(col_median if pd.notna(col_median) else 0)\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]; final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler(); scaler.fit(df_processed[final_numerical_features]); df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features]); fit_scaler = scaler\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler needed\"); df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "# --- Apply Preprocessing V4 (Base for Stacking) ---\n",
    "print(\"Preprocessing V4 (Base for Stacking)...\")\n",
    "train_df_processed_v4, fitted_imputers_v4, fitted_scaler_v4 = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers_v4, fit_scaler=fitted_scaler_v4)\n",
    "print(\"V4 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V4 (Common Input for Stacking) ---\n",
    "X_stack = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_stack = train_df_processed_v4['Target']\n",
    "X_test_stack = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "# Align Stacking Data\n",
    "train_cols_stack = X_stack.columns; test_cols_stack = X_test_stack.columns\n",
    "missing_in_test_stack = set(train_cols_stack) - set(test_cols_stack); [X_test_stack.insert(loc=X_stack.columns.get_loc(c), column=c, value=0) for c in missing_in_test_stack]\n",
    "missing_in_train_stack = set(test_cols_stack) - set(train_cols_stack); [X_stack.insert(loc=X_test_stack.columns.get_loc(c), column=c, value=0) for c in missing_in_train_stack]\n",
    "X_test_stack = X_test_stack[train_cols_stack]\n",
    "\n",
    "\n",
    "# --- Define UNIFIED Preprocessing Pipeline ---\n",
    "numerical_features = X_stack.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_stack.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# Numerical pipeline: Impute + Scale (already done in V4 func, so just pass through?) - Let's keep it for safety\n",
    "numerical_pipeline = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "# Categorical pipeline: Impute + OHE\n",
    "categorical_pipeline = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "# Combined preprocessor\n",
    "unified_preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Use Previously Found BEST Hyperparameters for GBCs ---\n",
    "best_params_gbc_v1_direct = { # From RandomizedSearch run on V1 features\n",
    "    'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'sqrt', 'max_depth': 2,\n",
    "    'learning_rate': 0.08, 'random_state':42}\n",
    "\n",
    "best_params_gbc_v2_direct = { # From RandomizedSearch run on V4 features+OHE\n",
    "    'subsample': 0.7, 'n_estimators': 300, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'log2', 'max_depth': 2,\n",
    "    'learning_rate': 0.05, 'random_state':42}\n",
    "\n",
    "# --- Define Hyperparameters for Base LGBM (Using previous best from its RandomizedSearch) ---\n",
    "best_params_lgbm_direct = {\n",
    "    'subsample': 0.9, 'reg_lambda': 0.0, # reg_lambda was 0 in best run\n",
    "    'reg_alpha': 0.1, 'num_leaves': 31, 'n_estimators': 500,\n",
    "    'min_child_samples': 30, 'max_depth': 3, 'learning_rate': 0.01,\n",
    "    'colsample_bytree': 0.7, 'random_state': 42, 'objective':'binary'\n",
    "}\n",
    "\n",
    "\n",
    "# --- Define Base Estimator Pipelines for Stacking (Using Unified Preprocessor) ---\n",
    "pipeline_base_gbc_v1 = Pipeline([\n",
    "    ('preprocess', unified_preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v1_direct))\n",
    "])\n",
    "pipeline_base_gbc_v2 = Pipeline([\n",
    "    ('preprocess', unified_preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v2_direct))\n",
    "])\n",
    "pipeline_base_lgbm = Pipeline([\n",
    "     ('preprocess', unified_preprocessor),\n",
    "     ('classifier', LGBMClassifier(**best_params_lgbm_direct)) # Add LGBM\n",
    "])\n",
    "\n",
    "# List of base estimators\n",
    "base_estimators = [\n",
    "    ('gbc_v1', pipeline_base_gbc_v1),\n",
    "    ('gbc_v2', pipeline_base_gbc_v2),\n",
    "    ('lgbm', pipeline_base_lgbm) # Include LGBM\n",
    "]\n",
    "\n",
    "\n",
    "# --- Define Meta-Model ---\n",
    "# Still use Logistic Regression, can be surprisingly effective\n",
    "meta_model = LogisticRegression(solver='liblinear', C=0.1, random_state=42)\n",
    "\n",
    "# --- Define Cross-Validation Strategy for Meta-Model ---\n",
    "N_SPLITS_STACK = 5; RANDOM_STATE_STACK = 42\n",
    "cv_stack = StratifiedKFold(n_splits=N_SPLITS_STACK, shuffle=True, random_state=RANDOM_STATE_STACK)\n",
    "\n",
    "# --- Create Stacking Classifier ---\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=cv_stack,\n",
    "    stack_method='predict_proba',\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# --- Train Stacking Classifier ---\n",
    "print(\"\\nTraining Stacking Classifier (GBC V1, GBC V2, LGBM)...\")\n",
    "stacking_clf.fit(X_stack, y_stack) # Fit on V4 features\n",
    "print(\"Stacking Classifier training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Stacking Classifier ---\n",
    "print(\"Predicting on test data using the Stacking Classifier...\")\n",
    "test_predictions_stacking = stacking_clf.predict(X_test_stack) # Predict on V4 test features\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for Stacking ---\n",
    "submission_df_stacking = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_stacking})\n",
    "submission_filename_stacking = 'submission_stacking_gbc1_gbc2_lgbm.csv' # New filename\n",
    "submission_df_stacking.to_csv(submission_filename_stacking, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_stacking}' created successfully.\")\n",
    "print(submission_df_stacking.head())\n",
    "print(f\"\\nPredicted target distribution (Stacking + LGBM):\\n{submission_df_stacking['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate Stacking model on training data\n",
    "train_preds_stacking = stacking_clf.predict(X_stack)\n",
    "train_accuracy_stacking = accuracy_score(y_stack, train_preds_stacking)\n",
    "try:\n",
    "    train_roc_auc_stacking = roc_auc_score(y_stack, stacking_clf.predict_proba(X_stack)[:, 1])\n",
    "    print(f\"\\n--- Stacking Model Training Set Evaluation ---\")\n",
    "    print(f\"Accuracy: {train_accuracy_stacking:.4f}\")\n",
    "    print(f\"ROC AUC: {train_roc_auc_stacking:.5f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not calculate ROC AUC for stacking on training set: {e}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy_stacking:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8029d-128f-4128-b236-7a942bbad17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
