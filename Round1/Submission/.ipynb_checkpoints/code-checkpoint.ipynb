{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96208800-22cf-4090-9174-9071d646af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed (e.g., from one-hot encoder)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Often related to sparse output default\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# --- Feature Engineering & Preprocessing ---\n",
    "\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years\n",
    "    current_year = datetime.datetime.now().year\n",
    "    # Use a reasonable reference year based on Dt_Customer if available, otherwise current year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except: # Handle cases where Dt_Customer might not exist or be parseable easily\n",
    "        reference_year = current_year\n",
    "\n",
    "    # Replace very old birth years (e.g., < 1910) with NaN to be imputed later\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "\n",
    "    # Calculate Age (handle potential NaNs in Year_Birth temporarily)\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 2. Process Dt_Customer\n",
    "    # Convert 'Dt_Customer' to datetime, coercing errors\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "\n",
    "    # Find the latest date for calculating tenure if not provided (from training set)\n",
    "    if is_train:\n",
    "        global global_latest_date # Store latest date from training set globally\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1)\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        # Fallback if called on test set first or global_latest_date isn't set\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1) # Use start of next year as reference\n",
    "        print(f\"Warning: Using fallback latest date: {latest_date_to_use}\")\n",
    "\n",
    "\n",
    "    # Calculate Customer_Lifetime (Tenure in days)\n",
    "    # Handle potential NaT dates resulting from coerce errors\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    df_processed['Customer_Lifetime'].fillna(df_processed['Customer_Lifetime'].median(), inplace=True) # Impute NaNs created by NaT\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    # Consolidate categories\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner',\n",
    "        'Together': 'Partner',\n",
    "        'Absurd': 'Single',\n",
    "        'Alone': 'Single',\n",
    "        'YOLO': 'Single',\n",
    "        'Widow': 'Single',\n",
    "        'Divorced':'Single'\n",
    "         }) # Grouping Married/Together and others into Single for simplicity\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'}) # Group '2n Cycle' with 'Master'\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # Optionally drop original columns if 'Children' is deemed sufficient\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns (identified during EDA)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore') # errors='ignore' in case they were already dropped\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# Preprocess training data\n",
    "train_df_processed = preprocess_data(train_df, is_train=True)\n",
    "# Preprocess test data using the latest date from training data\n",
    "test_df_processed = preprocess_data(test_df, is_train=False, latest_date=global_latest_date)\n",
    "\n",
    "print(\"Feature engineering complete.\")\n",
    "print(\"\\nTrain Data Info after processing:\")\n",
    "train_df_processed.info()\n",
    "print(\"\\nTest Data Info after processing:\")\n",
    "test_df_processed.info()\n",
    "\n",
    "\n",
    "# --- Model Training ---\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns - crucial after feature engineering if columns were added/dropped differently (shouldn't happen here but good practice)\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0 # Add missing columns to test set with default value (0)\n",
    "\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0 # Add missing columns to train set with default value (0) - less likely\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types for preprocessing\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')), # Impute missing numericals (Age, Income, Customer_Lifetime)\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')), # Impute missing categoricals (if any)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Use sparse_output=False for easier debugging if needed\n",
    "])\n",
    "\n",
    "# Create a column transformer to apply different pipelines to different columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough') # Keep any columns not specified (though there shouldn't be any here)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "# GradientBoostingClassifier often works well. random_state for reproducibility.\n",
    "# Consider tuning hyperparameters later using GridSearchCV or RandomizedSearchCV\n",
    "model = GradientBoostingClassifier(n_estimators=150, # Increased slightly\n",
    "                                 learning_rate=0.08, # Slightly decreased\n",
    "                                 max_depth=4,       # Increased slightly\n",
    "                                 subsample=0.8,     # Added subsampling\n",
    "                                 random_state=42)\n",
    "\n",
    "# Create the full pipeline: preprocess + model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training dataset\n",
    "print(\"\\nTraining the model...\")\n",
    "pipeline.fit(X, y)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Prediction ---\n",
    "\n",
    "# Predict on the preprocessed test data\n",
    "print(\"Predicting on test data...\")\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Submission File Generation ---\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = 'submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename}' created successfully.\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nPredicted target distribution:\\n{submission_df['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate on the training set (for sanity check, not a true performance measure)\n",
    "train_preds = pipeline.predict(X)\n",
    "train_accuracy = accuracy_score(y, train_preds)\n",
    "train_roc_auc = roc_auc_score(y, pipeline.predict_proba(X)[:, 1]) # Use probabilities for AUC\n",
    "print(f\"\\n--- Training Set Evaluation (Sanity Check) ---\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc:.4f}\")\n",
    "# print(\"Classification Report:\\n\", classification_report(y, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b1ffc-1bb9-4f55-aa9b-586aa725681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed (e.g., from one-hot encoder)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Often related to sparse output default\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    # test_df = pd.read_csv(\"test.csv\") # Not needed for CV evaluation\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Training data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (same as before) ---\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years & Calculate Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except:\n",
    "        reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Process Dt_Customer & Calculate Customer_Lifetime\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1)\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    # Impute median *before* dropping Dt_Customer to handle NaTs properly\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median() # Calculate median only once\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True) # Keep originals for now\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing to Training Data ---\n",
    "train_df_processed = preprocess_data(train_df.copy(), is_train=True) # Use copy to be safe\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- Prepare Data for CV ---\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "\n",
    "# Identify column types (ensure this happens *after* preprocessing)\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Check if Customer_Lifetime needs explicit imputation placeholder if it wasn't numeric initially\n",
    "if 'Customer_Lifetime' in numerical_features:\n",
    "     print(\"Customer_Lifetime treated as numerical.\")\n",
    "else:\n",
    "     print(\"Warning: Customer_Lifetime might not be numerical after preprocessing.\")\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Steps ---\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# --- Define Model (Using the same parameters as your previous run) ---\n",
    "# You can adjust these parameters later based on CV results\n",
    "model = GradientBoostingClassifier(n_estimators=150,\n",
    "                                 learning_rate=0.08,\n",
    "                                 max_depth=4,\n",
    "                                 subsample=0.8,\n",
    "                                 random_state=42)\n",
    "\n",
    "# --- Create Full Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# --- Set up K-Fold Cross-Validation ---\n",
    "N_SPLITS = 5 # Number of folds (5 or 10 are common)\n",
    "RANDOM_STATE_KFOLD = 42 # For reproducible splits\n",
    "\n",
    "# Use StratifiedKFold to maintain target class distribution in each fold\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "print(f\"\\nStarting {N_SPLITS}-Fold Cross-Validation...\")\n",
    "\n",
    "# --- Perform Cross-Validation and Calculate Scores ---\n",
    "\n",
    "# Accuracy Scores\n",
    "accuracy_scores = cross_val_score(pipeline, X, y, cv=skf, scoring='accuracy', n_jobs=-1) # n_jobs=-1 uses all processors\n",
    "\n",
    "# ROC AUC Scores\n",
    "# Note: cross_val_score calculates ROC AUC based on predict_proba internally\n",
    "roc_auc_scores = cross_val_score(pipeline, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(\"Cross-Validation finished.\")\n",
    "\n",
    "# --- Report Results ---\n",
    "print(\"\\n--- Cross-Validation Results ---\")\n",
    "print(f\"Accuracy Scores per Fold: {accuracy_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Std Dev Accuracy: {np.std(accuracy_scores):.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"ROC AUC Scores per Fold: {roc_auc_scores}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Std Dev ROC AUC: {np.std(roc_auc_scores):.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Note on Final Training ---\n",
    "print(\"\\nNOTE: The scores above are estimates of generalization performance.\")\n",
    "print(\"For the final submission, you should train the pipeline on the *entire* training set (X, y)\")\n",
    "print(\"and then predict on the preprocessed test set.\")\n",
    "print(\"Example final training step (run this *after* CV and hyperparameter tuning):\")\n",
    "print(\"# pipeline.fit(X, y)\")\n",
    "print(\"# test_predictions = pipeline.predict(X_test) # Assuming X_test is preprocessed test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825432d-4310-4f85-afed-46fddb49cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\") # Needed for final submission ID mapping\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (same as before) ---\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years & Calculate Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except:\n",
    "        reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Process Dt_Customer & Calculate Customer_Lifetime\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1) # Define global latest date from train set\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else: # Fallback if test is processed first somehow (shouldn't happen here)\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        print(f\"Warning: Using fallback latest date: {latest_date_to_use}\")\n",
    "\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True) # Keep originals for now\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing ---\n",
    "train_df_processed = preprocess_data(train_df.copy(), is_train=True)\n",
    "# Preprocess test data using the date derived from training data\n",
    "test_df_processed = preprocess_data(test_df.copy(), is_train=False, latest_date=global_latest_date)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data ---\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns after preprocessing (important!)\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# --- Define Preprocessing Steps ---\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Set sparse_output=True for large datasets if memory is an issue\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model ---\n",
    "# We will tune the parameters of this model\n",
    "base_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# --- Create Full Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', base_model) # Placeholder name 'classifier'\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for RandomizedSearchCV ---\n",
    "# Adjust ranges based on previous results and desired exploration\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [50, 80, 100, 150, 200], # Range around potentially good values\n",
    "    'classifier__learning_rate': [0.01, 0.02, 0.05, 0.08, 0.1, 0.15], # Wider range, including lower values\n",
    "    'classifier__max_depth': [2, 3, 4], # Focus on shallower trees to reduce overfitting\n",
    "    'classifier__min_samples_leaf': [5, 10, 15, 20], # Force more samples per leaf\n",
    "    'classifier__min_samples_split': [10, 20, 30], # Force more samples for a split\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9], # Explore subsampling ratios\n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.7, 0.8, None] # Limit features per split\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy (same as before) ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV ---\n",
    "N_ITER = 50 # Number of parameter settings to sample. Increase for more thorough search (e.g., 100), decrease for speed.\n",
    "SCORING_METRIC = 'roc_auc' # Optimize for ROC AUC, common for binary classification\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=N_ITER,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    random_state=42, # For reproducible search results\n",
    "    verbose=1 # Set to 1 or 2 to see progress\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV with {N_ITER} iterations for {SCORING_METRIC}...\")\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best Results ---\n",
    "print(\"\\n--- Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "# Nicely print the best parameters found\n",
    "best_params = random_search.best_params_\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final Model with Best Parameters ---\n",
    "print(\"\\nTraining final model on the entire training set using best parameters...\")\n",
    "# The best estimator found by RandomizedSearchCV is already fitted on the full data\n",
    "# if refit=True (default), but we fit it explicitly for clarity.\n",
    "# Alternatively, you could just use: best_pipeline = random_search.best_estimator_\n",
    "best_pipeline = pipeline # Start with the original pipeline structure\n",
    "best_pipeline.set_params(**best_params) # Set the best parameters found\n",
    "best_pipeline.fit(X, y)\n",
    "print(\"Final model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data ---\n",
    "print(\"Predicting on test data using the tuned model...\")\n",
    "test_predictions = best_pipeline.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions})\n",
    "submission_filename = 'submission_tuned_gbc.csv' # New filename\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename}' created successfully.\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nPredicted target distribution:\\n{submission_df['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* model on the training set\n",
    "train_preds_tuned = best_pipeline.predict(X)\n",
    "train_accuracy_tuned = accuracy_score(y, train_preds_tuned)\n",
    "train_roc_auc_tuned = roc_auc_score(y, best_pipeline.predict_proba(X)[:, 1])\n",
    "print(f\"\\n--- Tuned Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_tuned:.4f}\")\n",
    "print(\"(Compare these to the initial overfit scores and the CV scores)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b186ba-3cd1-48c8-a606-38cb1591633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2610a1-4eb5-4c1c-a450-e67464349b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer # Ensure make_scorer is imported if needed, though cross_val_score handles it\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "import xgboost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    print(\"XGBoost not found. Please install it using: pip install xgboost\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (Identical to previous step) ---\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years & Calculate Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        # Attempt to get reference year from Dt_Customer\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True) # Drop temporary column\n",
    "    except Exception as e: # Broad exception for safety if Dt_Customer is missing or unparseable\n",
    "        print(f\"Warning: Could not parse Dt_Customer for reference year. Using current year. Error: {e}\")\n",
    "        reference_year = current_year\n",
    "\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "    # 2. Process Dt_Customer & Calculate Customer_Lifetime\n",
    "    # Convert 'Dt_Customer' to datetime, coercing errors\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "\n",
    "    # Find the latest date for calculating tenure if not provided (from training set)\n",
    "    if is_train:\n",
    "        global global_latest_date # Store latest date from training set globally\n",
    "        # Handle case where all Dt_Customer might be NaT after coercion\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            global_latest_date = valid_dates.max() + datetime.timedelta(days=1)\n",
    "        else:\n",
    "            # Fallback if no valid dates found in training set\n",
    "            global_latest_date = datetime.datetime(reference_year + 1, 1, 1)\n",
    "            print(f\"Warning: No valid Dt_Customer found in training set. Using fallback latest date: {global_latest_date}\")\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        # Fallback if called on test set first or global_latest_date isn't set\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        print(f\"Warning: Using fallback latest date for test set: {latest_date_to_use}\")\n",
    "\n",
    "    # Calculate Customer_Lifetime (Tenure in days)\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    # Impute median *before* dropping Dt_Customer to handle NaTs properly\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # Keep original Kidhome/Teenhome for now, might be useful features\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing ---\n",
    "train_df_processed = preprocess_data(train_df.copy(), is_train=True)\n",
    "# Check if global_latest_date was set correctly\n",
    "if 'global_latest_date' not in globals():\n",
    "     print(\"Error: global_latest_date not set during training preprocessing. Exiting.\")\n",
    "     # Handle this case appropriately, maybe define a default or raise error\n",
    "     # For now, let's set a default, but ideally the training data processing should succeed\n",
    "     global_latest_date = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "     print(f\"Using current date as fallback for global_latest_date: {global_latest_date}\")\n",
    "\n",
    "test_df_processed = preprocess_data(test_df.copy(), is_train=False, latest_date=global_latest_date)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data ---\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns after preprocessing\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# --- Define Preprocessing Steps (Same as before) ---\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Set sparse=True for large data if needed\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model: XGBoost ---\n",
    "# Use_label_encoder=False is recommended for newer XGBoost versions\n",
    "# eval_metric='logloss' or 'auc' are common for binary classification\n",
    "base_model_xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# --- Create Full Pipeline with XGBoost ---\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', base_model_xgb) # Step name remains 'classifier'\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for XGBoost RandomizedSearchCV ---\n",
    "# These ranges are starting points; adjust based on results or computational budget\n",
    "param_dist_xgb = {\n",
    "    'classifier__n_estimators': [100, 150, 200, 300, 400], # Number of boosting rounds\n",
    "    'classifier__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15], # Step size shrinkage\n",
    "    'classifier__max_depth': [2, 3, 4, 5], # Maximum depth of a tree\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Fraction of samples used per tree\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Fraction of features used per tree\n",
    "    'classifier__gamma': [0, 0.1, 0.2, 0.5], # Minimum loss reduction required to make a further partition\n",
    "    'classifier__reg_alpha': [0, 0.001, 0.01, 0.1], # L1 regularization term\n",
    "    'classifier__reg_lambda': [0.5, 1, 1.5] # L2 regularization term (default is 1)\n",
    "    # Add 'min_child_weight': [1, 3, 5] if needed (minimum sum of instance weight needed in a child)\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy (same as before) ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for XGBoost ---\n",
    "N_ITER_XGB = 75 # Increase iterations for potentially better results (vs 50 for GBC)\n",
    "SCORING_METRIC = 'roc_auc' # Optimize for ROC AUC\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=pipeline_xgb,\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=N_ITER_XGB,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    random_state=42, # For reproducible search results\n",
    "    verbose=1 # Set to 1 or 2 to see progress\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for XGBoost with {N_ITER_XGB} iterations for {SCORING_METRIC}...\")\n",
    "random_search_xgb.fit(X, y)\n",
    "print(\"XGBoost RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best XGBoost Results ---\n",
    "print(\"\\n--- XGBoost Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_xgb.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_xgb = random_search_xgb.best_params_\n",
    "for param, value in best_params_xgb.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final XGBoost Model with Best Parameters ---\n",
    "print(\"\\nTraining final XGBoost model on the entire training set using best parameters...\")\n",
    "# The best estimator is automatically refit on the whole training data by RandomizedSearchCV\n",
    "best_pipeline_xgb = random_search_xgb.best_estimator_\n",
    "# Explicit refit just to be sure (Optional, default behavior of RS CV is refit=True)\n",
    "# best_pipeline_xgb.fit(X, y)\n",
    "print(\"Final XGBoost model training complete.\")\n",
    "\n",
    "\n",
    "# --- Predict on Test Data with Tuned XGBoost ---\n",
    "print(\"Predicting on test data using the tuned XGBoost model...\")\n",
    "test_predictions_xgb = best_pipeline_xgb.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for XGBoost ---\n",
    "submission_df_xgb = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_xgb})\n",
    "submission_filename_xgb = 'submission_tuned_xgb.csv' # New filename\n",
    "submission_df_xgb.to_csv(submission_filename_xgb, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_xgb}' created successfully.\")\n",
    "print(submission_df_xgb.head())\n",
    "print(f\"\\nPredicted target distribution (XGBoost):\\n{submission_df_xgb['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* XGBoost model on the training set\n",
    "train_preds_xgb_tuned = best_pipeline_xgb.predict(X)\n",
    "train_accuracy_xgb_tuned = accuracy_score(y, train_preds_xgb_tuned)\n",
    "train_roc_auc_xgb_tuned = roc_auc_score(y, best_pipeline_xgb.predict_proba(X)[:, 1])\n",
    "print(f\"\\n--- Tuned XGBoost Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_xgb_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_xgb_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64d34d-c10d-47d5-b3dc-319304ec13e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Keep GBC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer # Added FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function (MODIFIED) ---\n",
    "def preprocess_data_v2(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering (v2) and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # --- Original Preprocessing ---\n",
    "    # 1. Handle Birth Year & Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not parse Dt_Customer for reference year. Using current year. Error: {e}\")\n",
    "        reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True) # Keep Age\n",
    "\n",
    "    # 2. Process Dt_Customer & Lifetime + Extract Date Features\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v2 # Use a new global var name if running in same session\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        if not valid_dates.empty:\n",
    "            global_latest_date_v2 = valid_dates.max() + datetime.timedelta(days=1)\n",
    "        else:\n",
    "            global_latest_date_v2 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "            print(f\"Warning: No valid Dt_Customer. Using fallback latest date: {global_latest_date_v2}\")\n",
    "        latest_date_to_use = global_latest_date_v2\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "\n",
    "    # --> NEW: Extract Date Features BEFORE calculating lifetime and dropping\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer'].dt.dayofweek\n",
    "    # Impute NaNs in date features (e.g., with mode or median year/month)\n",
    "    df_processed['Enroll_Month'].fillna(df_processed['Enroll_Month'].mode()[0], inplace=True)\n",
    "    df_processed['Enroll_Year'].fillna(df_processed['Enroll_Year'].median(), inplace=True)\n",
    "    df_processed['Enroll_DayOfWeek'].fillna(df_processed['Enroll_DayOfWeek'].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True) # Now drop Dt_Customer\n",
    "\n",
    "    # 3. Simplify Marital Status (Keeping original for now - let's test)\n",
    "    # df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({ ... }) # Keep original\n",
    "\n",
    "    # 4. Simplify Education (Keeping original for now - let's test)\n",
    "    # df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'}) # Keep original\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    # Impute NaNs in spending columns *before* summing (using 0 or median)\n",
    "    for col in mnt_cols:\n",
    "        df_processed[col] = df_processed[col].fillna(0) # Simple imputation with 0 for spending\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # --- NEW Features ---\n",
    "    # Ratio Features (handle division by zero)\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    # Replace inf values that might result from 0/0\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "\n",
    "    # Income related (handle division by zero and potential NaNs in Income)\n",
    "    # Impute Income NaNs *before* using it in calculations\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    # Add 1 to avoid division by zero if Children=0 and partner=1 (or single=1)\n",
    "    num_people = df_processed['Children'] + df_processed['Marital_Status'].apply(lambda x: 1 if x=='Single' else 2) # Simple adult estimate\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / num_people.replace(0, 1)).fillna(0) # Replace 0 people with 1\n",
    "\n",
    "\n",
    "    # Spending per Purchase (handle division by zero)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Total_Purchases'].replace(0, 1)).fillna(0)\n",
    "\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply NEW Preprocessing ---\n",
    "train_df_processed_v2 = preprocess_data_v2(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v2' not in globals(): # Check the new global var\n",
    "     global_latest_date_v2 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "     print(f\"Error: global_latest_date_v2 not set. Using fallback: {global_latest_date_v2}\")\n",
    "test_df_processed_v2 = preprocess_data_v2(test_df.copy(), is_train=False, latest_date=global_latest_date_v2)\n",
    "print(\"V2 Preprocessing complete.\")\n",
    "\n",
    "# --- Prepare Data (using v2 processed data) ---\n",
    "X_v2 = train_df_processed_v2.drop(['ID', 'Target'], axis=1)\n",
    "y_v2 = train_df_processed_v2['Target'] # Target remains the same\n",
    "X_test_v2 = test_df_processed_v2.drop('ID', axis=1)\n",
    "\n",
    "# Align columns after V2 preprocessing\n",
    "train_cols_v2 = X_v2.columns\n",
    "test_cols_v2 = X_test_v2.columns\n",
    "\n",
    "missing_in_test_v2 = set(train_cols_v2) - set(test_cols_v2)\n",
    "for c in missing_in_test_v2:\n",
    "    X_test_v2[c] = 0\n",
    "missing_in_train_v2 = set(test_cols_v2) - set(train_cols_v2)\n",
    "for c in missing_in_train_v2:\n",
    "    X_v2[c] = 0\n",
    "\n",
    "X_test_v2 = X_test_v2[train_cols_v2] # Ensure order is the same\n",
    "\n",
    "# --- Define Preprocessing Steps (Potentially updated if features changed type) ---\n",
    "numerical_features_v2 = X_v2.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2 = X_v2.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nV2 Numerical features ({len(numerical_features_v2)}): {numerical_features_v2}\")\n",
    "print(f\"V2 Categorical features ({len(categorical_features_v2)}): {categorical_features_v2}\")\n",
    "\n",
    "\n",
    "# Log transformer function\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=False) # validate=False to handle 0s after log1p\n",
    "\n",
    "# Update Numerical Pipeline to include Log Transform for specific skewed features\n",
    "# Identify potentially skewed features (Income, Spending)\n",
    "skewed_num_features = ['Income', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
    "                       'MntSweetProducts', 'MntGoldProds', 'Total_Mnt', 'Income_per_Person', 'Spending_per_Purchase']\n",
    "# Make sure these features actually exist after preprocessing\n",
    "skewed_num_features = [f for f in skewed_num_features if f in numerical_features_v2]\n",
    "other_num_features = [f for f in numerical_features_v2 if f not in skewed_num_features]\n",
    "\n",
    "\n",
    "numerical_pipeline_v2 = Pipeline([\n",
    "    # Impute FIRST\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    # Apply log transform only to skewed columns (using ColumnTransformer within Pipeline - tricky!)\n",
    "    # Easier approach: Apply log transform in preprocess_data_v2 or handle separately if needed.\n",
    "    # For simplicity here, let's apply StandardScaler to all imputed numericals.\n",
    "    # Consider log transform within preprocess_data_v2 if it proves beneficial.\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline_v2 = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Update Preprocessor\n",
    "preprocessor_v2 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2, numerical_features_v2), # Apply updated pipeline to all numerical\n",
    "    ('cat', categorical_pipeline_v2, categorical_features_v2)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model: Gradient Boosting (Retuning this one) ---\n",
    "base_model_gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# --- Create Full Pipeline with GBC V2 ---\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('classifier', base_model_gbc)\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for GBC RandomizedSearchCV (Centered around previous best) ---\n",
    "param_dist_gbc_v2 = {\n",
    "    'classifier__n_estimators': [150, 200, 250, 300], # Explore higher values slightly\n",
    "    'classifier__learning_rate': [0.02, 0.05, 0.08, 0.1], # Narrower range around 0.08\n",
    "    'classifier__max_depth': [2, 3], # Keep focusing on shallow trees\n",
    "    'classifier__min_samples_leaf': [15, 20, 25], # Stay around the previous best\n",
    "    'classifier__min_samples_split': [15, 20, 30], # Stay around the previous best\n",
    "    'classifier__subsample': [0.5, 0.6, 0.7], # Explore around 0.6\n",
    "    'classifier__max_features': ['sqrt', 'log2'] # Keep simpler options\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy (same as before) ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for GBC V2 ---\n",
    "N_ITER_GBC_V2 = 50 # Number of iterations for retuning\n",
    "SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "random_search_gbc_v2 = RandomizedSearchCV(\n",
    "    estimator=pipeline_gbc_v2,\n",
    "    param_distributions=param_dist_gbc_v2,\n",
    "    n_iter=N_ITER_GBC_V2,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for GBC (V2 Features) with {N_ITER_GBC_V2} iterations for {SCORING_METRIC}...\")\n",
    "random_search_gbc_v2.fit(X_v2, y_v2) # Use V2 features and original target\n",
    "print(\"GBC V2 RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best GBC V2 Results ---\n",
    "print(\"\\n--- GBC V2 Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_gbc_v2.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_gbc_v2 = random_search_gbc_v2.best_params_\n",
    "for param, value in best_params_gbc_v2.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final GBC V2 Model with Best Parameters ---\n",
    "print(\"\\nTraining final GBC V2 model on the entire training set using best parameters...\")\n",
    "best_pipeline_gbc_v2 = random_search_gbc_v2.best_estimator_\n",
    "print(\"Final GBC V2 model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Tuned GBC V2 ---\n",
    "print(\"Predicting on test data using the tuned GBC V2 model...\")\n",
    "test_predictions_gbc_v2 = best_pipeline_gbc_v2.predict(X_test_v2) # Use V2 test features\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for GBC V2 ---\n",
    "submission_df_gbc_v2 = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_gbc_v2})\n",
    "submission_filename_gbc_v2 = 'submission_tuned_gbc_v2_features.csv' # New filename\n",
    "submission_df_gbc_v2.to_csv(submission_filename_gbc_v2, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_gbc_v2}' created successfully.\")\n",
    "print(submission_df_gbc_v2.head())\n",
    "print(f\"\\nPredicted target distribution (GBC V2 Features):\\n{submission_df_gbc_v2['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* GBC V2 model on the training set\n",
    "train_preds_gbc_v2_tuned = best_pipeline_gbc_v2.predict(X_v2)\n",
    "train_accuracy_gbc_v2_tuned = accuracy_score(y_v2, train_preds_gbc_v2_tuned)\n",
    "train_roc_auc_gbc_v2_tuned = roc_auc_score(y_v2, best_pipeline_gbc_v2.predict_proba(X_v2)[:, 1])\n",
    "print(f\"\\n--- Tuned GBC V2 Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_gbc_v2_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v2_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64d632-491e-4577-9784-13c0eb33cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming best_pipeline_gbc_v2 is your fitted V2 GBC pipeline\n",
    "# Get feature names after one-hot encoding\n",
    "ohe_feature_names = best_pipeline_gbc_v2.named_steps['preprocessor'] \\\n",
    "                    .named_transformers_['cat'] \\\n",
    "                    .named_steps['onehot'] \\\n",
    "                    .get_feature_names_out(categorical_features_v2)\n",
    "all_feature_names = numerical_features_v2 + list(ohe_feature_names)\n",
    "\n",
    "# Get importances\n",
    "importances = best_pipeline_gbc_v2.named_steps['classifier'].feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'feature': all_feature_names, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "print(\"\\nTop 20 Feature Importances (GBC V2):\")\n",
    "print(feature_importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a048a4e-5af5-4994-8a12-e92a278e5d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "V1 Preprocessing complete.\n",
      "V2 Preprocessing complete.\n",
      "Training Model 1 (GBC V1)...\n",
      "Model 1 training complete.\n",
      "Training Model 2 (GBC V2)...\n",
      "Model 2 training complete.\n",
      "Predicting probabilities...\n",
      "Averaging predictions...\n",
      "\n",
      "Submission file 'submission_ensemble_gbc_v1_v2.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (Ensemble):\n",
      "Target\n",
      "0    0.616642\n",
      "1    0.383358\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Model 1 (GBC V1) Training Set Eval ---\n",
      "ROC AUC: 0.9547\n",
      "\n",
      "--- Model 2 (GBC V2) Training Set Eval ---\n",
      "ROC AUC: 0.9569\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold # Keep for reference if needed later\n",
    "# Removed RandomizedSearchCV as we are using pre-found params\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions (V1 and V2) ---\n",
    "\n",
    "# Function V1 (leading to 0.845 score)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    df_processed = df.copy()\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0) # Impute before sum\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    # Impute Income (might be needed if not done before FE)\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    # Impute Age (might be needed if Year_Birth had NaNs)\n",
    "    age_median = df_processed['Age'].median()\n",
    "    df_processed['Age'].fillna(age_median, inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# Function V2 (leading to 0.848 score) - simplified, assuming it's the same as last run\n",
    "def preprocess_data_v2(df, is_train=True, latest_date=None):\n",
    "    df_processed = df.copy()\n",
    "    # --- Previous steps: Age, Lifetime, Date Features ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v2\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        global_latest_date_v2 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v2\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer'].dt.month.fillna(df_processed['Dt_Customer'].dt.month.mode()[0])\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer'].dt.year.fillna(df_processed['Dt_Customer'].dt.year.median())\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer'].dt.dayofweek.fillna(df_processed['Dt_Customer'].dt.dayofweek.mode()[0])\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # --- V2 specific additions / kept originals ---\n",
    "    # Marital_Status kept original\n",
    "    # Education kept original\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0)\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    num_people = df_processed['Children'] + df_processed['Marital_Status'].apply(lambda x: 1 if x in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / num_people.replace(0, 1)).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Total_Purchases'].replace(0, 1)).fillna(0)\n",
    "    # Impute Age (might be needed if Year_Birth had NaNs)\n",
    "    age_median = df_processed['Age'].median()\n",
    "    df_processed['Age'].fillna(age_median, inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing V1 ---\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "\n",
    "# --- Apply Preprocessing V2 ---\n",
    "train_df_processed_v2 = preprocess_data_v2(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v2' not in globals(): global_latest_date_v2 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v2 = preprocess_data_v2(test_df.copy(), is_train=False, latest_date=global_latest_date_v2)\n",
    "print(\"V2 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V1 ---\n",
    "X_v1 = train_df_processed_v1.drop(['ID', 'Target'], axis=1)\n",
    "y_v1 = train_df_processed_v1['Target']\n",
    "X_test_v1 = test_df_processed_v1.drop('ID', axis=1)\n",
    "# Align\n",
    "train_cols_v1 = X_v1.columns\n",
    "missing_in_test_v1 = set(train_cols_v1) - set(X_test_v1.columns)\n",
    "for c in missing_in_test_v1: X_test_v1[c] = 0\n",
    "missing_in_train_v1 = set(X_test_v1.columns) - set(train_cols_v1)\n",
    "for c in missing_in_train_v1: X_v1[c] = 0\n",
    "X_test_v1 = X_test_v1[train_cols_v1]\n",
    "\n",
    "# --- Prepare Data V2 ---\n",
    "X_v2 = train_df_processed_v2.drop(['ID', 'Target'], axis=1)\n",
    "y_v2 = train_df_processed_v2['Target']\n",
    "X_test_v2 = test_df_processed_v2.drop('ID', axis=1)\n",
    "# Align\n",
    "train_cols_v2 = X_v2.columns\n",
    "missing_in_test_v2 = set(train_cols_v2) - set(X_test_v2.columns)\n",
    "for c in missing_in_test_v2: X_test_v2[c] = 0\n",
    "missing_in_train_v2 = set(X_test_v2.columns) - set(train_cols_v2)\n",
    "for c in missing_in_train_v2: X_v2[c] = 0\n",
    "X_test_v2 = X_test_v2[train_cols_v2]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines (Need separate ones for V1 and V2 features) ---\n",
    "\n",
    "# Pipeline V1 Definition\n",
    "numerical_features_v1 = X_v1.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v1 = X_v1.select_dtypes(exclude=np.number).columns.tolist()\n",
    "numerical_pipeline_v1 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v1 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v1 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v1, numerical_features_v1),\n",
    "    ('cat', categorical_pipeline_v1, categorical_features_v1)], remainder='passthrough')\n",
    "\n",
    "# Pipeline V2 Definition\n",
    "numerical_features_v2 = X_v2.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2 = X_v2.select_dtypes(exclude=np.number).columns.tolist()\n",
    "numerical_pipeline_v2 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v2 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v2 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2, numerical_features_v2),\n",
    "    ('cat', categorical_pipeline_v2, categorical_features_v2)], remainder='passthrough')\n",
    "\n",
    "# --- Define BEST Hyperparameters found previously ---\n",
    "\n",
    "# Best parameters for GBC with V1 features (resulted in 0.845 Kaggle score)\n",
    "# Note: These are the params *you reported* finding previously. Double-check if needed.\n",
    "best_params_gbc_v1 = {\n",
    "    'classifier__subsample': 0.6,\n",
    "    'classifier__n_estimators': 200,\n",
    "    'classifier__min_samples_split': 20,\n",
    "    'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'sqrt',\n",
    "    'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.08\n",
    "}\n",
    "\n",
    "# Best parameters for GBC with V2 features (resulted in 0.848 Kaggle score)\n",
    "best_params_gbc_v2 = {\n",
    "    'classifier__subsample': 0.7,\n",
    "    'classifier__n_estimators': 300,\n",
    "    'classifier__min_samples_split': 20,\n",
    "    'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'log2',\n",
    "    'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.05\n",
    "}\n",
    "\n",
    "\n",
    "# --- Build and Train Model 1 (GBC V1) ---\n",
    "print(\"Training Model 1 (GBC V1)...\")\n",
    "pipeline_gbc_v1 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v1),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42)) # Base model\n",
    "])\n",
    "pipeline_gbc_v1.set_params(**best_params_gbc_v1) # Apply best params\n",
    "pipeline_gbc_v1.fit(X_v1, y_v1)\n",
    "print(\"Model 1 training complete.\")\n",
    "\n",
    "# --- Build and Train Model 2 (GBC V2) ---\n",
    "print(\"Training Model 2 (GBC V2)...\")\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42)) # Base model\n",
    "])\n",
    "pipeline_gbc_v2.set_params(**best_params_gbc_v2) # Apply best params\n",
    "pipeline_gbc_v2.fit(X_v2, y_v2)\n",
    "print(\"Model 2 training complete.\")\n",
    "\n",
    "\n",
    "# --- Predict Probabilities on Test Set ---\n",
    "print(\"Predicting probabilities...\")\n",
    "# IMPORTANT: Use the correctly preprocessed test set for each model!\n",
    "probs_gbc_v1 = pipeline_gbc_v1.predict_proba(X_test_v1)[:, 1]\n",
    "probs_gbc_v2 = pipeline_gbc_v2.predict_proba(X_test_v2)[:, 1]\n",
    "\n",
    "# --- Ensemble Averaging ---\n",
    "print(\"Averaging predictions...\")\n",
    "# Simple average (you could also try weighted average if desired)\n",
    "avg_probs = (probs_gbc_v1 + probs_gbc_v2) / 2\n",
    "\n",
    "# Convert probabilities to 0/1 using 0.5 threshold\n",
    "final_predictions = (avg_probs >= 0.5).astype(int)\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df_ensemble = pd.DataFrame({'ID': test_df['ID'], 'Target': final_predictions})\n",
    "submission_filename_ensemble = 'submission_ensemble_gbc_v1_v2.csv'\n",
    "submission_df_ensemble.to_csv(submission_filename_ensemble, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_ensemble}' created successfully.\")\n",
    "print(submission_df_ensemble.head())\n",
    "print(f\"\\nPredicted target distribution (Ensemble):\\n{submission_df_ensemble['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate component models on training data (as a rough check)\n",
    "train_preds_gbc_v1 = pipeline_gbc_v1.predict(X_v1)\n",
    "train_roc_auc_gbc_v1 = roc_auc_score(y_v1, pipeline_gbc_v1.predict_proba(X_v1)[:, 1])\n",
    "print(f\"\\n--- Model 1 (GBC V1) Training Set Eval ---\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v1:.4f}\")\n",
    "\n",
    "train_preds_gbc_v2 = pipeline_gbc_v2.predict(X_v2)\n",
    "train_roc_auc_gbc_v2 = roc_auc_score(y_v2, pipeline_gbc_v2.predict_proba(X_v2)[:, 1])\n",
    "print(f\"\\n--- Model 2 (GBC V2) Training Set Eval ---\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0cbc33-b907-403b-b388-d24d944277dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcbff6-6ab7-4696-bb75-006ec0c16a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "import lightgbm\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    print(\"LightGBM not found. Please install it using: pip install lightgbm\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function V2 (from previous step) ---\n",
    "def preprocess_data_v2(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering (v2) and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    # --- Original Preprocessing ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v2\n",
    "        valid_dates = df_processed['Dt_Customer'].dropna()\n",
    "        global_latest_date_v2 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v2\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "\n",
    "    # Extract Date Features BEFORE calculating lifetime and dropping\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer'].dt.month.fillna(df_processed['Dt_Customer'].dt.month.mode()[0])\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer'].dt.year.fillna(df_processed['Dt_Customer'].dt.year.median())\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer'].dt.dayofweek.fillna(df_processed['Dt_Customer'].dt.dayofweek.mode()[0])\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    median_lifetime = df_processed['Customer_Lifetime'].median()\n",
    "    df_processed['Customer_Lifetime'].fillna(median_lifetime, inplace=True)\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "    # Keep Original Marital_Status & Education for V2\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0) # Impute spending NaNs with 0\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # --- V2 Features ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt']).fillna(0)\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    income_median = df_processed['Income'].median()\n",
    "    df_processed['Income'].fillna(income_median, inplace=True)\n",
    "    # Estimate adults based on Marital_Status original values\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if x in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / num_people.replace(0, 1)).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Total_Purchases'].replace(0, 1)).fillna(0)\n",
    "    age_median = df_processed['Age'].median()\n",
    "    df_processed['Age'].fillna(age_median, inplace=True)\n",
    "    # Drop Num_Adults helper column\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- Apply Preprocessing V2 ---\n",
    "train_df_processed_v2 = preprocess_data_v2(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v2' not in globals(): global_latest_date_v2 = datetime.datetime.now() + datetime.timedelta(days=1); print(f\"Error: global_latest_date_v2 not set. Using fallback: {global_latest_date_v2}\")\n",
    "test_df_processed_v2 = preprocess_data_v2(test_df.copy(), is_train=False, latest_date=global_latest_date_v2)\n",
    "print(\"V2 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V2 ---\n",
    "X_v2 = train_df_processed_v2.drop(['ID', 'Target'], axis=1)\n",
    "y_v2 = train_df_processed_v2['Target']\n",
    "X_test_v2 = test_df_processed_v2.drop('ID', axis=1)\n",
    "# Align columns\n",
    "train_cols_v2 = X_v2.columns\n",
    "test_cols_v2 = X_test_v2.columns\n",
    "missing_in_test_v2 = set(train_cols_v2) - set(test_cols_v2)\n",
    "for c in missing_in_test_v2: X_test_v2[c] = 0\n",
    "missing_in_train_v2 = set(test_cols_v2) - set(train_cols_v2)\n",
    "for c in missing_in_train_v2: X_v2[c] = 0\n",
    "X_test_v2 = X_test_v2[train_cols_v2]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Steps (Using V2 Features) ---\n",
    "numerical_features_v2 = X_v2.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2 = X_v2.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# Ensure 'Age' and 'Customer_Lifetime' are correctly identified if they exist\n",
    "print(f\"\\nV2 Numerical features ({len(numerical_features_v2)}): {numerical_features_v2}\")\n",
    "print(f\"V2 Categorical features ({len(categorical_features_v2)}): {categorical_features_v2}\")\n",
    "\n",
    "numerical_pipeline_v2 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v2 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v2 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2, numerical_features_v2),\n",
    "    ('cat', categorical_pipeline_v2, categorical_features_v2)], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Base Model: LightGBM ---\n",
    "base_model_lgbm = LGBMClassifier(random_state=42, objective='binary') # objective='binary' is good practice\n",
    "\n",
    "# --- Create Full Pipeline with LightGBM ---\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('classifier', base_model_lgbm) # Step name remains 'classifier'\n",
    "])\n",
    "\n",
    "# --- Define Parameter Grid for LightGBM RandomizedSearchCV ---\n",
    "param_dist_lgbm = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'classifier__learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 4, 5, 7, -1], # -1 means no limit\n",
    "    'classifier__num_leaves': [10, 15, 20, 31, 40], # Should be < 2^max_depth\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Alias: bagging_fraction\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Alias: feature_fraction\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1, 0.5, 1.0], # L1\n",
    "    'classifier__reg_lambda': [0, 0.1, 0.5, 1.0, 2.0], # L2\n",
    "    'classifier__min_child_samples': [10, 20, 30, 50] # Min data in leaf\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for LightGBM ---\n",
    "N_ITER_LGBM = 75 # Number of iterations\n",
    "SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "random_search_lgbm = RandomizedSearchCV(\n",
    "    estimator=pipeline_lgbm,\n",
    "    param_distributions=param_dist_lgbm,\n",
    "    n_iter=N_ITER_LGBM,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for LightGBM with {N_ITER_LGBM} iterations for {SCORING_METRIC}...\")\n",
    "random_search_lgbm.fit(X_v2, y_v2) # Use V2 features\n",
    "print(\"LightGBM RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best LightGBM Results ---\n",
    "print(\"\\n--- LightGBM Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_lgbm.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_lgbm = random_search_lgbm.best_params_\n",
    "for param, value in best_params_lgbm.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final LightGBM Model with Best Parameters ---\n",
    "print(\"\\nTraining final LightGBM model on the entire training set using best parameters...\")\n",
    "best_pipeline_lgbm = random_search_lgbm.best_estimator_\n",
    "# best_pipeline_lgbm.fit(X_v2, y_v2) # Already refit by default\n",
    "print(\"Final LightGBM model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Tuned LightGBM ---\n",
    "print(\"Predicting on test data using the tuned LightGBM model...\")\n",
    "test_predictions_lgbm = best_pipeline_lgbm.predict(X_test_v2) # Use V2 test features\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for LightGBM ---\n",
    "submission_df_lgbm = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_lgbm})\n",
    "submission_filename_lgbm = 'submission_tuned_lgbm_v2_features.csv' # New filename\n",
    "submission_df_lgbm.to_csv(submission_filename_lgbm, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_lgbm}' created successfully.\")\n",
    "print(submission_df_lgbm.head())\n",
    "print(f\"\\nPredicted target distribution (LightGBM):\\n{submission_df_lgbm['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* LightGBM model on the training set\n",
    "train_preds_lgbm_tuned = best_pipeline_lgbm.predict(X_v2)\n",
    "train_accuracy_lgbm_tuned = accuracy_score(y_v2, train_preds_lgbm_tuned)\n",
    "train_roc_auc_lgbm_tuned = roc_auc_score(y_v2, best_pipeline_lgbm.predict_proba(X_v2)[:, 1])\n",
    "print(f\"\\n--- Tuned LightGBM Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_lgbm_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_lgbm_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6d14f-6262-49a2-ac6f-4174d748b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773ae0e1-c330-4681-a5c8-f4646a290f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Imputers fitted on training data (excluding Target).\n",
      "Scaler fitted on training data (excluding Target).\n",
      "Test data imputed using fitted imputers.\n",
      "Test data scaled using fitted scaler.\n",
      "V4 Preprocessing (Manual Impute/Scale - Corrected) complete.\n",
      "\n",
      "V4 Categorical features by name: ['Education', 'Marital_Status']\n",
      "\n",
      "Starting RandomizedSearchCV for CatBoost (Manual Preprocessing) with 75 iterations for roc_auc...\n",
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n",
      "CatBoost RandomizedSearchCV finished.\n",
      "\n",
      "--- CatBoost Hyperparameter Tuning Results ---\n",
      "Best Score (roc_auc): 0.9260\n",
      "Best Parameters:\n",
      "  subsample: 0.9\n",
      "  learning_rate: 0.07\n",
      "  l2_leaf_reg: 3\n",
      "  iterations: 100\n",
      "  depth: 4\n",
      "  border_count: 64\n",
      "\n",
      "Training final CatBoost model on the entire training set using best parameters...\n",
      "Final CatBoost model training complete.\n",
      "Predicting on test data using the tuned CatBoost model...\n",
      "Prediction complete.\n",
      "\n",
      "Submission file 'submission_tuned_catboost_v4_manualprep.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (CatBoost V4):\n",
      "Target\n",
      "0    0.6211\n",
      "1    0.3789\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Tuned CatBoost V4 Model Training Set Evaluation ---\n",
      "Accuracy: 0.8813\n",
      "ROC AUC: 0.9602\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError:\n",
    "    print(\"CatBoost not found. Please install it using: pip install catboost\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function V4 (Corrected Target Exclusion) ---\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    \"\"\" V2 features, keeps original categoricals, handles imputation+scaling manually \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    target_col = 'Target' # Define target column name\n",
    "\n",
    "    # --- Step 1: Initial Feature Creation ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1)\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Identify initial feature types *excluding Target and ID*\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    # --- Step 2: Imputation ---\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        # Fit only on the selected features from the training set\n",
    "        imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        # Fit only on the selected features from the training set\n",
    "        imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "        print(\"Imputers fitted on training data (excluding Target).\")\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers must be provided for test data\")\n",
    "        try:\n",
    "            # Transform using the same feature list used in fit\n",
    "            df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "            df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "        except Exception as e:\n",
    "             print(f\"Error during imputation transformation: {e}\")\n",
    "             print(\"Columns being imputed (Num):\", initial_numerical_features)\n",
    "             print(\"Columns being imputed (Cat):\", initial_categorical_features)\n",
    "             print(\"Columns available in df:\", df_processed.columns.tolist())\n",
    "             raise e\n",
    "        print(\"Test data imputed using fitted imputers.\")\n",
    "\n",
    "    # Convert imputed arrays back to DataFrame -> Important: Keep Target if it exists!\n",
    "    original_cols = df_processed.columns # Store original columns before potential array conversion\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    # Add back ID and Target if they exist\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed # Overwrite df_processed with the correctly columned DataFrame\n",
    "\n",
    "    # --- Step 3: Create Derived Features ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    # --- Step 4: Scaling ---\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        # Fit only on numerical features (excluding Target)\n",
    "        scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "        print(\"Scaler fitted on training data (excluding Target).\")\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler must be provided for test data\")\n",
    "        try:\n",
    "             # Transform using the same feature list used in fit\n",
    "             df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "        except Exception as e:\n",
    "             print(f\"Error during scaling transformation: {e}\")\n",
    "             print(\"Columns being scaled:\", final_numerical_features)\n",
    "             print(\"Columns available in df:\", df_processed.columns.tolist())\n",
    "             raise e\n",
    "        print(\"Test data scaled using fitted scaler.\")\n",
    "\n",
    "    # --- Step 5: Final Type Conversion for CatBoost ---\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features:\n",
    "        df_processed[col] = df_processed[col].astype(str)\n",
    "\n",
    "    # Ensure all column names (except Target maybe) are strings\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: # Rename target back if needed\n",
    "         df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "\n",
    "\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V4 (Manual - Corrected) ---\n",
    "# Need error handling around these calls as well\n",
    "try:\n",
    "    train_df_processed_v4, fitted_imputers, fitted_scaler = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "    if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1); print(f\"Error: global_latest_date_v4 not set. Using fallback: {global_latest_date_v4}\")\n",
    "    test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers, fit_scaler=fitted_scaler)\n",
    "    print(\"V4 Preprocessing (Manual Impute/Scale - Corrected) complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during preprocessing: {e}\")\n",
    "    exit() # Exit if preprocessing fails\n",
    "\n",
    "# --- Prepare Data V4 ---\n",
    "try:\n",
    "    X_v4 = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "    y_v4 = train_df_processed_v4['Target'] # Target should exist here\n",
    "    X_test_v4 = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "\n",
    "    # Align columns\n",
    "    train_cols_v4 = X_v4.columns\n",
    "    test_cols_v4 = X_test_v4.columns\n",
    "    missing_in_test_v4 = set(train_cols_v4) - set(test_cols_v4)\n",
    "    for c in missing_in_test_v4: X_test_v4[c] = 0\n",
    "    missing_in_train_v4 = set(test_cols_v4) - set(train_cols_v4)\n",
    "    for c in missing_in_train_v4: X_v4[c] = 0\n",
    "    X_test_v4 = X_test_v4[train_cols_v4]\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError during data preparation after preprocessing: {e}\")\n",
    "    print(\"Columns in train_df_processed_v4:\", train_df_processed_v4.columns)\n",
    "    print(\"Columns in test_df_processed_v4:\", test_df_processed_v4.columns)\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during data preparation: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Identify Categorical Feature *Names* for CatBoost AFTER all processing\n",
    "categorical_features_v4_names = X_v4.select_dtypes(include='object').columns.tolist()\n",
    "print(f\"\\nV4 Categorical features by name: {categorical_features_v4_names}\")\n",
    "\n",
    "\n",
    "# --- Define Base Model: CatBoost (No Pipeline Needed) ---\n",
    "base_model_catboost = CatBoostClassifier(\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    cat_features=categorical_features_v4_names # Pass NAMES\n",
    ")\n",
    "\n",
    "# --- Define Parameter Grid for CatBoost RandomizedSearchCV ---\n",
    "param_dist_catboost = {\n",
    "    'iterations': [100, 200, 300, 500, 700, 900],\n",
    "    'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.07],\n",
    "    'depth': [4, 5, 6, 7],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 64, 128],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "# --- Set up K-Fold Strategy ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for CatBoost (No Pipeline) ---\n",
    "N_ITER_CATBOOST = 75\n",
    "SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "random_search_catboost = RandomizedSearchCV(\n",
    "    estimator=base_model_catboost,\n",
    "    param_distributions=param_dist_catboost,\n",
    "    n_iter=N_ITER_CATBOOST,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for CatBoost (Manual Preprocessing) with {N_ITER_CATBOOST} iterations for {SCORING_METRIC}...\")\n",
    "# Fit directly on the preprocessed dataframes\n",
    "random_search_catboost.fit(X_v4, y_v4)\n",
    "print(\"CatBoost RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best CatBoost Results ---\n",
    "print(\"\\n--- CatBoost Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_catboost.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\")\n",
    "best_params_catboost = random_search_catboost.best_params_\n",
    "for param, value in best_params_catboost.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final CatBoost Model with Best Parameters ---\n",
    "print(\"\\nTraining final CatBoost model on the entire training set using best parameters...\")\n",
    "best_model_catboost = random_search_catboost.best_estimator_\n",
    "print(\"Final CatBoost model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Tuned CatBoost ---\n",
    "print(\"Predicting on test data using the tuned CatBoost model...\")\n",
    "test_predictions_catboost = best_model_catboost.predict(X_test_v4)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for CatBoost ---\n",
    "submission_df_catboost = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_catboost})\n",
    "submission_filename_catboost = 'submission_tuned_catboost_v4_manualprep.csv' # New filename\n",
    "submission_df_catboost.to_csv(submission_filename_catboost, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_catboost}' created successfully.\")\n",
    "print(submission_df_catboost.head())\n",
    "print(f\"\\nPredicted target distribution (CatBoost V4):\\n{submission_df_catboost['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* CatBoost model on the training set\n",
    "train_preds_catboost_tuned = best_model_catboost.predict(X_v4)\n",
    "train_accuracy_catboost_tuned = accuracy_score(y_v4, train_preds_catboost_tuned)\n",
    "train_roc_auc_catboost_tuned = roc_auc_score(y_v4, best_model_catboost.predict_proba(X_v4)[:, 1])\n",
    "print(f\"\\n--- Tuned CatBoost V4 Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_catboost_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_catboost_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5f37fb-975c-49cc-b222-9c36473558b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing V1...\n",
      "V1 Preprocessing complete.\n",
      "\n",
      "Preprocessing V4 (for CatBoost)...\n",
      "V4 Preprocessing complete.\n",
      "\n",
      "Training Model 1 (GBC V1)...\n",
      "Model 1 training complete.\n",
      "Training Model 2 (GBC V2 Features + OHE)...\n",
      "Model 2 training complete.\n",
      "Training Model 3 (CatBoost V4 Features - Manual Prep)...\n",
      "Model 3 training complete.\n",
      "Predicting probabilities...\n",
      "Averaging predictions...\n",
      "\n",
      "Submission file 'submission_ensemble_3model_avg.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (3-Model Ensemble):\n",
      "Target\n",
      "0    0.624071\n",
      "1    0.375929\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# No CV needed here, just loading/training final models\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression # Added for meta-model if doing stacking later\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError:\n",
    "    print(\"CatBoost not found. Please install it using: pip install catboost\")\n",
    "    exit()\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions (V1 and V4 needed) ---\n",
    "\n",
    "# Function V1 (Correct definition for GBC V1 - returns only DataFrame)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    \"\"\" V1 features with simplified categoricals, no manual impute/scale return \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    target_col = 'Target'\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1 # Keep distinct global date var\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        # Use try-except for safety if no valid dates exist\n",
    "        try:\n",
    "            global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: # Handle case where valid_dates might be empty or cause issues\n",
    "             global_latest_date_v1 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "             print(f\"Warning: Error setting global_latest_date_v1. Using fallback: {global_latest_date_v1}\")\n",
    "\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # V1 Simplifications\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner', 'Together': 'Partner',\n",
    "        'Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'\n",
    "    })\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0) # Simple imputation\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Simple median imputation for remaining numericals (before pipeline)\n",
    "    cols_to_impute_num = ['Age', 'Income', 'Customer_Lifetime', 'Total_Mnt'] # Add others if needed\n",
    "    for col in cols_to_impute_num:\n",
    "         if col in df_processed.columns and df_processed[col].isnull().any():\n",
    "              median_val = df_processed[col].median() # Calculate median before filling\n",
    "              df_processed[col].fillna(median_val, inplace=True)\n",
    "\n",
    "    # Simple mode imputation for remaining categoricals (before pipeline)\n",
    "    cat_cols_simple = ['Marital_Status', 'Education']\n",
    "    for col in cat_cols_simple:\n",
    "        if col in df_processed.columns and df_processed[col].isnull().any():\n",
    "            mode_val = df_processed[col].mode()[0] # Calculate mode before filling\n",
    "            df_processed[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "    return df_processed # Only return the DataFrame\n",
    "\n",
    "\n",
    "# Function V4 (for CatBoost - Manual Impute/Scale) - Keep as is from previous correct version\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    # ... (Keep the full V4 function definition from the previous working block) ...\n",
    "    df_processed = df.copy()\n",
    "    target_col = 'Target' # Define target column name\n",
    "    # --- Step 1: Initial Feature Creation (before imputation/scaling) ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth'] # Age calculated, may have NaNs\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True) # Keep intermediate\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        # Use try-except for safety if no valid dates exist\n",
    "        try:\n",
    "            global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: # Handle case where valid_dates might be empty or cause issues\n",
    "             global_latest_date_v4 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "             print(f\"Warning: Error setting global_latest_date_v4. Using fallback: {global_latest_date_v4}\")\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month # Keep as number initially for imputation\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek # Keep as number initially\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days # May have NaNs\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore') # Drop original and intermediate date cols\n",
    "    # Keep Original Marital_Status & Education\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1) # Calculated before imputation, may include NaNs if components are NaN\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    # Don't create ratio/derived features yet\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1) # Denominator stored temporarily\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Identify initial feature types *excluding Target and ID*\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    # --- Step 2: Imputation ---\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "        # print(\"Imputers fitted on training data (excluding Target).\") # Optional print\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers must be provided for test data\")\n",
    "        try:\n",
    "            df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "            df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "        except Exception as e:\n",
    "             print(f\"Error during imputation transformation: {e}\")\n",
    "             print(\"Columns available in df:\", df_processed.columns.tolist())\n",
    "             raise e\n",
    "        # print(\"Test data imputed using fitted imputers.\") # Optional print\n",
    "\n",
    "    # Reconstruct DataFrame -> Important: Keep Target if it exists!\n",
    "    original_cols = df_processed.columns\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "\n",
    "    # --- Step 3: Create Derived Features ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    # --- Step 4: Scaling ---\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "        # print(\"Scaler fitted on training data (excluding Target).\") # Optional\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler must be provided for test data\")\n",
    "        try:\n",
    "             df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "        except Exception as e:\n",
    "             print(f\"Error during scaling transformation: {e}\")\n",
    "             raise e\n",
    "        # print(\"Test data scaled using fitted scaler.\") # Optional\n",
    "\n",
    "    # --- Step 5: Final Type Conversion for CatBoost ---\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V1 (Corrected Call) ---\n",
    "print(\"Preprocessing V1...\")\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True) # Now expects only 1 return value\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "\n",
    "# --- Apply Preprocessing V4 (for CatBoost - Manual Impute/Scale) ---\n",
    "print(\"\\nPreprocessing V4 (for CatBoost)...\")\n",
    "train_df_processed_v4, fitted_imputers_v4, fitted_scaler_v4 = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers_v4, fit_scaler=fitted_scaler_v4)\n",
    "print(\"V4 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V1 (for GBC V1) ---\n",
    "# ... (rest of data prep V1 is likely fine) ...\n",
    "X_v1 = train_df_processed_v1.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v1 = train_df_processed_v1['Target']\n",
    "X_test_v1 = test_df_processed_v1.drop('ID', axis=1, errors='ignore')\n",
    "train_cols_v1 = X_v1.columns; test_cols_v1 = X_test_v1.columns\n",
    "missing_in_test_v1 = set(train_cols_v1) - set(test_cols_v1)\n",
    "for c in missing_in_test_v1: X_test_v1[c] = 0\n",
    "missing_in_train_v1 = set(test_cols_v1) - set(train_cols_v1)\n",
    "for c in missing_in_train_v1: X_v1[c] = 0\n",
    "X_test_v1 = X_test_v1[train_cols_v1]\n",
    "\n",
    "# --- Prepare Data V2 (for GBC V2 - using V4 preproc function for consistency, but applying OHE) ---\n",
    "X_v2_like = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v2_like = train_df_processed_v4['Target'] # Target comes from V4 processed train\n",
    "X_test_v2_like = test_df_processed_v4.drop('ID', axis=1, errors='ignore') # Features come from V4 processed test\n",
    "# Align V2_like\n",
    "train_cols_v2_like = X_v2_like.columns; test_cols_v2_like = X_test_v2_like.columns\n",
    "missing_in_test_v2_like = set(train_cols_v2_like) - set(test_cols_v2_like)\n",
    "for c in missing_in_test_v2_like: X_test_v2_like[c] = 0\n",
    "missing_in_train_v2_like = set(test_cols_v2_like) - set(train_cols_v2_like)\n",
    "for c in missing_in_train_v2_like: X_v2_like[c] = 0\n",
    "X_test_v2_like = X_test_v2_like[train_cols_v2_like]\n",
    "\n",
    "\n",
    "# --- Prepare Data V4 (for CatBoost) ---\n",
    "X_v4 = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v4 = train_df_processed_v4['Target']\n",
    "X_test_v4 = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "# Align V4 (redundant if V2_like is aligned, but safe)\n",
    "train_cols_v4 = X_v4.columns\n",
    "test_cols_v4 = X_test_v4.columns\n",
    "missing_in_test_v4 = set(train_cols_v4) - set(test_cols_v4)\n",
    "for c in missing_in_test_v4: X_test_v4[c] = 0\n",
    "missing_in_train_v4 = set(test_cols_v4) - set(train_cols_v4)\n",
    "for c in missing_in_train_v4: X_v4[c] = 0\n",
    "X_test_v4 = X_test_v4[train_cols_v4]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines V1 (GBC V1 requires OHE) ---\n",
    "numerical_features_v1 = X_v1.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v1 = X_v1.select_dtypes(exclude=np.number).columns.tolist()\n",
    "numerical_pipeline_v1 = Pipeline([('imputer_num', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_pipeline_v1 = Pipeline([('imputer_cat', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v1 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v1, numerical_features_v1),\n",
    "    ('cat', categorical_pipeline_v1, categorical_features_v1)], remainder='passthrough')\n",
    "\n",
    "# --- Define Preprocessing Pipelines V2 (GBC V2 requires OHE on V4 features) ---\n",
    "numerical_features_v2_like = X_v2_like.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v2_like = X_v2_like.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# Define pipelines assuming imputation happened in preprocess_data_v4\n",
    "numerical_pipeline_v2_like = Pipeline([('scaler', StandardScaler())]) # Scaling only\n",
    "categorical_pipeline_v2_like = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]) # OHE only\n",
    "preprocessor_v2_like = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v2_like, numerical_features_v2_like),\n",
    "    ('cat', categorical_pipeline_v2_like, categorical_features_v2_like)], remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define BEST Hyperparameters ---\n",
    "# ... (keep best params definitions) ...\n",
    "best_params_gbc_v1 = {\n",
    "    'classifier__subsample': 0.6, 'classifier__n_estimators': 200,\n",
    "    'classifier__min_samples_split': 20, 'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'sqrt', 'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.08}\n",
    "best_params_gbc_v2 = { # Params that gave 0.848\n",
    "    'classifier__subsample': 0.7, 'classifier__n_estimators': 300,\n",
    "    'classifier__min_samples_split': 20, 'classifier__min_samples_leaf': 20,\n",
    "    'classifier__max_features': 'log2', 'classifier__max_depth': 2,\n",
    "    'classifier__learning_rate': 0.05}\n",
    "best_params_catboost_v4 = { # Params found for CatBoost V4\n",
    "    'subsample': 0.9, 'learning_rate': 0.07, 'l2_leaf_reg': 3,\n",
    "    'iterations': 100, 'depth': 4, 'border_count': 64}\n",
    "\n",
    "# --- Build and Train Model 1 (GBC V1) ---\n",
    "print(\"\\nTraining Model 1 (GBC V1)...\")\n",
    "pipeline_gbc_v1 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v1),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "pipeline_gbc_v1.set_params(**best_params_gbc_v1)\n",
    "pipeline_gbc_v1.fit(X_v1, y_v1)\n",
    "print(\"Model 1 training complete.\")\n",
    "\n",
    "# --- Build and Train Model 2 (GBC V2 features + OHE) ---\n",
    "print(\"Training Model 2 (GBC V2 Features + OHE)...\")\n",
    "pipeline_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2_like),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "pipeline_gbc_v2.set_params(**best_params_gbc_v2)\n",
    "pipeline_gbc_v2.fit(X_v2_like, y_v2_like)\n",
    "print(\"Model 2 training complete.\")\n",
    "\n",
    "# --- Build and Train Model 3 (CatBoost V4 features - Manual Prep) ---\n",
    "print(\"Training Model 3 (CatBoost V4 Features - Manual Prep)...\")\n",
    "# Identify final features for CatBoost training\n",
    "categorical_features_v4_names = X_v4.select_dtypes(include='object').columns.tolist() # Names needed for CatBoost model init\n",
    "\n",
    "model_catboost_final = CatBoostClassifier(\n",
    "    random_state=42, verbose=0, loss_function='Logloss', eval_metric='AUC',\n",
    "    cat_features=categorical_features_v4_names, **best_params_catboost_v4 # Apply best params\n",
    ")\n",
    "# Train directly on the V4 data (already imputed and scaled in preprocess function)\n",
    "model_catboost_final.fit(X_v4, y_v4)\n",
    "print(\"Model 3 training complete.\")\n",
    "\n",
    "# --- Predict Probabilities on Test Set ---\n",
    "print(\"Predicting probabilities...\")\n",
    "probs_gbc_v1 = pipeline_gbc_v1.predict_proba(X_test_v1)[:, 1]\n",
    "probs_gbc_v2 = pipeline_gbc_v2.predict_proba(X_test_v2_like)[:, 1]\n",
    "probs_catboost = model_catboost_final.predict_proba(X_test_v4)[:, 1] # Predict on manually prepped test data\n",
    "\n",
    "# --- Ensemble Averaging ---\n",
    "print(\"Averaging predictions...\")\n",
    "avg_probs_3 = (probs_gbc_v1 + probs_gbc_v2 + probs_catboost) / 3\n",
    "final_predictions = (avg_probs_3 >= 0.5).astype(int)\n",
    "\n",
    "# --- Generate Submission File ---\n",
    "submission_df_ensemble3 = pd.DataFrame({'ID': test_df['ID'], 'Target': final_predictions})\n",
    "submission_filename_ensemble3 = 'submission_ensemble_3model_avg.csv'\n",
    "submission_df_ensemble3.to_csv(submission_filename_ensemble3, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_ensemble3}' created successfully.\")\n",
    "print(submission_df_ensemble3.head())\n",
    "print(f\"\\nPredicted target distribution (3-Model Ensemble):\\n{submission_df_ensemble3['Target'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e8d069-758d-4f75-aeda-1d9474293b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Preprocessing V5 (Poly/Interaction Features)...\n",
      "Added 36 polynomial/interaction features.\n",
      "V5 Preprocessing complete.\n",
      "\n",
      "V5 Numerical features (72): 72 features\n",
      "V5 Categorical features (2): ['Education', 'Marital_Status']\n",
      "\n",
      "Starting RandomizedSearchCV for GBC V5 (Poly/Interact + Selection) with 60 iterations...\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "GBC V5 RandomizedSearchCV finished.\n",
      "\n",
      "--- GBC V5 (Poly/Interact + Selection) Hyperparameter Tuning Results ---\n",
      "Best Score (roc_auc): 0.92078\n",
      "Best Parameters:\n",
      "  classifier__subsample: 0.7\n",
      "  classifier__n_estimators: 200\n",
      "  classifier__min_samples_split: 20\n",
      "  classifier__min_samples_leaf: 15\n",
      "  classifier__max_features: log2\n",
      "  classifier__max_depth: 2\n",
      "  classifier__learning_rate: 0.07\n",
      "\n",
      "Training final GBC V5 model on the entire training set...\n",
      "Final GBC V5 model training complete.\n",
      "Predicting on test data using the tuned GBC V5 model...\n",
      "Prediction complete.\n",
      "\n",
      "Submission file 'submission_tuned_gbc_v5_poly_select.csv' created successfully.\n",
      "      ID  Target\n",
      "0   4390       1\n",
      "1  10478       1\n",
      "2   1081       1\n",
      "3   4261       1\n",
      "4   9916       0\n",
      "\n",
      "Predicted target distribution (GBC V5):\n",
      "Target\n",
      "0    0.631501\n",
      "1    0.368499\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Tuned GBC V5 Model Training Set Evaluation ---\n",
      "Accuracy: 0.8749\n",
      "ROC AUC: 0.95686\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV # Using Lasso for feature selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Function V5 (Adds Poly/Interaction) ---\n",
    "def preprocess_data_v5_poly_interact(df, is_train=True, latest_date=None, fit_imputers=None, poly_feature_names=None):\n",
    "    \"\"\" V4 + Polynomial/Interaction features, returns df before scaling \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    target_col = 'Target'\n",
    "    # --- Initial Feature Creation (Same as V4) ---\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v5 # Use distinct name\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        global_latest_date_v5 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v5\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1)\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # --- Imputation (Manual) ---\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed for test\")\n",
    "        df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    # Reconstruct DataFrame\n",
    "    original_cols = df_processed.columns\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "\n",
    "    # --- Create Derived Features (Post-Imputation) ---\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    # --- Add Interaction & Polynomial Features ---\n",
    "    # Select top numerical features based on previous importance analysis\n",
    "    poly_cols = ['Recency', 'MntWines', 'Total_CmpAccepted', 'Total_Purchases', 'NumWebPurchases', 'Customer_Lifetime', 'Total_Mnt', 'Age']\n",
    "    # Ensure these columns exist after previous steps\n",
    "    poly_cols = [col for col in poly_cols if col in df_processed.columns]\n",
    "\n",
    "    if is_train:\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False) # Try full quadratic\n",
    "        poly_features = poly.fit_transform(df_processed[poly_cols])\n",
    "        # Get new feature names\n",
    "        poly_feature_names = poly.get_feature_names_out(poly_cols)\n",
    "        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_processed.index)\n",
    "        # Drop original columns used in poly features to avoid duplication? Optional, keep for now.\n",
    "        # df_processed = df_processed.drop(columns=poly_cols)\n",
    "        df_processed = pd.concat([df_processed, poly_df.drop(columns=poly_cols)], axis=1) # Add only new terms\n",
    "        print(f\"Added {len(poly_feature_names) - len(poly_cols)} polynomial/interaction features.\")\n",
    "    else:\n",
    "        if poly_feature_names is None:\n",
    "             raise ValueError(\"Polynomial feature names must be provided for test data transformation.\")\n",
    "        # Need to apply the *same* transformation. Requires fitted poly object or careful reconstruction.\n",
    "        # Easier approach for now: Recalculate on test, assuming same features are generated.\n",
    "        # THIS IS NOT IDEAL for production but simpler for testing feature impact here.\n",
    "        # A robust solution uses the fitted poly object from training.\n",
    "        poly_test = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "        poly_features_test = poly_test.fit_transform(df_processed[poly_cols])\n",
    "        poly_feature_names_test = poly_test.get_feature_names_out(poly_cols) # Get names from test fit\n",
    "        poly_df_test = pd.DataFrame(poly_features_test, columns=poly_feature_names_test, index=df_processed.index)\n",
    "\n",
    "        # Align columns based on training names (important!)\n",
    "        poly_df_test_aligned = pd.DataFrame(index=poly_df_test.index) # Empty df with same index\n",
    "        for col_name in poly_feature_names: # Iterate through names FROM TRAINING\n",
    "            if col_name in poly_df_test.columns and col_name not in poly_cols: # Check if exists in test and is a new term\n",
    "                poly_df_test_aligned[col_name] = poly_df_test[col_name]\n",
    "            elif col_name not in poly_cols: # If missing in test, add column of zeros\n",
    "                poly_df_test_aligned[col_name] = 0\n",
    "\n",
    "        df_processed = pd.concat([df_processed, poly_df_test_aligned], axis=1)\n",
    "\n",
    "    # --- Manual Interaction Example ---\n",
    "    if 'Recency' in df_processed.columns and 'Total_CmpAccepted' in df_processed.columns:\n",
    "         df_processed['Recency_x_Cmp'] = df_processed['Recency'] * df_processed['Total_CmpAccepted']\n",
    "\n",
    "\n",
    "    # Convert final categoricals to string (important BEFORE pipeline)\n",
    "    final_categorical_features = df_processed.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features:\n",
    "        df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str) # Ensure all column names are strings\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "\n",
    "\n",
    "    return df_processed, fit_imputers, poly_feature_names # Return poly names for test set\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V5 ---\n",
    "print(\"Preprocessing V5 (Poly/Interaction Features)...\")\n",
    "train_df_processed_v5, fitted_imputers_v5, poly_names_v5 = preprocess_data_v5_poly_interact(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v5' not in globals(): global_latest_date_v5 = datetime.datetime.now() + datetime.timedelta(days=1); print(f\"Error: global_latest_date_v5 not set. Using fallback: {global_latest_date_v5}\")\n",
    "test_df_processed_v5, _, _ = preprocess_data_v5_poly_interact(test_df.copy(), is_train=False, latest_date=global_latest_date_v5, fit_imputers=fitted_imputers_v5, poly_feature_names=poly_names_v5)\n",
    "print(\"V5 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V5 ---\n",
    "X_v5 = train_df_processed_v5.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v5 = train_df_processed_v5['Target']\n",
    "X_test_v5 = test_df_processed_v5.drop('ID', axis=1, errors='ignore')\n",
    "# Align columns\n",
    "train_cols_v5 = X_v5.columns\n",
    "test_cols_v5 = X_test_v5.columns\n",
    "missing_in_test_v5 = set(train_cols_v5) - set(test_cols_v5)\n",
    "for c in missing_in_test_v5: X_test_v5[c] = 0\n",
    "missing_in_train_v5 = set(test_cols_v5) - set(train_cols_v5)\n",
    "for c in missing_in_train_v5: X_v5[c] = 0\n",
    "X_test_v5 = X_test_v5[train_cols_v5] # Ensure order\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipeline V5 (OHE + Scale) ---\n",
    "numerical_features_v5 = X_v5.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_v5 = X_v5.select_dtypes(exclude=np.number).columns.tolist()\n",
    "print(f\"\\nV5 Numerical features ({len(numerical_features_v5)}): {len(numerical_features_v5)} features\") # Print count due to length\n",
    "# print(numerical_features_v5) # Optionally print list\n",
    "print(f\"V5 Categorical features ({len(categorical_features_v5)}): {categorical_features_v5}\")\n",
    "\n",
    "\n",
    "# Pipeline for GBC (Scale Numerics, OHE Categoricals)\n",
    "# Imputation already done in preprocess_data_v5\n",
    "numerical_pipeline_v5 = Pipeline([('scaler', StandardScaler())])\n",
    "categorical_pipeline_v5 = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v5 = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_v5, numerical_features_v5),\n",
    "    ('cat', categorical_pipeline_v5, categorical_features_v5)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define Feature Selector ---\n",
    "# Using LassoCV for selection based on linear importance\n",
    "# Threshold='median' means select features with importance > median importance\n",
    "# Can also use a float like '1.25*mean' or a specific value\n",
    "selector = SelectFromModel(\n",
    "    estimator=LassoCV(cv=3, random_state=42, max_iter=2000), # LassoCV finds best alpha\n",
    "    threshold='median', # Select features with importance above the median\n",
    "    prefit=False # Estimator will be fit automatically\n",
    ")\n",
    "\n",
    "# --- Define Base Model: GBC ---\n",
    "base_model_gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# --- Create Full Pipeline with Selection ---\n",
    "pipeline_gbc_v5_select = Pipeline([\n",
    "    ('preprocessor', preprocessor_v5),\n",
    "    ('selector', selector), # Add feature selection step\n",
    "    ('classifier', base_model_gbc)\n",
    "])\n",
    "\n",
    "\n",
    "# --- Define Parameter Grid for GBC V5 (Focus on GBC Params) ---\n",
    "# Use previous best GBC V2 params as a guide\n",
    "param_dist_gbc_v5 = {\n",
    "    'classifier__n_estimators': [200, 300, 400],\n",
    "    'classifier__learning_rate': [0.03, 0.05, 0.07, 0.1],\n",
    "    'classifier__max_depth': [2, 3],\n",
    "    'classifier__min_samples_leaf': [15, 20, 25],\n",
    "    'classifier__min_samples_split': [20, 30, 40],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8],\n",
    "    'classifier__max_features': ['sqrt', 'log2', None]\n",
    "    # Can also tune selector threshold if needed: 'selector__threshold': ['median', 'mean']\n",
    "}\n",
    "\n",
    "\n",
    "# --- Set up K-Fold Strategy ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE_KFOLD = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE_KFOLD)\n",
    "\n",
    "# --- Set up RandomizedSearchCV for GBC V5 ---\n",
    "N_ITER_GBC_V5 = 60 # Adjust iterations based on time constraints\n",
    "SCORING_METRIC = 'roc_auc'\n",
    "\n",
    "random_search_gbc_v5 = RandomizedSearchCV(\n",
    "    estimator=pipeline_gbc_v5_select,\n",
    "    param_distributions=param_dist_gbc_v5,\n",
    "    n_iter=N_ITER_GBC_V5,\n",
    "    scoring=SCORING_METRIC,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting RandomizedSearchCV for GBC V5 (Poly/Interact + Selection) with {N_ITER_GBC_V5} iterations...\")\n",
    "random_search_gbc_v5.fit(X_v5, y_v5)\n",
    "print(\"GBC V5 RandomizedSearchCV finished.\")\n",
    "\n",
    "# --- Report Best GBC V5 Results ---\n",
    "print(\"\\n--- GBC V5 (Poly/Interact + Selection) Hyperparameter Tuning Results ---\")\n",
    "print(f\"Best Score ({SCORING_METRIC}): {random_search_gbc_v5.best_score_:.5f}\") # More precision\n",
    "print(\"Best Parameters:\")\n",
    "best_params_gbc_v5 = random_search_gbc_v5.best_params_\n",
    "for param, value in best_params_gbc_v5.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# --- Train Final GBC V5 Model with Best Parameters ---\n",
    "print(\"\\nTraining final GBC V5 model on the entire training set...\")\n",
    "best_pipeline_gbc_v5 = random_search_gbc_v5.best_estimator_\n",
    "print(\"Final GBC V5 model training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Tuned GBC V5 ---\n",
    "print(\"Predicting on test data using the tuned GBC V5 model...\")\n",
    "test_predictions_gbc_v5 = best_pipeline_gbc_v5.predict(X_test_v5)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for GBC V5 ---\n",
    "submission_df_gbc_v5 = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_gbc_v5})\n",
    "submission_filename_gbc_v5 = 'submission_tuned_gbc_v5_poly_select.csv'\n",
    "submission_df_gbc_v5.to_csv(submission_filename_gbc_v5, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_gbc_v5}' created successfully.\")\n",
    "print(submission_df_gbc_v5.head())\n",
    "print(f\"\\nPredicted target distribution (GBC V5):\\n{submission_df_gbc_v5['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate the *tuned* GBC V5 model on the training set\n",
    "train_preds_gbc_v5_tuned = best_pipeline_gbc_v5.predict(X_v5)\n",
    "train_accuracy_gbc_v5_tuned = accuracy_score(y_v5, train_preds_gbc_v5_tuned)\n",
    "train_roc_auc_gbc_v5_tuned = roc_auc_score(y_v5, best_pipeline_gbc_v5.predict_proba(X_v5)[:, 1])\n",
    "print(f\"\\n--- Tuned GBC V5 Model Training Set Evaluation ---\")\n",
    "print(f\"Accuracy: {train_accuracy_gbc_v5_tuned:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc_gbc_v5_tuned:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34369f-7e2d-478c-afdf-da9eee1abbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier # Import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression # Meta-model\n",
    "import datetime\n",
    "import warnings\n",
    "try:\n",
    "    from catboost import CatBoostClassifier # Keep for potential future use\n",
    "except ImportError:\n",
    "    print(\"CatBoost not found.\")\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Feature Engineering & Preprocessing Functions ---\n",
    "# Using V1 function (for GBC V1) and V4 function (as base for GBC V2 structure)\n",
    "\n",
    "# Function V1 (Correct definition for GBC V1 - returns only DataFrame)\n",
    "def preprocess_data_v1(df, is_train=True, latest_date=None):\n",
    "    \"\"\" V1 features with simplified categoricals, basic imputation \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'; current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v1\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v1 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v1 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v1\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1)\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'})\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    for col in mnt_cols: df_processed[col] = df_processed[col].fillna(0)\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    cols_to_impute_num = ['Age', 'Income', 'Customer_Lifetime', 'Total_Mnt']\n",
    "    for col in cols_to_impute_num:\n",
    "         if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    cat_cols_simple = ['Marital_Status', 'Education']\n",
    "    for col in cat_cols_simple:\n",
    "        if col in df_processed.columns and df_processed[col].isnull().any(): df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "# Function V4 (Manual Impute/Scale) - Used as basis for GBC V2 features\n",
    "def preprocess_data_v4_catboost_manual(df, is_train=True, latest_date=None, fit_imputers=None, fit_scaler=None):\n",
    "    \"\"\" V2 features, keeps original categoricals, handles imputation+scaling manually \"\"\"\n",
    "    df_processed = df.copy(); target_col = 'Target'\n",
    "    current_year = datetime.datetime.now().year\n",
    "    try:\n",
    "        df_processed['Dt_Customer_temp'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "        max_enroll_year = df_processed['Dt_Customer_temp'].dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "        df_processed.drop('Dt_Customer_temp', axis=1, inplace=True)\n",
    "    except Exception: reference_year = current_year\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "    df_processed['Dt_Customer_dt'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    if is_train:\n",
    "        global global_latest_date_v4\n",
    "        valid_dates = df_processed['Dt_Customer_dt'].dropna()\n",
    "        try: global_latest_date_v4 = valid_dates.max() + datetime.timedelta(days=1) if not valid_dates.empty else datetime.datetime(reference_year + 1, 1, 1)\n",
    "        except TypeError: global_latest_date_v4 = datetime.datetime(reference_year + 1, 1, 1)\n",
    "        latest_date_to_use = global_latest_date_v4\n",
    "    elif latest_date: latest_date_to_use = latest_date\n",
    "    else: latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1); print(f\"Warning: Using fallback latest date for test: {latest_date_to_use}\")\n",
    "    df_processed['Enroll_Month'] = df_processed['Dt_Customer_dt'].dt.month\n",
    "    df_processed['Enroll_Year'] = df_processed['Dt_Customer_dt'].dt.year\n",
    "    df_processed['Enroll_DayOfWeek'] = df_processed['Dt_Customer_dt'].dt.dayofweek\n",
    "    mask = pd.notna(df_processed['Dt_Customer_dt'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer_dt']).dt.days\n",
    "    df_processed.drop(['Dt_Customer_dt', 'Dt_Customer'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']; purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']; cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed['Num_Adults'] = df_processed['Marital_Status'].apply(lambda x: 1 if str(x) in ['Single','Divorced','Widow','Absurd','Alone','YOLO'] else 2)\n",
    "    num_people = df_processed['Children'] + df_processed['Num_Adults']\n",
    "    df_processed['Income_per_Person_Denom'] = num_people.replace(0, 1)\n",
    "    df_processed['Spend_per_Purchase_Denom'] = df_processed['Total_Purchases'].replace(0, 1)\n",
    "    df_processed.drop('Num_Adults', axis=1, inplace=True, errors='ignore')\n",
    "    # Identify initial features *before* imputation\n",
    "    cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    initial_numerical_features = df_processed[cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    initial_categorical_features = df_processed[cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # Imputation\n",
    "    if is_train:\n",
    "        imputer_num = SimpleImputer(strategy='median'); imputer_num.fit(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_numerical_features] = imputer_num.transform(df_processed[initial_numerical_features])\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent'); imputer_cat.fit(df_processed[initial_categorical_features])\n",
    "        df_processed[initial_categorical_features] = imputer_cat.transform(df_processed[initial_categorical_features])\n",
    "        fit_imputers = {'num': imputer_num, 'cat': imputer_cat}\n",
    "    else:\n",
    "        if fit_imputers is None: raise ValueError(\"Fitted imputers needed\")\n",
    "        df_processed[initial_numerical_features] = fit_imputers['num'].transform(df_processed[initial_numerical_features])\n",
    "        df_processed[initial_categorical_features] = fit_imputers['cat'].transform(df_processed[initial_categorical_features])\n",
    "    # Reconstruct DataFrame\n",
    "    original_cols = df_processed.columns\n",
    "    imputed_cols = initial_numerical_features + initial_categorical_features\n",
    "    df_temp_imputed = pd.DataFrame(df_processed[imputed_cols], columns=imputed_cols, index=df_processed.index)\n",
    "    if 'ID' in original_cols: df_temp_imputed['ID'] = df_processed['ID']\n",
    "    if target_col in original_cols: df_temp_imputed[target_col] = df_processed[target_col]\n",
    "    df_processed = df_temp_imputed\n",
    "    # Create Derived Features\n",
    "    df_processed['Wine_Ratio'] = (df_processed['MntWines'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Meat_Ratio'] = (df_processed['MntMeatProducts'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Fruit_Ratio'] = (df_processed['MntFruits'] / df_processed['Total_Mnt'].replace(0, 1)).fillna(0)\n",
    "    df_processed['Income_per_Person'] = (df_processed['Income'] / df_processed['Income_per_Person_Denom']).fillna(0)\n",
    "    df_processed['Spending_per_Purchase'] = (df_processed['Total_Mnt'] / df_processed['Spend_per_Purchase_Denom']).fillna(0)\n",
    "    df_processed.drop(['Income_per_Person_Denom', 'Spend_per_Purchase_Denom'], axis=1, inplace=True, errors='ignore')\n",
    "    df_processed.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    # Scaling\n",
    "    final_cols_to_process = [col for col in df_processed.columns if col not in ['ID', target_col]]\n",
    "    final_numerical_features = df_processed[final_cols_to_process].select_dtypes(include=np.number).columns.tolist()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_processed[final_numerical_features])\n",
    "        df_processed[final_numerical_features] = scaler.transform(df_processed[final_numerical_features])\n",
    "        fit_scaler = scaler\n",
    "    else:\n",
    "        if fit_scaler is None: raise ValueError(\"Fitted scaler needed\")\n",
    "        df_processed[final_numerical_features] = fit_scaler.transform(df_processed[final_numerical_features])\n",
    "    # Final Type Conversion\n",
    "    final_categorical_features = df_processed[final_cols_to_process].select_dtypes(exclude=np.number).columns.tolist()\n",
    "    for col in final_categorical_features: df_processed[col] = df_processed[col].astype(str)\n",
    "    df_processed.columns = df_processed.columns.astype(str)\n",
    "    if target_col in df_processed.columns: df_processed.rename(columns={str(target_col): target_col}, inplace=True)\n",
    "    return df_processed, fit_imputers, fit_scaler\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing V1 ---\n",
    "print(\"Preprocessing V1...\")\n",
    "train_df_processed_v1 = preprocess_data_v1(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v1' not in globals(): global_latest_date_v1 = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "test_df_processed_v1 = preprocess_data_v1(test_df.copy(), is_train=False, latest_date=global_latest_date_v1)\n",
    "print(\"V1 Preprocessing complete.\")\n",
    "\n",
    "# --- Apply Preprocessing V4 (for GBC V2 Base) ---\n",
    "print(\"\\nPreprocessing V4 (for GBC V2 base)...\")\n",
    "train_df_processed_v4, fitted_imputers_v4, fitted_scaler_v4 = preprocess_data_v4_catboost_manual(train_df.copy(), is_train=True)\n",
    "if 'global_latest_date_v4' not in globals(): global_latest_date_v4 = datetime.datetime.now() + datetime.timedelta(days=1);\n",
    "test_df_processed_v4, _, _ = preprocess_data_v4_catboost_manual(test_df.copy(), is_train=False, latest_date=global_latest_date_v4, fit_imputers=fitted_imputers_v4, fit_scaler=fitted_scaler_v4)\n",
    "print(\"V4 Preprocessing complete.\")\n",
    "\n",
    "\n",
    "# --- Prepare Data V1 (Target for GBC V1) ---\n",
    "X_v1 = train_df_processed_v1.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_v1 = train_df_processed_v1['Target']\n",
    "X_test_v1 = test_df_processed_v1.drop('ID', axis=1, errors='ignore')\n",
    "train_cols_v1 = X_v1.columns; test_cols_v1 = X_test_v1.columns\n",
    "missing_in_test_v1 = set(train_cols_v1) - set(test_cols_v1)\n",
    "for c in missing_in_test_v1: X_test_v1[c] = 0\n",
    "missing_in_train_v1 = set(test_cols_v1) - set(train_cols_v1)\n",
    "for c in missing_in_train_v1: X_v1[c] = 0\n",
    "X_test_v1 = X_test_v1[train_cols_v1]\n",
    "\n",
    "# --- Prepare Data V4 (Target for GBC V2) ---\n",
    "# StackingClassifier needs the *same* X for all base estimators usually.\n",
    "# So we use V4 data structure for BOTH GBC models in stacking.\n",
    "X_stack = train_df_processed_v4.drop(['ID', 'Target'], axis=1, errors='ignore')\n",
    "y_stack = train_df_processed_v4['Target'] # Target is the same\n",
    "X_test_stack = test_df_processed_v4.drop('ID', axis=1, errors='ignore')\n",
    "# Align Stacking Data\n",
    "train_cols_stack = X_stack.columns; test_cols_stack = X_test_stack.columns\n",
    "missing_in_test_stack = set(train_cols_stack) - set(test_cols_stack)\n",
    "for c in missing_in_test_stack: X_test_stack[c] = 0\n",
    "missing_in_train_stack = set(test_cols_stack) - set(train_cols_stack)\n",
    "for c in missing_in_train_stack: X_stack[c] = 0\n",
    "X_test_stack = X_test_stack[train_cols_stack]\n",
    "\n",
    "\n",
    "# --- Define Preprocessing Pipelines for Base Models in Stacking ---\n",
    "\n",
    "# Preprocessor for GBC V1 base model (Simplified Cats + OHE + Scale)\n",
    "# Needs to operate on the common X_stack structure now\n",
    "numerical_features_stack = X_stack.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features_stack_orig = ['Education', 'Marital_Status'] # Original cats in V4 structure\n",
    "# Apply V1 simplification logic here before OHE\n",
    "def simplify_cats_v1(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Marital_Status'] = df_copy['Marital_Status'].replace({'Married': 'Partner', 'Together': 'Partner','Absurd': 'Single', 'Alone': 'Single', 'YOLO': 'Single', 'Widow': 'Single','Divorced':'Single'})\n",
    "    df_copy['Education'] = df_copy['Education'].replace({'2n Cycle': 'Master'})\n",
    "    return df_copy\n",
    "\n",
    "# Need FunctionTransformer if simplifying within pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "simplifier_v1 = FunctionTransformer(simplify_cats_v1, validate=False)\n",
    "\n",
    "# Updated Preprocessor V1 for Stacking\n",
    "numerical_pipeline_stack = Pipeline([('scaler', StandardScaler())]) # Assumes imputation done\n",
    "categorical_pipeline_v1_stack = Pipeline([\n",
    "    # ('simplifier', simplifier_v1), # Apply simplification - tricky with FunctionTransformer and column names\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "# Need to select *original* cat columns for OHE in V1 pipeline\n",
    "preprocessor_v1_stack = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_stack, numerical_features_stack),\n",
    "    ('cat', categorical_pipeline_v1_stack, categorical_features_stack_orig)], # OHE original cats\n",
    "    remainder='passthrough') # Pass through other V4 features\n",
    "\n",
    "\n",
    "# Preprocessor for GBC V2 base model (Original Cats + Date Cats + OHE + Scale)\n",
    "categorical_features_v4_names = X_stack.select_dtypes(include='object').columns.tolist() # All object columns in V4 structure\n",
    "numerical_pipeline_stack_v2 = Pipeline([('scaler', StandardScaler())]) # Assumes imputation done\n",
    "categorical_pipeline_v2_stack = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_v2_stack = ColumnTransformer([\n",
    "    ('num', numerical_pipeline_stack_v2, numerical_features_stack),\n",
    "    ('cat', categorical_pipeline_v2_stack, categorical_features_v4_names)], # OHE *all* object cols\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "# --- Define BEST Hyperparameters (from previous runs) ---\n",
    "best_params_gbc_v1_direct = { # No 'classifier__' prefix needed when setting on model directly\n",
    "    'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'sqrt', 'max_depth': 2,\n",
    "    'learning_rate': 0.08, 'random_state':42}\n",
    "\n",
    "best_params_gbc_v2_direct = { # Params that gave 0.848\n",
    "    'subsample': 0.7, 'n_estimators': 300, 'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20, 'max_features': 'log2', 'max_depth': 2,\n",
    "    'learning_rate': 0.05, 'random_state':42}\n",
    "\n",
    "# --- Define Base Estimator Pipelines for Stacking ---\n",
    "# Base Estimator 1: GBC with V1-style preprocessing\n",
    "base_gbc_v1 = Pipeline([\n",
    "    # Need to handle simplification OR use V1 data. Using V4 data + OHE original cats is simpler for stacking input consistency.\n",
    "    # Let's redefine GBC V1 slightly: V4 features, but OHE only original cats, use V1 params.\n",
    "    ('preprocessor', preprocessor_v1_stack), # Scales all numerics, OHEs 'Education', 'Marital_Status'\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v1_direct))\n",
    "])\n",
    "\n",
    "# Base Estimator 2: GBC with V2-style preprocessing (all cats OHE'd)\n",
    "base_gbc_v2 = Pipeline([\n",
    "    ('preprocessor', preprocessor_v2_stack), # Scales all numerics, OHEs all object columns\n",
    "    ('classifier', GradientBoostingClassifier(**best_params_gbc_v2_direct))\n",
    "])\n",
    "\n",
    "# List of base estimators for StackingClassifier\n",
    "base_estimators = [\n",
    "    ('gbc_v1_style', base_gbc_v1),\n",
    "    ('gbc_v2_style', base_gbc_v2)\n",
    "]\n",
    "\n",
    "# --- Define Meta-Model ---\n",
    "# Logistic Regression is a common choice, C=1 is default regularization\n",
    "meta_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "# --- Define Cross-Validation Strategy for Meta-Model ---\n",
    "# StackingClassifier uses this CV to generate the out-of-fold predictions for training the meta-model\n",
    "N_SPLITS_STACK = 5\n",
    "RANDOM_STATE_STACK = 42\n",
    "cv_stack = StratifiedKFold(n_splits=N_SPLITS_STACK, shuffle=True, random_state=RANDOM_STATE_STACK)\n",
    "\n",
    "# --- Create Stacking Classifier ---\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=cv_stack,\n",
    "    stack_method='predict_proba', # Use probabilities as input to meta-model\n",
    "    n_jobs=-1,\n",
    "    passthrough=False # Do not pass original features to final estimator\n",
    ")\n",
    "\n",
    "# --- Train Stacking Classifier ---\n",
    "print(\"\\nTraining Stacking Classifier...\")\n",
    "# Fit on the V4 data structure, the pipelines within handle specifics\n",
    "stacking_clf.fit(X_stack, y_stack)\n",
    "print(\"Stacking Classifier training complete.\")\n",
    "\n",
    "# --- Predict on Test Data with Stacking Classifier ---\n",
    "print(\"Predicting on test data using the Stacking Classifier...\")\n",
    "# Predict using the same V4 test data structure\n",
    "test_predictions_stacking = stacking_clf.predict(X_test_stack)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Generate Submission File for Stacking ---\n",
    "submission_df_stacking = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions_stacking})\n",
    "submission_filename_stacking = 'submission_stacking_gbc1_gbc2.csv'\n",
    "submission_df_stacking.to_csv(submission_filename_stacking, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename_stacking}' created successfully.\")\n",
    "print(submission_df_stacking.head())\n",
    "print(f\"\\nPredicted target distribution (Stacking):\\n{submission_df_stacking['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate Stacking model on training data (less reliable due to OOF nature)\n",
    "train_preds_stacking = stacking_clf.predict(X_stack)\n",
    "train_accuracy_stacking = accuracy_score(y_stack, train_preds_stacking)\n",
    "# Getting reliable ROC AUC requires predict_proba on OOF preds or separate validation\n",
    "try:\n",
    "    train_roc_auc_stacking = roc_auc_score(y_stack, stacking_clf.predict_proba(X_stack)[:, 1])\n",
    "    print(f\"\\n--- Stacking Model Training Set Evaluation ---\")\n",
    "    print(f\"Accuracy: {train_accuracy_stacking:.4f}\")\n",
    "    print(f\"ROC AUC: {train_roc_auc_stacking:.5f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not calculate ROC AUC for stacking on training set: {e}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy_stacking:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
