{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96208800-22cf-4090-9174-9071d646af4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\n",
      "Data loaded successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     exit()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# --- Feature Engineering & Preprocessing ---\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import datetime\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure train.csv, test.csv, and sample_submission.csv are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# --- Feature Engineering & Preprocessing ---\n",
    "\n",
    "def preprocess_data(df, is_train=True, latest_date=None):\n",
    "    \"\"\"Applies feature engineering and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. Handle Outlier/Implausible Birth Years\n",
    "    current_year = datetime.datetime.now().year\n",
    "    # Use a reasonable reference year based on Dt_Customer if available, otherwise current year\n",
    "    try:\n",
    "        max_enroll_year = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce').dt.year.max()\n",
    "        reference_year = max_enroll_year if pd.notna(max_enroll_year) else current_year\n",
    "    except: # Handle cases where Dt_Customer might not exist or be parseable easily\n",
    "        reference_year = current_year\n",
    "\n",
    "    # Replace very old birth years (e.g., < 1910) with NaN to be imputed later\n",
    "    df_processed.loc[df_processed['Year_Birth'] < 1910, 'Year_Birth'] = np.nan\n",
    "\n",
    "    # Calculate Age (handle potential NaNs in Year_Birth temporarily)\n",
    "    df_processed['Age'] = reference_year - df_processed['Year_Birth']\n",
    "    df_processed.drop('Year_Birth', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 2. Process Dt_Customer\n",
    "    # Convert 'Dt_Customer' to datetime, coercing errors\n",
    "    df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "\n",
    "    # Find the latest date for calculating tenure if not provided (from training set)\n",
    "    if is_train:\n",
    "        global global_latest_date # Store latest date from training set globally\n",
    "        global_latest_date = df_processed['Dt_Customer'].max() + datetime.timedelta(days=1)\n",
    "        latest_date_to_use = global_latest_date\n",
    "    elif latest_date:\n",
    "         latest_date_to_use = latest_date\n",
    "    else:\n",
    "        # Fallback if called on test set first or global_latest_date isn't set\n",
    "        latest_date_to_use = datetime.datetime(reference_year + 1, 1, 1) # Use start of next year as reference\n",
    "        print(f\"Warning: Using fallback latest date: {latest_date_to_use}\")\n",
    "\n",
    "\n",
    "    # Calculate Customer_Lifetime (Tenure in days)\n",
    "    # Handle potential NaT dates resulting from coerce errors\n",
    "    mask = pd.notna(df_processed['Dt_Customer'])\n",
    "    df_processed.loc[mask, 'Customer_Lifetime'] = (latest_date_to_use - df_processed.loc[mask, 'Dt_Customer']).dt.days\n",
    "    df_processed['Customer_Lifetime'].fillna(df_processed['Customer_Lifetime'].median(), inplace=True) # Impute NaNs created by NaT\n",
    "    df_processed.drop('Dt_Customer', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 3. Simplify Marital Status\n",
    "    # Consolidate categories\n",
    "    df_processed['Marital_Status'] = df_processed['Marital_Status'].replace({\n",
    "        'Married': 'Partner',\n",
    "        'Together': 'Partner',\n",
    "        'Absurd': 'Single',\n",
    "        'Alone': 'Single',\n",
    "        'YOLO': 'Single',\n",
    "        'Widow': 'Single',\n",
    "        'Divorced':'Single'\n",
    "         }) # Grouping Married/Together and others into Single for simplicity\n",
    "\n",
    "    # 4. Simplify Education\n",
    "    df_processed['Education'] = df_processed['Education'].replace({'2n Cycle': 'Master'}) # Group '2n Cycle' with 'Master'\n",
    "\n",
    "    # 5. Combine Children/Teens\n",
    "    df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
    "    # Optionally drop original columns if 'Children' is deemed sufficient\n",
    "    # df_processed.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)\n",
    "\n",
    "    # 6. Total Spending\n",
    "    mnt_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "    df_processed['Total_Mnt'] = df_processed[mnt_cols].sum(axis=1)\n",
    "\n",
    "    # 7. Total Purchases\n",
    "    purch_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "    df_processed['Total_Purchases'] = df_processed[purch_cols].sum(axis=1)\n",
    "\n",
    "    # 8. Total Campaigns Accepted\n",
    "    cmp_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "    df_processed['Total_CmpAccepted'] = df_processed[cmp_cols].sum(axis=1)\n",
    "\n",
    "    # 9. Drop Constant Columns (identified during EDA)\n",
    "    df_processed.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True, errors='ignore') # errors='ignore' in case they were already dropped\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# Preprocess training data\n",
    "train_df_processed = preprocess_data(train_df, is_train=True)\n",
    "# Preprocess test data using the latest date from training data\n",
    "test_df_processed = preprocess_data(test_df, is_train=False, latest_date=global_latest_date)\n",
    "\n",
    "print(\"Feature engineering complete.\")\n",
    "print(\"\\nTrain Data Info after processing:\")\n",
    "train_df_processed.info()\n",
    "print(\"\\nTest Data Info after processing:\")\n",
    "test_df_processed.info()\n",
    "\n",
    "\n",
    "# --- Model Training ---\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = train_df_processed.drop(['ID', 'Target'], axis=1)\n",
    "y = train_df_processed['Target']\n",
    "X_test = test_df_processed.drop('ID', axis=1)\n",
    "\n",
    "# Align columns - crucial after feature engineering if columns were added/dropped differently (shouldn't happen here but good practice)\n",
    "train_cols = X.columns\n",
    "test_cols = X_test.columns\n",
    "\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test[c] = 0 # Add missing columns to test set with default value (0)\n",
    "\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X[c] = 0 # Add missing columns to train set with default value (0) - less likely\n",
    "\n",
    "X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "\n",
    "# Identify column types for preprocessing\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')), # Impute missing numericals (Age, Income, Customer_Lifetime)\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='most_frequent')), # Impute missing categoricals (if any)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Use sparse_output=False for easier debugging if needed\n",
    "])\n",
    "\n",
    "# Create a column transformer to apply different pipelines to different columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='passthrough') # Keep any columns not specified (though there shouldn't be any here)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "# GradientBoostingClassifier often works well. random_state for reproducibility.\n",
    "# Consider tuning hyperparameters later using GridSearchCV or RandomizedSearchCV\n",
    "model = GradientBoostingClassifier(n_estimators=150, # Increased slightly\n",
    "                                 learning_rate=0.08, # Slightly decreased\n",
    "                                 max_depth=4,       # Increased slightly\n",
    "                                 subsample=0.8,     # Added subsampling\n",
    "                                 random_state=42)\n",
    "\n",
    "# Create the full pipeline: preprocess + model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Train the model on the entire training dataset\n",
    "print(\"\\nTraining the model...\")\n",
    "pipeline.fit(X, y)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Prediction ---\n",
    "\n",
    "# Predict on the preprocessed test data\n",
    "print(\"Predicting on test data...\")\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# --- Submission File Generation ---\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID'], 'Target': test_predictions})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = 'submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file '{submission_filename}' created successfully.\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nPredicted target distribution:\\n{submission_df['Target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Evaluate on the training set (for sanity check, not a true performance measure)\n",
    "train_preds = pipeline.predict(X)\n",
    "train_accuracy = accuracy_score(y, train_preds)\n",
    "train_roc_auc = roc_auc_score(y, pipeline.predict_proba(X)[:, 1]) # Use probabilities for AUC\n",
    "print(f\"\\n--- Training Set Evaluation (Sanity Check) ---\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"ROC AUC: {train_roc_auc:.4f}\")\n",
    "# print(\"Classification Report:\\n\", classification_report(y, train_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
